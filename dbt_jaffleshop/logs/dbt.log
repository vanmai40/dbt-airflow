2022-08-07 11:15:54.028930 (MainThread): Running with dbt=0.21.1
2022-08-07 11:15:54.040462 (MainThread): Profile not loaded due to error: Runtime Error
  Could not find profile named 'default'
2022-08-07 11:15:54.040462 (MainThread): No profile "default" found, continuing with no target
2022-08-07 11:15:54.056963 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.deps.DepsTask'>, debug=False, defer=None, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='deps', single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', warn_error=False, which='deps', write_json=True)
2022-08-07 11:15:54.057964 (MainThread): Tracking: do not track
2022-08-07 11:15:54.084574 (MainThread): Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-2ckgj7uw'
2022-08-07 11:15:54.084574 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/index.json
2022-08-07 11:15:54.259708 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/index.json 200
2022-08-07 11:15:54.259708 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
2022-08-07 11:15:54.356044 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
2022-08-07 11:15:54.358052 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json
2022-08-07 11:15:54.455465 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json 200
2022-08-07 11:15:54.455465 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 11:15:54.550603 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 11:15:54.566132 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 11:15:54.677551 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 11:15:54.678573 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json
2022-08-07 11:15:54.786714 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json 200
2022-08-07 11:15:54.789739 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json
2022-08-07 11:15:54.908410 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json 200
2022-08-07 11:15:54.909412 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
2022-08-07 11:15:55.026335 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
2022-08-07 11:15:55.033646 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json
2022-08-07 11:15:55.178127 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json 200
2022-08-07 11:15:55.178127 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 11:15:55.310492 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 11:15:55.329076 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 11:15:55.488237 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 11:15:55.488237 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
2022-08-07 11:15:55.662025 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
2022-08-07 11:15:55.664034 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 11:15:55.762618 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 11:15:55.777676 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json
2022-08-07 11:15:55.908564 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json 200
2022-08-07 11:15:55.911600 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
2022-08-07 11:15:56.002513 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
2022-08-07 11:15:56.010057 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json
2022-08-07 11:15:56.122615 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json 200
2022-08-07 11:15:56.122615 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 11:15:56.250909 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 11:15:56.251428 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json
2022-08-07 11:15:56.354203 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json 200
2022-08-07 11:15:56.354203 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json
2022-08-07 11:15:56.456170 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json 200
2022-08-07 11:15:56.456170 (MainThread): Installing fivetran/github@0.5.0
2022-08-07 11:15:57.018489 (MainThread):   Installed from version 0.5.0
2022-08-07 11:15:57.018489 (MainThread):   Updated version available: 0.5.1
2022-08-07 11:15:57.019481 (MainThread): Installing dbt-labs/dbt_utils@0.8.6
2022-08-07 11:15:57.475898 (MainThread):   Installed from version 0.8.6
2022-08-07 11:15:57.475898 (MainThread):   Up to date!
2022-08-07 11:15:57.476900 (MainThread): Installing fivetran/github_source@0.5.0
2022-08-07 11:15:57.963932 (MainThread):   Installed from version 0.5.0
2022-08-07 11:15:57.963932 (MainThread):   Up to date!
2022-08-07 11:15:57.964924 (MainThread): Installing fivetran/fivetran_utils@0.3.8
2022-08-07 11:15:58.220452 (MainThread):   Installed from version 0.3.8
2022-08-07 11:15:58.221452 (MainThread):   Up to date!
2022-08-07 11:15:58.221452 (MainThread): 
Updates available for packages: ['fivetran/github']                 
Update your versions in packages.yml, then run dbt deps
2022-08-07 11:15:58.223465 (MainThread): Flushing usage events
2022-08-07 12:31:53.003441 (MainThread): Running with dbt=0.21.1
2022-08-07 12:31:53.708124 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:31:53.888315 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 12:31:53.889796 (MainThread): Tracking: tracking
2022-08-07 12:31:53.899577 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BDC4420760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BDDC0C1F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BDDC0C1A30>]}
2022-08-07 12:31:53.919226 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BDDC0D8DC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BDDC0E0C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002BDDC0E8760>]}
2022-08-07 12:31:53.919226 (MainThread): Flushing usage events
2022-08-07 12:31:54.404528 (MainThread): Encountered an error:
2022-08-07 12:31:54.405521 (MainThread): Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils
2022-08-07 12:31:54.425022 (MainThread): Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 367, in load_projects
    project = self.new_project(str(path))
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 144, in new_project
    project = Project.from_project_root(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 643, in from_project_root
    return partial.render(renderer)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 289, in render
    return self.create_project(rendered)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 301, in create_project
    dbt_version = _get_required_version(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 226, in _get_required_version
    validate_version(dbt_version, project_dict['name'])
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 203, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =0.21.1
    Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
  Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 258, in run_from_args
    results = task.run()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 438, in run
    self._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 154, in _runtime_initialize
    super()._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 79, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\parser\manifest.py", line 179, in get_full_manifest
    projects = config.load_dependencies()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 347, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 369, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils

2022-08-07 12:32:28.195336 (MainThread): Running with dbt=0.21.1
2022-08-07 12:32:28.902371 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:32:29.087272 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 12:32:29.088273 (MainThread): Tracking: tracking
2022-08-07 12:32:29.098399 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000226440F0AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002265BD9CFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002265BD9CB20>]}
2022-08-07 12:32:29.109967 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002265BDB3DF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002265BDB7CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002265BDB5760>]}
2022-08-07 12:32:29.110968 (MainThread): Flushing usage events
2022-08-07 12:32:29.579352 (MainThread): Encountered an error:
2022-08-07 12:32:29.579352 (MainThread): Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils
2022-08-07 12:32:29.583363 (MainThread): Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 367, in load_projects
    project = self.new_project(str(path))
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 144, in new_project
    project = Project.from_project_root(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 643, in from_project_root
    return partial.render(renderer)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 289, in render
    return self.create_project(rendered)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 301, in create_project
    dbt_version = _get_required_version(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 226, in _get_required_version
    validate_version(dbt_version, project_dict['name'])
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 203, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =0.21.1
    Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
  Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 258, in run_from_args
    results = task.run()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 438, in run
    self._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 154, in _runtime_initialize
    super()._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 79, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\parser\manifest.py", line 179, in get_full_manifest
    projects = config.load_dependencies()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 347, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 369, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils

2022-08-07 12:32:46.795534 (MainThread): Running with dbt=0.21.1
2022-08-07 12:32:47.488011 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:32:47.669292 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.deps.DepsTask'>, debug=False, defer=None, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='deps', single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', warn_error=False, which='deps', write_json=True)
2022-08-07 12:32:47.669292 (MainThread): Tracking: tracking
2022-08-07 12:32:47.679818 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C3385AA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C1BBA0A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C338541F0>]}
2022-08-07 12:32:47.680767 (MainThread): Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-y67qk7xv'
2022-08-07 12:32:47.681767 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/index.json
2022-08-07 12:32:47.811381 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/index.json 200
2022-08-07 12:32:47.811381 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
2022-08-07 12:32:47.908543 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
2022-08-07 12:32:47.910542 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json
2022-08-07 12:32:48.011342 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json 200
2022-08-07 12:32:48.011342 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:32:48.101296 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:32:48.115873 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:32:48.201609 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:32:48.201609 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json
2022-08-07 12:32:48.294479 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json 200
2022-08-07 12:32:48.296479 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json
2022-08-07 12:32:48.389882 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json 200
2022-08-07 12:32:48.390881 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
2022-08-07 12:32:48.484970 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
2022-08-07 12:32:48.492078 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json
2022-08-07 12:32:48.589147 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json 200
2022-08-07 12:32:48.589147 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:32:48.685503 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:32:48.701234 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:32:48.776153 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:32:48.777153 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
2022-08-07 12:32:48.858552 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
2022-08-07 12:32:48.860599 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:32:48.976442 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:32:48.992019 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json
2022-08-07 12:32:49.084642 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json 200
2022-08-07 12:32:49.086642 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
2022-08-07 12:32:49.202302 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
2022-08-07 12:32:49.210854 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json
2022-08-07 12:32:49.313586 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.0.json 200
2022-08-07 12:32:49.314587 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:32:49.406209 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:32:49.406209 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json
2022-08-07 12:32:49.514463 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json 200
2022-08-07 12:32:49.514463 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json
2022-08-07 12:32:49.628207 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json 200
2022-08-07 12:32:49.628766 (MainThread): Installing fivetran/github@0.5.0
2022-08-07 12:32:50.205802 (MainThread):   Installed from version 0.5.0
2022-08-07 12:32:50.206805 (MainThread):   Updated version available: 0.5.1
2022-08-07 12:32:50.206805 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '309a15c0-858f-4481-bafd-2fc5824772d9', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C338545E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C33854D60>]}
2022-08-07 12:32:50.206805 (MainThread): Installing dbt-labs/dbt_utils@0.8.6
2022-08-07 12:32:50.791647 (MainThread):   Installed from version 0.8.6
2022-08-07 12:32:50.791647 (MainThread):   Up to date!
2022-08-07 12:32:50.792645 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '309a15c0-858f-4481-bafd-2fc5824772d9', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C3386BA00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C3386BB50>]}
2022-08-07 12:32:50.792645 (MainThread): Installing fivetran/github_source@0.5.0
2022-08-07 12:32:51.338374 (MainThread):   Installed from version 0.5.0
2022-08-07 12:32:51.338374 (MainThread):   Up to date!
2022-08-07 12:32:51.338374 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '309a15c0-858f-4481-bafd-2fc5824772d9', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C338547F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C33854760>]}
2022-08-07 12:32:51.339458 (MainThread): Installing fivetran/fivetran_utils@0.3.8
2022-08-07 12:32:51.594515 (MainThread):   Installed from version 0.3.8
2022-08-07 12:32:51.594515 (MainThread):   Up to date!
2022-08-07 12:32:51.595516 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '309a15c0-858f-4481-bafd-2fc5824772d9', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C3386B820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C3386BB80>]}
2022-08-07 12:32:51.595516 (MainThread): 
Updates available for packages: ['fivetran/github']                 
Update your versions in packages.yml, then run dbt deps
2022-08-07 12:32:51.597583 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C33848F40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C3385AD30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023C1BBA0A60>]}
2022-08-07 12:32:51.597583 (MainThread): Flushing usage events
2022-08-07 12:34:24.690959 (MainThread): Running with dbt=0.21.1
2022-08-07 12:34:25.382997 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:34:25.563293 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.deps.DepsTask'>, debug=False, defer=None, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='deps', single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', warn_error=False, which='deps', write_json=True)
2022-08-07 12:34:25.563293 (MainThread): Tracking: tracking
2022-08-07 12:34:25.574201 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B930D610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A1650670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B93045B0>]}
2022-08-07 12:34:25.575202 (MainThread): Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-gqv00nhz'
2022-08-07 12:34:25.575202 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/index.json
2022-08-07 12:34:25.731545 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/index.json 200
2022-08-07 12:34:25.732531 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
2022-08-07 12:34:25.831082 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
2022-08-07 12:34:25.833074 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.1.json
2022-08-07 12:34:26.139419 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.1.json 200
2022-08-07 12:34:26.140416 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:34:26.235683 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:34:26.249987 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:34:26.333162 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:34:26.333162 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json
2022-08-07 12:34:26.425007 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json 200
2022-08-07 12:34:26.427005 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json
2022-08-07 12:34:26.516564 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json 200
2022-08-07 12:34:26.516564 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
2022-08-07 12:34:26.621342 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
2022-08-07 12:34:26.628371 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json
2022-08-07 12:34:26.706651 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json 200
2022-08-07 12:34:26.706651 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:34:26.828549 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:34:26.846577 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:34:26.956568 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:34:26.956568 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
2022-08-07 12:34:27.036014 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
2022-08-07 12:34:27.038009 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:34:27.119090 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:34:27.134103 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json
2022-08-07 12:34:27.224512 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json 200
2022-08-07 12:34:27.226510 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
2022-08-07 12:34:27.322713 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
2022-08-07 12:34:27.331087 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.1.json
2022-08-07 12:34:27.441678 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.1.json 200
2022-08-07 12:34:27.441678 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:34:27.530941 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:34:27.531948 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json
2022-08-07 12:34:27.628683 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source/0.5.0.json 200
2022-08-07 12:34:27.628683 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json
2022-08-07 12:34:27.734550 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils/0.3.8.json 200
2022-08-07 12:34:27.735548 (MainThread): Installing fivetran/github@0.5.1
2022-08-07 12:34:28.227881 (MainThread):   Installed from version 0.5.1
2022-08-07 12:34:28.228880 (MainThread):   Up to date!
2022-08-07 12:34:28.228880 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '3927a083-c2fe-4d9d-b9ee-2c4e940bc4b6', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B9304E20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B9304940>]}
2022-08-07 12:34:28.228880 (MainThread): Installing dbt-labs/dbt_utils@0.8.6
2022-08-07 12:34:28.781422 (MainThread):   Installed from version 0.8.6
2022-08-07 12:34:28.781422 (MainThread):   Up to date!
2022-08-07 12:34:28.781422 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '3927a083-c2fe-4d9d-b9ee-2c4e940bc4b6', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B931BE50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B931BB20>]}
2022-08-07 12:34:28.782648 (MainThread): Installing fivetran/github_source@0.5.0
2022-08-07 12:34:29.304823 (MainThread):   Installed from version 0.5.0
2022-08-07 12:34:29.305823 (MainThread):   Up to date!
2022-08-07 12:34:29.305823 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '3927a083-c2fe-4d9d-b9ee-2c4e940bc4b6', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B9304670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B9304A00>]}
2022-08-07 12:34:29.305823 (MainThread): Installing fivetran/fivetran_utils@0.3.8
2022-08-07 12:34:29.520926 (MainThread):   Installed from version 0.3.8
2022-08-07 12:34:29.520926 (MainThread):   Up to date!
2022-08-07 12:34:29.520926 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '3927a083-c2fe-4d9d-b9ee-2c4e940bc4b6', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B931B820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B931BC10>]}
2022-08-07 12:34:29.522925 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B92FD490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254A1650670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000254B930D610>]}
2022-08-07 12:34:29.523925 (MainThread): Flushing usage events
2022-08-07 12:34:37.046468 (MainThread): Running with dbt=0.21.1
2022-08-07 12:34:37.739393 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:34:37.920657 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 12:34:37.921655 (MainThread): Tracking: tracking
2022-08-07 12:34:37.932207 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214D8650A60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214F02EFFA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214F02EFB80>]}
2022-08-07 12:34:37.944156 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214F0303E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214F0307D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000214F03057C0>]}
2022-08-07 12:34:37.944156 (MainThread): Flushing usage events
2022-08-07 12:34:38.406871 (MainThread): Encountered an error:
2022-08-07 12:34:38.408388 (MainThread): Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils
2022-08-07 12:34:38.410439 (MainThread): Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 367, in load_projects
    project = self.new_project(str(path))
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 144, in new_project
    project = Project.from_project_root(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 643, in from_project_root
    return partial.render(renderer)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 289, in render
    return self.create_project(rendered)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 301, in create_project
    dbt_version = _get_required_version(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 226, in _get_required_version
    validate_version(dbt_version, project_dict['name'])
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 203, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =0.21.1
    Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
  Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 258, in run_from_args
    results = task.run()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 438, in run
    self._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 154, in _runtime_initialize
    super()._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 79, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\parser\manifest.py", line 179, in get_full_manifest
    projects = config.load_dependencies()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 347, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 369, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils

2022-08-07 12:35:03.754074 (MainThread): Running with dbt=0.21.1
2022-08-07 12:35:04.456527 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:35:04.639231 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.deps.DepsTask'>, debug=False, defer=None, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='deps', single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', warn_error=False, which='deps', write_json=True)
2022-08-07 12:35:04.640270 (MainThread): Tracking: tracking
2022-08-07 12:35:04.651245 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001725162AAF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017239970AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000172516245B0>]}
2022-08-07 12:35:04.653245 (MainThread): Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-6tkk8s2p'
2022-08-07 12:35:04.653245 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/index.json
2022-08-07 12:35:04.838493 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/index.json 200
2022-08-07 12:35:04.839538 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
2022-08-07 12:35:04.940889 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
2022-08-07 12:35:04.942892 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.1.json
2022-08-07 12:35:05.157574 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github/0.5.1.json 200
2022-08-07 12:35:05.158619 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:35:05.259271 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:35:05.274027 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000017239970AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001725163CD60>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001725163CCD0>]}
2022-08-07 12:35:05.274027 (MainThread): Flushing usage events
2022-08-07 12:35:05.731234 (MainThread): Encountered an error:
2022-08-07 12:35:05.731234 (MainThread): Could not find a matching version for package dbt-labs/dbt_utils
  Requested range: =1.0.0, =1.0.0
  Available versions: ['0.0.1', '0.1.0', '0.1.1', '0.1.2', '0.1.3', '0.1.4', '0.1.5', '0.1.6', '0.1.7', '0.1.8', '0.1.9', '0.1.10', '0.1.11', '0.1.12', '0.1.13', '0.1.14', '0.1.15', '0.1.16', '0.1.17', '0.1.18', '0.1.19', '0.1.20', '0.1.21', '0.1.22', '0.1.23', '0.1.24', '0.1.25', '0.2.0', '0.2.1', '0.2.2', '0.2.3', '0.2.4', '0.2.5', '0.3.0', '0.4.0', '0.4.1', '0.5.0', '0.5.1', '0.6.0', '0.6.1', '0.6.2', '0.6.3', '0.6.4', '0.6.5', '0.6.6', '0.7.0', '0.7.1', '0.7.2', '0.7.3', '0.7.4', '0.7.5', '0.7.6', '0.8.0', '0.8.1', '0.8.2', '0.8.3', '0.8.4', '0.8.5', '0.8.6']
2022-08-07 12:35:05.755852 (MainThread): Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 258, in run_from_args
    results = task.run()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\deps.py", line 53, in run
    final_deps = resolve_packages(packages, self.config)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\deps\resolver.py", line 137, in resolve_packages
    target = final[package].resolved().fetch_metadata(config, renderer)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\deps\registry.py", line 142, in resolved
    package_version_not_found(self.package, range_, installable)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\exceptions.py", line 710, in package_version_not_found
    raise_dependency_error(base_msg.format(package_name,
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\exceptions.py", line 455, in raise_dependency_error
    raise DependencyException(msg)
dbt.exceptions.DependencyException: Could not find a matching version for package dbt-labs/dbt_utils
  Requested range: =1.0.0, =1.0.0
  Available versions: ['0.0.1', '0.1.0', '0.1.1', '0.1.2', '0.1.3', '0.1.4', '0.1.5', '0.1.6', '0.1.7', '0.1.8', '0.1.9', '0.1.10', '0.1.11', '0.1.12', '0.1.13', '0.1.14', '0.1.15', '0.1.16', '0.1.17', '0.1.18', '0.1.19', '0.1.20', '0.1.21', '0.1.22', '0.1.23', '0.1.24', '0.1.25', '0.2.0', '0.2.1', '0.2.2', '0.2.3', '0.2.4', '0.2.5', '0.3.0', '0.4.0', '0.4.1', '0.5.0', '0.5.1', '0.6.0', '0.6.1', '0.6.2', '0.6.3', '0.6.4', '0.6.5', '0.6.6', '0.7.0', '0.7.1', '0.7.2', '0.7.3', '0.7.4', '0.7.5', '0.7.6', '0.8.0', '0.8.1', '0.8.2', '0.8.3', '0.8.4', '0.8.5', '0.8.6']

2022-08-07 12:35:51.645687 (MainThread): Running with dbt=0.21.1
2022-08-07 12:35:52.350554 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:35:52.531584 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.deps.DepsTask'>, debug=False, defer=None, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='deps', single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', warn_error=False, which='deps', write_json=True)
2022-08-07 12:35:52.532587 (MainThread): Tracking: tracking
2022-08-07 12:35:52.542155 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8A3C5B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023FF2160760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8A3D05E0>]}
2022-08-07 12:35:52.544169 (MainThread): Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-_abwllbc'
2022-08-07 12:35:52.544169 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/index.json
2022-08-07 12:35:52.705071 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/index.json 200
2022-08-07 12:35:52.706083 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:35:52.810354 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:35:52.826043 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:35:52.922103 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:35:52.923077 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
2022-08-07 12:35:53.024572 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
2022-08-07 12:35:53.038639 (MainThread): Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json
2022-08-07 12:35:53.146751 (MainThread): Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils/0.8.6.json 200
2022-08-07 12:35:53.146751 (MainThread): Installing dbt-labs/dbt_utils@0.8.6
2022-08-07 12:35:53.656450 (MainThread):   Installed from version 0.8.6
2022-08-07 12:35:53.657451 (MainThread):   Up to date!
2022-08-07 12:35:53.657451 (MainThread): Sending event: {'category': 'dbt', 'action': 'package', 'label': '644744bc-f44c-49b4-829f-bd572759fd7c', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8A3E52E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8A3E54C0>]}
2022-08-07 12:35:53.658449 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8A3C5C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8A3CBD90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000023F8A3E5370>]}
2022-08-07 12:35:53.658449 (MainThread): Flushing usage events
2022-08-07 12:35:58.928298 (MainThread): Running with dbt=0.21.1
2022-08-07 12:35:59.628756 (MainThread): `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.
Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations
2022-08-07 12:35:59.808552 (MainThread): running dbt with arguments Namespace(cls=<class 'dbt.task.run.RunTask'>, debug=False, defer=None, exclude=None, fail_fast=False, full_refresh=False, log_cache_events=False, log_format='default', partial_parse=None, profile=None, profiles_dir='C:\\Users\\Vanmai40\\.dbt', project_dir=None, record_timing_info=None, rpc_method='run', select=None, selector_name=None, single_threaded=False, state=None, strict=False, target=None, test_new_parser=False, threads=None, use_cache=True, use_colors=None, use_experimental_parser=False, vars='{}', version_check=True, warn_error=False, which='run', write_json=True)
2022-08-07 12:35:59.809592 (MainThread): Tracking: tracking
2022-08-07 12:35:59.818775 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210EC2A0AF0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210FFF51F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210FFF51BE0>]}
2022-08-07 12:35:59.831382 (MainThread): Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210FFF62EE0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210FFF6BD00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210FFF776D0>]}
2022-08-07 12:35:59.831382 (MainThread): Flushing usage events
2022-08-07 12:36:00.280121 (MainThread): Encountered an error:
2022-08-07 12:36:00.280121 (MainThread): Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils
2022-08-07 12:36:00.283136 (MainThread): Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 367, in load_projects
    project = self.new_project(str(path))
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 144, in new_project
    project = Project.from_project_root(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 643, in from_project_root
    return partial.render(renderer)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 289, in render
    return self.create_project(rendered)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 301, in create_project
    dbt_version = _get_required_version(
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 226, in _get_required_version
    validate_version(dbt_version, project_dict['name'])
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\project.py", line 203, in validate_version
    raise DbtProjectError(msg)
dbt.exceptions.DbtProjectError: Runtime Error
  This version of dbt is not supported with the 'dbt_utils' package.
    Installed version of dbt: =0.21.1
    Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
  Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
  

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

The above exception was the direct cause of the following exception:

Traceback (most recent call last):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 127, in main
    results, succeeded = handle_and_check(args)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 205, in handle_and_check
    task, res = run_from_args(parsed)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\main.py", line 258, in run_from_args
    results = task.run()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 438, in run
    self._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 154, in _runtime_initialize
    super()._runtime_initialize()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 92, in _runtime_initialize
    self.load_manifest()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\task\runnable.py", line 79, in load_manifest
    self.manifest = ManifestLoader.get_full_manifest(self.config)
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\parser\manifest.py", line 179, in get_full_manifest
    projects = config.load_dependencies()
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 347, in load_dependencies
    for project_name, project in self.load_projects(project_paths):
  File "c:\users\vanmai40\appdata\local\programs\python\python38\lib\site-packages\dbt\config\runtime.py", line 369, in load_projects
    raise DbtProjectError(
dbt.exceptions.DbtProjectError: Runtime Error
  Failed to read package: Runtime Error
    This version of dbt is not supported with the 'dbt_utils' package.
      Installed version of dbt: =0.21.1
      Required version of dbt for 'dbt_utils': ['>=1.0.0', '<2.0.0']
    Check the requirements for the 'dbt_utils' package, or run dbt again with --no-version-check
    
  
  Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils\dbt_project.yml

Error encountered in C:\Users\Vanmai40\Python\dbt-airflow-gbq-pipeline\dbt-x-airflow\dbt_modules\dbt_utils



============================== 2022-08-07 12:40:15.414028 | 6482fd63-6db0-4137-9043-a52a71e27ada ==============================
[0m12:40:15.414028 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:40:15.414028 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m12:40:15.415016 [debug] [MainThread]: Tracking: tracking
[0m12:40:15.436822 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976E79310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976E796D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976E793D0>]}
[0m12:40:15.438331 [debug] [MainThread]: Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-zo5jejtb'
[0m12:40:15.439374 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m12:40:15.571498 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m12:40:15.571498 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github.json
[0m12:40:15.680531 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github.json 200
[0m12:40:15.682544 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m12:40:15.804483 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m12:40:15.821031 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json
[0m12:40:15.929123 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/github_source.json 200
[0m12:40:15.932155 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json
[0m12:40:16.055599 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/fivetran/fivetran_utils.json 200
[0m12:40:16.118365 [info ] [MainThread]: Installing fivetran/github
[0m12:40:16.599767 [info ] [MainThread]:   Installed from version 0.5.1
[0m12:40:16.599767 [info ] [MainThread]:   Up to date!
[0m12:40:16.600768 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '6482fd63-6db0-4137-9043-a52a71e27ada', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976E79730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976E794C0>]}
[0m12:40:16.600768 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m12:40:17.090539 [info ] [MainThread]:   Installed from version 0.8.6
[0m12:40:17.090539 [info ] [MainThread]:   Up to date!
[0m12:40:17.091531 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '6482fd63-6db0-4137-9043-a52a71e27ada', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976EDDFA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976EDDFD0>]}
[0m12:40:17.091531 [info ] [MainThread]: Installing fivetran/github_source
[0m12:40:17.554235 [info ] [MainThread]:   Installed from version 0.5.0
[0m12:40:17.555235 [info ] [MainThread]:   Up to date!
[0m12:40:17.556232 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '6482fd63-6db0-4137-9043-a52a71e27ada', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976ED7B20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976ED7BE0>]}
[0m12:40:17.556232 [info ] [MainThread]: Installing fivetran/fivetran_utils
[0m12:40:17.834496 [info ] [MainThread]:   Installed from version 0.3.8
[0m12:40:17.834496 [info ] [MainThread]:   Up to date!
[0m12:40:17.835497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '6482fd63-6db0-4137-9043-a52a71e27ada', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976ED7100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976ED76A0>]}
[0m12:40:17.836498 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976E17820>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976EDDD90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000028976EDDE50>]}


============================== 2022-08-07 12:40:43.575150 | 9e153938-2adf-4ab2-a38e-d21a9b644249 ==============================
[0m12:40:43.575150 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:40:43.576149 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:40:43.576149 [debug] [MainThread]: Tracking: tracking
[0m12:40:43.590121 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012749B7B8E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012749B7B970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012749B7B820>]}
[0m12:40:43.670800 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m12:40:43.671762 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '9e153938-2adf-4ab2-a38e-d21a9b644249', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012749B29730>]}
[0m12:40:44.671128 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m12:40:44.672134 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:40:44.690161 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:40:44.695160 [debug] [MainThread]: Parsing macros\etc.sql
[0m12:40:44.697128 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:40:44.698756 [debug] [MainThread]: Parsing macros\materializations\copy.sql
[0m12:40:44.701773 [debug] [MainThread]: Parsing macros\materializations\incremental.sql
[0m12:40:44.715347 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:40:44.718355 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:40:44.719620 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:40:44.723637 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:40:44.725635 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:40:44.726628 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:40:44.726628 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:40:44.727650 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:40:44.728665 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:40:44.728665 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:40:44.729661 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:40:44.729661 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:40:44.729661 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:40:44.730661 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:40:44.731660 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:40:44.731660 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:40:44.732660 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:40:44.733671 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:40:44.744237 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:40:44.752792 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:40:44.754793 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:40:44.758504 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:40:44.764512 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:40:44.769015 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:40:44.780580 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:40:44.782565 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:40:44.789557 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:40:44.793569 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:40:44.794569 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:40:44.795569 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:40:44.795569 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:40:44.796570 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:40:44.797565 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:40:44.799670 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:40:44.801659 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:40:44.803185 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:40:44.807191 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:40:44.810740 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:40:44.819302 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:40:44.820300 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:40:44.835093 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:40:44.846680 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:40:44.850576 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:40:44.855560 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:40:44.858559 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:40:44.860674 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:40:44.861688 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:40:44.866884 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:40:44.882659 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:40:44.889276 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:40:44.900789 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:40:44.912342 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:40:44.913304 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:40:44.927598 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:40:44.928633 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:40:44.934328 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:40:44.935328 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:40:44.936329 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:40:44.937558 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:40:44.938565 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:40:44.939571 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:40:44.944574 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:40:44.945570 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:40:44.946577 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:40:44.947579 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:40:44.949099 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:40:44.949099 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:40:44.950116 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:40:44.951115 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:40:44.952157 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:40:44.953334 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:40:44.955352 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:40:44.955352 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:40:44.956348 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:40:44.957378 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:40:44.958378 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:40:44.959884 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:40:44.960892 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:40:44.962923 [debug] [MainThread]: Parsing macros\cross_db_utils\any_value.sql
[0m12:40:44.963892 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m12:40:44.965898 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m12:40:44.966889 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m12:40:44.969434 [debug] [MainThread]: Parsing macros\cross_db_utils\bool_or.sql
[0m12:40:44.971427 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m12:40:44.973432 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_bool_to_text.sql
[0m12:40:44.974454 [debug] [MainThread]: Parsing macros\cross_db_utils\concat.sql
[0m12:40:44.975456 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m12:40:44.978421 [debug] [MainThread]: Parsing macros\cross_db_utils\datatypes.sql
[0m12:40:44.986810 [debug] [MainThread]: Parsing macros\cross_db_utils\dateadd.sql
[0m12:40:44.988315 [debug] [MainThread]: Parsing macros\cross_db_utils\datediff.sql
[0m12:40:44.996681 [debug] [MainThread]: Parsing macros\cross_db_utils\date_trunc.sql
[0m12:40:44.998124 [debug] [MainThread]: Parsing macros\cross_db_utils\escape_single_quotes.sql
[0m12:40:44.999112 [debug] [MainThread]: Parsing macros\cross_db_utils\except.sql
[0m12:40:45.000143 [debug] [MainThread]: Parsing macros\cross_db_utils\hash.sql
[0m12:40:45.001110 [debug] [MainThread]: Parsing macros\cross_db_utils\identifier.sql
[0m12:40:45.003122 [debug] [MainThread]: Parsing macros\cross_db_utils\intersect.sql
[0m12:40:45.004120 [debug] [MainThread]: Parsing macros\cross_db_utils\last_day.sql
[0m12:40:45.007109 [debug] [MainThread]: Parsing macros\cross_db_utils\length.sql
[0m12:40:45.008112 [debug] [MainThread]: Parsing macros\cross_db_utils\listagg.sql
[0m12:40:45.015179 [debug] [MainThread]: Parsing macros\cross_db_utils\literal.sql
[0m12:40:45.016178 [debug] [MainThread]: Parsing macros\cross_db_utils\position.sql
[0m12:40:45.017179 [debug] [MainThread]: Parsing macros\cross_db_utils\replace.sql
[0m12:40:45.018181 [debug] [MainThread]: Parsing macros\cross_db_utils\right.sql
[0m12:40:45.020179 [debug] [MainThread]: Parsing macros\cross_db_utils\safe_cast.sql
[0m12:40:45.022185 [debug] [MainThread]: Parsing macros\cross_db_utils\split_part.sql
[0m12:40:45.026195 [debug] [MainThread]: Parsing macros\cross_db_utils\width_bucket.sql
[0m12:40:45.030202 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_ephemeral.sql
[0m12:40:45.032202 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_relation.sql
[0m12:40:45.033195 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m12:40:45.035209 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m12:40:45.036229 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m12:40:45.037225 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m12:40:45.040811 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m12:40:45.041792 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m12:40:45.043324 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m12:40:45.044329 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m12:40:45.051842 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m12:40:45.053840 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m12:40:45.054841 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m12:40:45.055840 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m12:40:45.058304 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m12:40:45.060345 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m12:40:45.062344 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m12:40:45.063349 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m12:40:45.065346 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m12:40:45.067311 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m12:40:45.068346 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m12:40:45.068346 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m12:40:45.069769 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m12:40:45.070732 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m12:40:45.090394 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m12:40:45.094393 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m12:40:45.099976 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m12:40:45.103122 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m12:40:45.108123 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m12:40:45.110140 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m12:40:45.112173 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m12:40:45.115173 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m12:40:45.117139 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m12:40:45.122157 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m12:40:45.124144 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m12:40:45.125152 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m12:40:45.126144 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m12:40:45.131764 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m12:40:45.133099 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m12:40:45.134068 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m12:40:45.137067 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m12:40:45.138604 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m12:40:45.142716 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m12:40:45.145714 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m12:40:45.155015 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m12:40:45.161565 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m12:40:45.163571 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m12:40:45.164572 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m12:40:45.167573 [debug] [MainThread]: Parsing macros\add_dbt_source_relation.sql
[0m12:40:45.167573 [debug] [MainThread]: Parsing macros\add_pass_through_columns.sql
[0m12:40:45.169075 [debug] [MainThread]: Parsing macros\array_agg.sql
[0m12:40:45.170080 [debug] [MainThread]: Parsing macros\calculated_fields.sql
[0m12:40:45.171121 [debug] [MainThread]: Parsing macros\ceiling.sql
[0m12:40:45.172087 [debug] [MainThread]: Parsing macros\collect_freshness.sql
[0m12:40:45.175120 [debug] [MainThread]: Parsing macros\dummy_coalesce_value.sql
[0m12:40:45.178142 [debug] [MainThread]: Parsing macros\empty_variable_warning.sql
[0m12:40:45.179180 [debug] [MainThread]: Parsing macros\enabled_vars.sql
[0m12:40:45.180149 [debug] [MainThread]: Parsing macros\enabled_vars_one_true.sql
[0m12:40:45.181157 [debug] [MainThread]: Parsing macros\fill_pass_through_columns.sql
[0m12:40:45.182163 [debug] [MainThread]: Parsing macros\fill_staging_columns.sql
[0m12:40:45.185707 [debug] [MainThread]: Parsing macros\first_value.sql
[0m12:40:45.186718 [debug] [MainThread]: Parsing macros\json_extract.sql
[0m12:40:45.190295 [debug] [MainThread]: Parsing macros\json_parse.sql
[0m12:40:45.194699 [debug] [MainThread]: Parsing macros\max_bool.sql
[0m12:40:45.195699 [debug] [MainThread]: Parsing macros\percentile.sql
[0m12:40:45.198261 [debug] [MainThread]: Parsing macros\persist_pass_through_columns.sql
[0m12:40:45.199272 [debug] [MainThread]: Parsing macros\pivot_json_extract.sql
[0m12:40:45.200311 [debug] [MainThread]: Parsing macros\remove_prefix_from_columns.sql
[0m12:40:45.201277 [debug] [MainThread]: Parsing macros\seed_data_helper.sql
[0m12:40:45.203285 [debug] [MainThread]: Parsing macros\snowflake_seed_data.sql
[0m12:40:45.203285 [debug] [MainThread]: Parsing macros\source_relation.sql
[0m12:40:45.205310 [debug] [MainThread]: Parsing macros\string_agg.sql
[0m12:40:45.207661 [debug] [MainThread]: Parsing macros\timestamp_add.sql
[0m12:40:45.210701 [debug] [MainThread]: Parsing macros\timestamp_diff.sql
[0m12:40:45.218182 [debug] [MainThread]: Parsing macros\try_cast.sql
[0m12:40:45.221239 [debug] [MainThread]: Parsing macros\union_data.sql
[0m12:40:45.227275 [debug] [MainThread]: Parsing macros\union_relations.sql
[0m12:40:45.234397 [debug] [MainThread]: Parsing macros\get_issue_assignee_columns.sql
[0m12:40:45.235392 [debug] [MainThread]: Parsing macros\get_issue_closed_history_columns.sql
[0m12:40:45.236396 [debug] [MainThread]: Parsing macros\get_issue_columns.sql
[0m12:40:45.239794 [debug] [MainThread]: Parsing macros\get_issue_comment_columns.sql
[0m12:40:45.240792 [debug] [MainThread]: Parsing macros\get_issue_label_columns.sql
[0m12:40:45.241795 [debug] [MainThread]: Parsing macros\get_issue_merged_columns.sql
[0m12:40:45.243792 [debug] [MainThread]: Parsing macros\get_label_columns.sql
[0m12:40:45.244805 [debug] [MainThread]: Parsing macros\get_pull_request_columns.sql
[0m12:40:45.248321 [debug] [MainThread]: Parsing macros\get_pull_request_review_columns.sql
[0m12:40:45.250363 [debug] [MainThread]: Parsing macros\get_repository_columns.sql
[0m12:40:45.252356 [debug] [MainThread]: Parsing macros\get_repo_team_columns.sql
[0m12:40:45.253371 [debug] [MainThread]: Parsing macros\get_requested_reviewer_history_columns.sql
[0m12:40:45.255369 [debug] [MainThread]: Parsing macros\get_team_columns.sql
[0m12:40:45.256362 [debug] [MainThread]: Parsing macros\get_user_columns.sql
[0m12:40:45.654735 [debug] [MainThread]: 1699: static parser successfully parsed prod\agg_transactions.sql
[0m12:40:45.665123 [debug] [MainThread]: 1699: static parser successfully parsed prod\dim_customers.sql
[0m12:40:45.667159 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m12:40:45.671243 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m12:40:45.671243 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_customers.sql
[0m12:40:45.673707 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_orders.sql
[0m12:40:45.675706 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m12:40:45.679659 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m12:40:45.707073 [debug] [MainThread]: 1603: static parser failed on github__daily_metrics.sql
[0m12:40:45.713818 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__daily_metrics.sql
[0m12:40:45.714831 [debug] [MainThread]: 1603: static parser failed on github__issues.sql
[0m12:40:45.718343 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__issues.sql
[0m12:40:45.719358 [debug] [MainThread]: 1603: static parser failed on github__monthly_metrics.sql
[0m12:40:45.722359 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__monthly_metrics.sql
[0m12:40:45.723353 [debug] [MainThread]: 1699: static parser successfully parsed github__pull_requests.sql
[0m12:40:45.725351 [debug] [MainThread]: 1603: static parser failed on github__quarterly_metrics.sql
[0m12:40:45.729795 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__quarterly_metrics.sql
[0m12:40:45.730784 [debug] [MainThread]: 1603: static parser failed on github__weekly_metrics.sql
[0m12:40:45.734335 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__weekly_metrics.sql
[0m12:40:45.735373 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_assignees.sql
[0m12:40:45.741865 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_assignees.sql
[0m12:40:45.742861 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_comments.sql
[0m12:40:45.746827 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_comments.sql
[0m12:40:45.748193 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_joined.sql
[0m12:40:45.758160 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_joined.sql
[0m12:40:45.758160 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_labels.sql
[0m12:40:45.762973 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_labels.sql
[0m12:40:45.764988 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_label_joined.sql
[0m12:40:45.769566 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_label_joined.sql
[0m12:40:45.770579 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_open_length.sql
[0m12:40:45.778737 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_open_length.sql
[0m12:40:45.779725 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_reviewers.sql
[0m12:40:45.785270 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_reviewers.sql
[0m12:40:45.786281 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_times.sql
[0m12:40:45.794574 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_times.sql
[0m12:40:45.795573 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__repository_teams.sql
[0m12:40:45.802826 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__repository_teams.sql
[0m12:40:45.811131 [debug] [MainThread]: 1603: static parser failed on stg_github__issue.sql
[0m12:40:45.875217 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue.sql
[0m12:40:45.876243 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_assignee.sql
[0m12:40:45.883137 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_assignee.sql
[0m12:40:45.884679 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_closed_history.sql
[0m12:40:45.892720 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_closed_history.sql
[0m12:40:45.893715 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_comment.sql
[0m12:40:45.902192 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_comment.sql
[0m12:40:45.903181 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_label.sql
[0m12:40:45.910741 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_label.sql
[0m12:40:45.911743 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_merged.sql
[0m12:40:45.919665 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_merged.sql
[0m12:40:45.920705 [debug] [MainThread]: 1603: static parser failed on stg_github__label.sql
[0m12:40:45.930209 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__label.sql
[0m12:40:45.931178 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request.sql
[0m12:40:45.944818 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request.sql
[0m12:40:45.945840 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request_review.sql
[0m12:40:45.955711 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request_review.sql
[0m12:40:45.956714 [debug] [MainThread]: 1603: static parser failed on stg_github__repository.sql
[0m12:40:45.969447 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repository.sql
[0m12:40:45.970444 [debug] [MainThread]: 1603: static parser failed on stg_github__repo_team.sql
[0m12:40:45.978469 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repo_team.sql
[0m12:40:45.978469 [debug] [MainThread]: 1603: static parser failed on stg_github__requested_reviewer_history.sql
[0m12:40:45.987596 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__requested_reviewer_history.sql
[0m12:40:45.988615 [debug] [MainThread]: 1603: static parser failed on stg_github__team.sql
[0m12:40:45.999246 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__team.sql
[0m12:40:46.000246 [debug] [MainThread]: 1603: static parser failed on stg_github__user.sql
[0m12:40:46.007516 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__user.sql
[0m12:40:46.008515 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_assignee_tmp.sql
[0m12:40:46.012864 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_assignee_tmp.sql
[0m12:40:46.013865 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:40:46.016830 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:40:46.018230 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_comment_tmp.sql
[0m12:40:46.021232 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_comment_tmp.sql
[0m12:40:46.023196 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_label_tmp.sql
[0m12:40:46.028205 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_label_tmp.sql
[0m12:40:46.028728 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_merged_tmp.sql
[0m12:40:46.031736 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_merged_tmp.sql
[0m12:40:46.033297 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_tmp.sql
[0m12:40:46.037311 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_tmp.sql
[0m12:40:46.038300 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__label_tmp.sql
[0m12:40:46.041838 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__label_tmp.sql
[0m12:40:46.042833 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_review_tmp.sql
[0m12:40:46.045859 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_review_tmp.sql
[0m12:40:46.046825 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_tmp.sql
[0m12:40:46.050208 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_tmp.sql
[0m12:40:46.051211 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repository_tmp.sql
[0m12:40:46.055176 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repository_tmp.sql
[0m12:40:46.056208 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repo_team_tmp.sql
[0m12:40:46.060790 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repo_team_tmp.sql
[0m12:40:46.061793 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:40:46.065068 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:40:46.066069 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__team_tmp.sql
[0m12:40:46.068650 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__team_tmp.sql
[0m12:40:46.069716 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__user_tmp.sql
[0m12:40:46.073757 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__user_tmp.sql
[0m12:40:46.091828 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'stg_cus' in the 'models' section of file 'models\sources.yml'
[0m12:40:46.094308 [warn ] [MainThread]: [[33mWARNING[0m]: Did not find matching node for patch with name 'stg_ord' in the 'models' section of file 'models\sources.yml'
[0m12:40:46.311723 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000012749BBA130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001274AFA14F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001274AFA1640>]}


============================== 2022-08-07 12:41:45.598581 | 4a3fdf9f-e8f0-4779-a64b-a120b7544383 ==============================
[0m12:41:45.598581 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:41:45.599548 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:41:45.599548 [debug] [MainThread]: Tracking: tracking
[0m12:41:45.611678 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1163BA040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1163BAA00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1163BA580>]}
[0m12:41:45.656921 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m12:41:45.656921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '4a3fdf9f-e8f0-4779-a64b-a120b7544383', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1163819A0>]}
[0m12:41:45.770899 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m12:41:45.770899 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:41:45.790916 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:41:45.795424 [debug] [MainThread]: Parsing macros\etc.sql
[0m12:41:45.796425 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:41:45.798929 [debug] [MainThread]: Parsing macros\materializations\copy.sql
[0m12:41:45.800939 [debug] [MainThread]: Parsing macros\materializations\incremental.sql
[0m12:41:45.813449 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:41:45.815449 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:41:45.817449 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:41:45.820958 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:41:45.823463 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:41:45.823463 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:41:45.824470 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:41:45.825469 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:41:45.825469 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:41:45.826469 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:41:45.826469 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:41:45.826469 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:41:45.827469 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:41:45.827469 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:41:45.828469 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:41:45.828974 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:41:45.828974 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:41:45.830987 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:41:45.841553 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:41:45.849057 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:41:45.851062 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:41:45.853567 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:41:45.859081 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:41:45.863091 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:41:45.874600 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:41:45.876600 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:41:45.883611 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:41:45.887616 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:41:45.888617 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:41:45.889122 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:41:45.890127 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:41:45.890127 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:41:45.892127 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:41:45.893127 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:41:45.895127 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:41:45.897127 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:41:45.900132 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:41:45.903132 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:41:45.910639 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:41:45.911640 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:41:45.924209 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:41:45.936780 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:41:45.939359 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:41:45.943360 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:41:45.946402 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:41:45.948405 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:41:45.948960 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:41:45.954057 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:41:45.967606 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:41:45.973200 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:41:45.982124 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:41:45.991236 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:41:45.993206 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:41:46.006382 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:41:46.007390 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:41:46.011209 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:41:46.012208 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:41:46.013210 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:41:46.014209 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:41:46.015208 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:41:46.015208 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:41:46.020700 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:41:46.021702 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:41:46.022701 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:41:46.023700 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:41:46.024750 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:41:46.025742 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:41:46.026742 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:41:46.026742 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:41:46.028289 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:41:46.029367 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:41:46.031363 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:41:46.032366 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:41:46.032366 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:41:46.033372 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:41:46.034920 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:41:46.036014 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:41:46.037015 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:41:46.039595 [debug] [MainThread]: Parsing macros\cross_db_utils\any_value.sql
[0m12:41:46.040600 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m12:41:46.042641 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m12:41:46.044640 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m12:41:46.046640 [debug] [MainThread]: Parsing macros\cross_db_utils\bool_or.sql
[0m12:41:46.047782 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m12:41:46.049828 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_bool_to_text.sql
[0m12:41:46.050827 [debug] [MainThread]: Parsing macros\cross_db_utils\concat.sql
[0m12:41:46.051829 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m12:41:46.054827 [debug] [MainThread]: Parsing macros\cross_db_utils\datatypes.sql
[0m12:41:46.059435 [debug] [MainThread]: Parsing macros\cross_db_utils\dateadd.sql
[0m12:41:46.061436 [debug] [MainThread]: Parsing macros\cross_db_utils\datediff.sql
[0m12:41:46.068361 [debug] [MainThread]: Parsing macros\cross_db_utils\date_trunc.sql
[0m12:41:46.070773 [debug] [MainThread]: Parsing macros\cross_db_utils\escape_single_quotes.sql
[0m12:41:46.071782 [debug] [MainThread]: Parsing macros\cross_db_utils\except.sql
[0m12:41:46.072773 [debug] [MainThread]: Parsing macros\cross_db_utils\hash.sql
[0m12:41:46.073771 [debug] [MainThread]: Parsing macros\cross_db_utils\identifier.sql
[0m12:41:46.075770 [debug] [MainThread]: Parsing macros\cross_db_utils\intersect.sql
[0m12:41:46.075770 [debug] [MainThread]: Parsing macros\cross_db_utils\last_day.sql
[0m12:41:46.079339 [debug] [MainThread]: Parsing macros\cross_db_utils\length.sql
[0m12:41:46.080342 [debug] [MainThread]: Parsing macros\cross_db_utils\listagg.sql
[0m12:41:46.087339 [debug] [MainThread]: Parsing macros\cross_db_utils\literal.sql
[0m12:41:46.087339 [debug] [MainThread]: Parsing macros\cross_db_utils\position.sql
[0m12:41:46.089414 [debug] [MainThread]: Parsing macros\cross_db_utils\replace.sql
[0m12:41:46.090404 [debug] [MainThread]: Parsing macros\cross_db_utils\right.sql
[0m12:41:46.091450 [debug] [MainThread]: Parsing macros\cross_db_utils\safe_cast.sql
[0m12:41:46.093605 [debug] [MainThread]: Parsing macros\cross_db_utils\split_part.sql
[0m12:41:46.097619 [debug] [MainThread]: Parsing macros\cross_db_utils\width_bucket.sql
[0m12:41:46.101879 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_ephemeral.sql
[0m12:41:46.103880 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_relation.sql
[0m12:41:46.104921 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m12:41:46.106922 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m12:41:46.106922 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m12:41:46.109231 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m12:41:46.112236 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m12:41:46.113228 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m12:41:46.115235 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m12:41:46.116231 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m12:41:46.123559 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m12:41:46.124608 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m12:41:46.125678 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m12:41:46.128227 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m12:41:46.128284 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m12:41:46.130818 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m12:41:46.133817 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m12:41:46.134816 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m12:41:46.135364 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m12:41:46.138432 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m12:41:46.138432 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m12:41:46.139429 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m12:41:46.140429 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m12:41:46.141429 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m12:41:46.160758 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m12:41:46.163763 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m12:41:46.169322 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m12:41:46.172352 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m12:41:46.177353 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m12:41:46.179885 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m12:41:46.181878 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m12:41:46.184250 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m12:41:46.187241 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m12:41:46.191890 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m12:41:46.192892 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m12:41:46.194890 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m12:41:46.195889 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m12:41:46.200208 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m12:41:46.201174 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m12:41:46.203212 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m12:41:46.206209 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m12:41:46.207209 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m12:41:46.210808 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m12:41:46.213146 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m12:41:46.222211 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m12:41:46.228702 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m12:41:46.229679 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m12:41:46.231691 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m12:41:46.233702 [debug] [MainThread]: Parsing macros\add_dbt_source_relation.sql
[0m12:41:46.233702 [debug] [MainThread]: Parsing macros\add_pass_through_columns.sql
[0m12:41:46.236143 [debug] [MainThread]: Parsing macros\array_agg.sql
[0m12:41:46.237143 [debug] [MainThread]: Parsing macros\calculated_fields.sql
[0m12:41:46.237143 [debug] [MainThread]: Parsing macros\ceiling.sql
[0m12:41:46.238689 [debug] [MainThread]: Parsing macros\collect_freshness.sql
[0m12:41:46.241761 [debug] [MainThread]: Parsing macros\dummy_coalesce_value.sql
[0m12:41:46.244793 [debug] [MainThread]: Parsing macros\empty_variable_warning.sql
[0m12:41:46.245803 [debug] [MainThread]: Parsing macros\enabled_vars.sql
[0m12:41:46.246793 [debug] [MainThread]: Parsing macros\enabled_vars_one_true.sql
[0m12:41:46.246793 [debug] [MainThread]: Parsing macros\fill_pass_through_columns.sql
[0m12:41:46.248340 [debug] [MainThread]: Parsing macros\fill_staging_columns.sql
[0m12:41:46.252481 [debug] [MainThread]: Parsing macros\first_value.sql
[0m12:41:46.253483 [debug] [MainThread]: Parsing macros\json_extract.sql
[0m12:41:46.256484 [debug] [MainThread]: Parsing macros\json_parse.sql
[0m12:41:46.260752 [debug] [MainThread]: Parsing macros\max_bool.sql
[0m12:41:46.261750 [debug] [MainThread]: Parsing macros\percentile.sql
[0m12:41:46.264754 [debug] [MainThread]: Parsing macros\persist_pass_through_columns.sql
[0m12:41:46.265750 [debug] [MainThread]: Parsing macros\pivot_json_extract.sql
[0m12:41:46.266754 [debug] [MainThread]: Parsing macros\remove_prefix_from_columns.sql
[0m12:41:46.266754 [debug] [MainThread]: Parsing macros\seed_data_helper.sql
[0m12:41:46.269386 [debug] [MainThread]: Parsing macros\snowflake_seed_data.sql
[0m12:41:46.269386 [debug] [MainThread]: Parsing macros\source_relation.sql
[0m12:41:46.271385 [debug] [MainThread]: Parsing macros\string_agg.sql
[0m12:41:46.273723 [debug] [MainThread]: Parsing macros\timestamp_add.sql
[0m12:41:46.276719 [debug] [MainThread]: Parsing macros\timestamp_diff.sql
[0m12:41:46.284309 [debug] [MainThread]: Parsing macros\try_cast.sql
[0m12:41:46.287316 [debug] [MainThread]: Parsing macros\union_data.sql
[0m12:41:46.293363 [debug] [MainThread]: Parsing macros\union_relations.sql
[0m12:41:46.299962 [debug] [MainThread]: Parsing macros\get_issue_assignee_columns.sql
[0m12:41:46.300959 [debug] [MainThread]: Parsing macros\get_issue_closed_history_columns.sql
[0m12:41:46.301919 [debug] [MainThread]: Parsing macros\get_issue_columns.sql
[0m12:41:46.305171 [debug] [MainThread]: Parsing macros\get_issue_comment_columns.sql
[0m12:41:46.307204 [debug] [MainThread]: Parsing macros\get_issue_label_columns.sql
[0m12:41:46.308203 [debug] [MainThread]: Parsing macros\get_issue_merged_columns.sql
[0m12:41:46.308755 [debug] [MainThread]: Parsing macros\get_label_columns.sql
[0m12:41:46.310853 [debug] [MainThread]: Parsing macros\get_pull_request_columns.sql
[0m12:41:46.313853 [debug] [MainThread]: Parsing macros\get_pull_request_review_columns.sql
[0m12:41:46.314852 [debug] [MainThread]: Parsing macros\get_repository_columns.sql
[0m12:41:46.318166 [debug] [MainThread]: Parsing macros\get_repo_team_columns.sql
[0m12:41:46.319163 [debug] [MainThread]: Parsing macros\get_requested_reviewer_history_columns.sql
[0m12:41:46.320163 [debug] [MainThread]: Parsing macros\get_team_columns.sql
[0m12:41:46.322168 [debug] [MainThread]: Parsing macros\get_user_columns.sql
[0m12:41:46.746777 [debug] [MainThread]: 1699: static parser successfully parsed prod\agg_transactions.sql
[0m12:41:46.756321 [debug] [MainThread]: 1699: static parser successfully parsed prod\dim_customers.sql
[0m12:41:46.758819 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m12:41:46.762831 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m12:41:46.763831 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_customers.sql
[0m12:41:46.765864 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_orders.sql
[0m12:41:46.766865 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m12:41:46.771073 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m12:41:46.798189 [debug] [MainThread]: 1603: static parser failed on github__daily_metrics.sql
[0m12:41:46.806186 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__daily_metrics.sql
[0m12:41:46.807187 [debug] [MainThread]: 1603: static parser failed on github__issues.sql
[0m12:41:46.810729 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__issues.sql
[0m12:41:46.811727 [debug] [MainThread]: 1603: static parser failed on github__monthly_metrics.sql
[0m12:41:46.815215 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__monthly_metrics.sql
[0m12:41:46.816214 [debug] [MainThread]: 1699: static parser successfully parsed github__pull_requests.sql
[0m12:41:46.817179 [debug] [MainThread]: 1603: static parser failed on github__quarterly_metrics.sql
[0m12:41:46.822087 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__quarterly_metrics.sql
[0m12:41:46.822087 [debug] [MainThread]: 1603: static parser failed on github__weekly_metrics.sql
[0m12:41:46.826121 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__weekly_metrics.sql
[0m12:41:46.827087 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_assignees.sql
[0m12:41:46.834026 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_assignees.sql
[0m12:41:46.834026 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_comments.sql
[0m12:41:46.838679 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_comments.sql
[0m12:41:46.840012 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_joined.sql
[0m12:41:46.849688 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_joined.sql
[0m12:41:46.850728 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_labels.sql
[0m12:41:46.854735 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_labels.sql
[0m12:41:46.855733 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_label_joined.sql
[0m12:41:46.860207 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_label_joined.sql
[0m12:41:46.861207 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_open_length.sql
[0m12:41:46.869717 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_open_length.sql
[0m12:41:46.870718 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_reviewers.sql
[0m12:41:46.875229 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_reviewers.sql
[0m12:41:46.877229 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_times.sql
[0m12:41:46.886744 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_times.sql
[0m12:41:46.886744 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__repository_teams.sql
[0m12:41:46.894258 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__repository_teams.sql
[0m12:41:46.903272 [debug] [MainThread]: 1603: static parser failed on stg_github__issue.sql
[0m12:41:46.966488 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue.sql
[0m12:41:46.967488 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_assignee.sql
[0m12:41:46.975129 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_assignee.sql
[0m12:41:46.976147 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_closed_history.sql
[0m12:41:46.985154 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_closed_history.sql
[0m12:41:46.986154 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_comment.sql
[0m12:41:46.995668 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_comment.sql
[0m12:41:46.996668 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_label.sql
[0m12:41:47.004240 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_label.sql
[0m12:41:47.005240 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_merged.sql
[0m12:41:47.013245 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_merged.sql
[0m12:41:47.014246 [debug] [MainThread]: 1603: static parser failed on stg_github__label.sql
[0m12:41:47.023261 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__label.sql
[0m12:41:47.024267 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request.sql
[0m12:41:47.038296 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request.sql
[0m12:41:47.039297 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request_review.sql
[0m12:41:47.049807 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request_review.sql
[0m12:41:47.050808 [debug] [MainThread]: 1603: static parser failed on stg_github__repository.sql
[0m12:41:47.062834 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repository.sql
[0m12:41:47.064834 [debug] [MainThread]: 1603: static parser failed on stg_github__repo_team.sql
[0m12:41:47.072346 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repo_team.sql
[0m12:41:47.073345 [debug] [MainThread]: 1603: static parser failed on stg_github__requested_reviewer_history.sql
[0m12:41:47.081854 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__requested_reviewer_history.sql
[0m12:41:47.084369 [debug] [MainThread]: 1603: static parser failed on stg_github__team.sql
[0m12:41:47.094882 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__team.sql
[0m12:41:47.095882 [debug] [MainThread]: 1603: static parser failed on stg_github__user.sql
[0m12:41:47.102393 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__user.sql
[0m12:41:47.103394 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_assignee_tmp.sql
[0m12:41:47.107393 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_assignee_tmp.sql
[0m12:41:47.108898 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:41:47.111904 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:41:47.113409 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_comment_tmp.sql
[0m12:41:47.116415 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_comment_tmp.sql
[0m12:41:47.117415 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_label_tmp.sql
[0m12:41:47.121931 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_label_tmp.sql
[0m12:41:47.122932 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_merged_tmp.sql
[0m12:41:47.126931 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_merged_tmp.sql
[0m12:41:47.128437 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_tmp.sql
[0m12:41:47.132444 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_tmp.sql
[0m12:41:47.133444 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__label_tmp.sql
[0m12:41:47.137041 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__label_tmp.sql
[0m12:41:47.137041 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_review_tmp.sql
[0m12:41:47.141634 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_review_tmp.sql
[0m12:41:47.142668 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_tmp.sql
[0m12:41:47.146706 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_tmp.sql
[0m12:41:47.146706 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repository_tmp.sql
[0m12:41:47.152422 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repository_tmp.sql
[0m12:41:47.153423 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repo_team_tmp.sql
[0m12:41:47.156420 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repo_team_tmp.sql
[0m12:41:47.157693 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:41:47.161732 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:41:47.162732 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__team_tmp.sql
[0m12:41:47.165732 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__team_tmp.sql
[0m12:41:47.166732 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__user_tmp.sql
[0m12:41:47.170364 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__user_tmp.sql
[0m12:41:47.393050 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D117702190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1177E23A0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002D1177E2340>]}


============================== 2022-08-07 12:42:42.267127 | 842e5894-12f3-4c56-b806-843c8faf4697 ==============================
[0m12:42:42.268129 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:42:42.268636 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:42:42.268636 [debug] [MainThread]: Tracking: tracking
[0m12:42:42.280168 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A78E0986D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A78E0988B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A78E098640>]}
[0m12:42:42.319659 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m12:42:42.320664 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '842e5894-12f3-4c56-b806-843c8faf4697', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A78E0637C0>]}
[0m12:42:42.446748 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m12:42:42.448291 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:42:42.467096 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:42:42.471735 [debug] [MainThread]: Parsing macros\etc.sql
[0m12:42:42.473737 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:42:42.475742 [debug] [MainThread]: Parsing macros\materializations\copy.sql
[0m12:42:42.478275 [debug] [MainThread]: Parsing macros\materializations\incremental.sql
[0m12:42:42.491790 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:42:42.494301 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:42:42.495301 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:42:42.499810 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:42:42.501816 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:42:42.502815 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:42:42.502815 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:42:42.503815 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:42:42.504815 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:42:42.504815 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:42:42.505815 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:42:42.505815 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:42:42.506815 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:42:42.506815 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:42:42.506815 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:42:42.508320 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:42:42.508320 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:42:42.510326 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:42:42.520835 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:42:42.528850 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:42:42.530861 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:42:42.533860 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:42:42.539370 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:42:42.543369 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:42:42.555510 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:42:42.557510 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:42:42.564785 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:42:42.568286 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:42:42.568886 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:42:42.569926 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:42:42.569926 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:42:42.570930 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:42:42.571927 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:42:42.572897 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:42:42.574924 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:42:42.576896 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:42:42.580520 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:42:42.583559 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:42:42.591085 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:42:42.592085 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:42:42.604664 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:42:42.616811 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:42:42.619429 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:42:42.623470 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:42:42.625468 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:42:42.627444 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:42:42.629485 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:42:42.633496 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:42:42.648252 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:42:42.653755 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:42:42.663304 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:42:42.673952 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:42:42.674992 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:42:42.688253 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:42:42.690293 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:42:42.694295 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:42:42.695259 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:42:42.696295 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:42:42.697294 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:42:42.697294 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:42:42.698832 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:42:42.703412 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:42:42.704450 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:42:42.705420 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:42:42.706419 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:42:42.707424 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:42:42.708486 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:42:42.709502 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:42:42.709502 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:42:42.711513 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:42:42.712513 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:42:42.714513 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:42:42.715513 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:42:42.716513 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:42:42.717518 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:42:42.717518 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:42:42.718529 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:42:42.720529 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:42:42.723528 [debug] [MainThread]: Parsing macros\cross_db_utils\any_value.sql
[0m12:42:42.724529 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m12:42:42.726529 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m12:42:42.728523 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m12:42:42.730033 [debug] [MainThread]: Parsing macros\cross_db_utils\bool_or.sql
[0m12:42:42.732034 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m12:42:42.733538 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_bool_to_text.sql
[0m12:42:42.734544 [debug] [MainThread]: Parsing macros\cross_db_utils\concat.sql
[0m12:42:42.735544 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m12:42:42.739049 [debug] [MainThread]: Parsing macros\cross_db_utils\datatypes.sql
[0m12:42:42.744060 [debug] [MainThread]: Parsing macros\cross_db_utils\dateadd.sql
[0m12:42:42.746060 [debug] [MainThread]: Parsing macros\cross_db_utils\datediff.sql
[0m12:42:42.754642 [debug] [MainThread]: Parsing macros\cross_db_utils\date_trunc.sql
[0m12:42:42.755644 [debug] [MainThread]: Parsing macros\cross_db_utils\escape_single_quotes.sql
[0m12:42:42.756644 [debug] [MainThread]: Parsing macros\cross_db_utils\except.sql
[0m12:42:42.758247 [debug] [MainThread]: Parsing macros\cross_db_utils\hash.sql
[0m12:42:42.759286 [debug] [MainThread]: Parsing macros\cross_db_utils\identifier.sql
[0m12:42:42.760283 [debug] [MainThread]: Parsing macros\cross_db_utils\intersect.sql
[0m12:42:42.761291 [debug] [MainThread]: Parsing macros\cross_db_utils\last_day.sql
[0m12:42:42.764295 [debug] [MainThread]: Parsing macros\cross_db_utils\length.sql
[0m12:42:42.765294 [debug] [MainThread]: Parsing macros\cross_db_utils\listagg.sql
[0m12:42:42.771968 [debug] [MainThread]: Parsing macros\cross_db_utils\literal.sql
[0m12:42:42.772936 [debug] [MainThread]: Parsing macros\cross_db_utils\position.sql
[0m12:42:42.773967 [debug] [MainThread]: Parsing macros\cross_db_utils\replace.sql
[0m12:42:42.774967 [debug] [MainThread]: Parsing macros\cross_db_utils\right.sql
[0m12:42:42.776933 [debug] [MainThread]: Parsing macros\cross_db_utils\safe_cast.sql
[0m12:42:42.778477 [debug] [MainThread]: Parsing macros\cross_db_utils\split_part.sql
[0m12:42:42.782516 [debug] [MainThread]: Parsing macros\cross_db_utils\width_bucket.sql
[0m12:42:42.786525 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_ephemeral.sql
[0m12:42:42.788993 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_relation.sql
[0m12:42:42.790033 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m12:42:42.792001 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m12:42:42.792001 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m12:42:42.794587 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m12:42:42.797595 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m12:42:42.799101 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m12:42:42.800143 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m12:42:42.801150 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m12:42:42.807150 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m12:42:42.809733 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m12:42:42.810706 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m12:42:42.812728 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m12:42:42.813728 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m12:42:42.815729 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m12:42:42.818270 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m12:42:42.819345 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m12:42:42.820344 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m12:42:42.822746 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m12:42:42.823787 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m12:42:42.824790 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m12:42:42.825754 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m12:42:42.826756 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m12:42:42.845363 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m12:42:42.848897 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m12:42:42.854413 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m12:42:42.858413 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m12:42:42.862927 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m12:42:42.865040 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m12:42:42.867066 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m12:42:42.870139 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m12:42:42.873139 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m12:42:42.878700 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m12:42:42.879705 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m12:42:42.880705 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m12:42:42.881705 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m12:42:42.887221 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m12:42:42.888734 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m12:42:42.890759 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m12:42:42.893759 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m12:42:42.894755 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m12:42:42.898261 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m12:42:42.901267 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m12:42:42.910775 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m12:42:42.917287 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m12:42:42.918791 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m12:42:42.919797 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m12:42:42.921802 [debug] [MainThread]: Parsing macros\add_dbt_source_relation.sql
[0m12:42:42.922802 [debug] [MainThread]: Parsing macros\add_pass_through_columns.sql
[0m12:42:42.924802 [debug] [MainThread]: Parsing macros\array_agg.sql
[0m12:42:42.925803 [debug] [MainThread]: Parsing macros\calculated_fields.sql
[0m12:42:42.926802 [debug] [MainThread]: Parsing macros\ceiling.sql
[0m12:42:42.926802 [debug] [MainThread]: Parsing macros\collect_freshness.sql
[0m12:42:42.931313 [debug] [MainThread]: Parsing macros\dummy_coalesce_value.sql
[0m12:42:42.933314 [debug] [MainThread]: Parsing macros\empty_variable_warning.sql
[0m12:42:42.934314 [debug] [MainThread]: Parsing macros\enabled_vars.sql
[0m12:42:42.935313 [debug] [MainThread]: Parsing macros\enabled_vars_one_true.sql
[0m12:42:42.936314 [debug] [MainThread]: Parsing macros\fill_pass_through_columns.sql
[0m12:42:42.937314 [debug] [MainThread]: Parsing macros\fill_staging_columns.sql
[0m12:42:42.940822 [debug] [MainThread]: Parsing macros\first_value.sql
[0m12:42:42.943327 [debug] [MainThread]: Parsing macros\json_extract.sql
[0m12:42:42.945332 [debug] [MainThread]: Parsing macros\json_parse.sql
[0m12:42:42.950851 [debug] [MainThread]: Parsing macros\max_bool.sql
[0m12:42:42.951851 [debug] [MainThread]: Parsing macros\percentile.sql
[0m12:42:42.954850 [debug] [MainThread]: Parsing macros\persist_pass_through_columns.sql
[0m12:42:42.955851 [debug] [MainThread]: Parsing macros\pivot_json_extract.sql
[0m12:42:42.956850 [debug] [MainThread]: Parsing macros\remove_prefix_from_columns.sql
[0m12:42:42.958356 [debug] [MainThread]: Parsing macros\seed_data_helper.sql
[0m12:42:42.959362 [debug] [MainThread]: Parsing macros\snowflake_seed_data.sql
[0m12:42:42.960362 [debug] [MainThread]: Parsing macros\source_relation.sql
[0m12:42:42.962362 [debug] [MainThread]: Parsing macros\string_agg.sql
[0m12:42:42.964362 [debug] [MainThread]: Parsing macros\timestamp_add.sql
[0m12:42:42.967362 [debug] [MainThread]: Parsing macros\timestamp_diff.sql
[0m12:42:42.975452 [debug] [MainThread]: Parsing macros\try_cast.sql
[0m12:42:42.978957 [debug] [MainThread]: Parsing macros\union_data.sql
[0m12:42:42.983968 [debug] [MainThread]: Parsing macros\union_relations.sql
[0m12:42:42.991010 [debug] [MainThread]: Parsing macros\get_issue_assignee_columns.sql
[0m12:42:42.992013 [debug] [MainThread]: Parsing macros\get_issue_closed_history_columns.sql
[0m12:42:42.993979 [debug] [MainThread]: Parsing macros\get_issue_columns.sql
[0m12:42:42.996979 [debug] [MainThread]: Parsing macros\get_issue_comment_columns.sql
[0m12:42:42.998557 [debug] [MainThread]: Parsing macros\get_issue_label_columns.sql
[0m12:42:42.999563 [debug] [MainThread]: Parsing macros\get_issue_merged_columns.sql
[0m12:42:43.000562 [debug] [MainThread]: Parsing macros\get_label_columns.sql
[0m12:42:43.002563 [debug] [MainThread]: Parsing macros\get_pull_request_columns.sql
[0m12:42:43.005567 [debug] [MainThread]: Parsing macros\get_pull_request_review_columns.sql
[0m12:42:43.007567 [debug] [MainThread]: Parsing macros\get_repository_columns.sql
[0m12:42:43.010078 [debug] [MainThread]: Parsing macros\get_repo_team_columns.sql
[0m12:42:43.011083 [debug] [MainThread]: Parsing macros\get_requested_reviewer_history_columns.sql
[0m12:42:43.013082 [debug] [MainThread]: Parsing macros\get_team_columns.sql
[0m12:42:43.015082 [debug] [MainThread]: Parsing macros\get_user_columns.sql
[0m12:42:43.429465 [debug] [MainThread]: 1699: static parser successfully parsed prod\agg_transactions.sql
[0m12:42:43.439554 [debug] [MainThread]: 1699: static parser successfully parsed prod\dim_customers.sql
[0m12:42:43.441482 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m12:42:43.445478 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m12:42:43.446479 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_customers.sql
[0m12:42:43.448478 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_orders.sql
[0m12:42:43.450651 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m12:42:43.454660 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m12:42:43.479456 [debug] [MainThread]: 1603: static parser failed on github__daily_metrics.sql
[0m12:42:43.486501 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__daily_metrics.sql
[0m12:42:43.487501 [debug] [MainThread]: 1603: static parser failed on github__issues.sql
[0m12:42:43.491031 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__issues.sql
[0m12:42:43.492030 [debug] [MainThread]: 1603: static parser failed on github__monthly_metrics.sql
[0m12:42:43.494996 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__monthly_metrics.sql
[0m12:42:43.495996 [debug] [MainThread]: 1699: static parser successfully parsed github__pull_requests.sql
[0m12:42:43.498503 [debug] [MainThread]: 1603: static parser failed on github__quarterly_metrics.sql
[0m12:42:43.502509 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__quarterly_metrics.sql
[0m12:42:43.503509 [debug] [MainThread]: 1603: static parser failed on github__weekly_metrics.sql
[0m12:42:43.507510 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__weekly_metrics.sql
[0m12:42:43.509015 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_assignees.sql
[0m12:42:43.515532 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_assignees.sql
[0m12:42:43.516533 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_comments.sql
[0m12:42:43.521049 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_comments.sql
[0m12:42:43.522049 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_joined.sql
[0m12:42:43.532559 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_joined.sql
[0m12:42:43.533560 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_labels.sql
[0m12:42:43.537560 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_labels.sql
[0m12:42:43.539064 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_label_joined.sql
[0m12:42:43.543576 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_label_joined.sql
[0m12:42:43.544583 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_open_length.sql
[0m12:42:43.553100 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_open_length.sql
[0m12:42:43.554100 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_reviewers.sql
[0m12:42:43.558605 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_reviewers.sql
[0m12:42:43.559612 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_times.sql
[0m12:42:43.568611 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_times.sql
[0m12:42:43.569681 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__repository_teams.sql
[0m12:42:43.575693 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__repository_teams.sql
[0m12:42:43.585266 [debug] [MainThread]: 1603: static parser failed on stg_github__issue.sql
[0m12:42:43.646919 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue.sql
[0m12:42:43.648461 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_assignee.sql
[0m12:42:43.655500 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_assignee.sql
[0m12:42:43.656468 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_closed_history.sql
[0m12:42:43.664593 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_closed_history.sql
[0m12:42:43.665596 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_comment.sql
[0m12:42:43.674973 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_comment.sql
[0m12:42:43.675969 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_label.sql
[0m12:42:43.684522 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_label.sql
[0m12:42:43.684522 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_merged.sql
[0m12:42:43.691978 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_merged.sql
[0m12:42:43.693518 [debug] [MainThread]: 1603: static parser failed on stg_github__label.sql
[0m12:42:43.702989 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__label.sql
[0m12:42:43.702989 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request.sql
[0m12:42:43.716539 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request.sql
[0m12:42:43.717538 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request_review.sql
[0m12:42:43.727656 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request_review.sql
[0m12:42:43.728658 [debug] [MainThread]: 1603: static parser failed on stg_github__repository.sql
[0m12:42:43.740649 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repository.sql
[0m12:42:43.741648 [debug] [MainThread]: 1603: static parser failed on stg_github__repo_team.sql
[0m12:42:43.750054 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repo_team.sql
[0m12:42:43.751052 [debug] [MainThread]: 1603: static parser failed on stg_github__requested_reviewer_history.sql
[0m12:42:43.759998 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__requested_reviewer_history.sql
[0m12:42:43.761005 [debug] [MainThread]: 1603: static parser failed on stg_github__team.sql
[0m12:42:43.771158 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__team.sql
[0m12:42:43.771158 [debug] [MainThread]: 1603: static parser failed on stg_github__user.sql
[0m12:42:43.779722 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__user.sql
[0m12:42:43.779722 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_assignee_tmp.sql
[0m12:42:43.784246 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_assignee_tmp.sql
[0m12:42:43.785246 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:42:43.788760 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:42:43.789775 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_comment_tmp.sql
[0m12:42:43.792786 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_comment_tmp.sql
[0m12:42:43.793786 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_label_tmp.sql
[0m12:42:43.798300 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_label_tmp.sql
[0m12:42:43.799312 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_merged_tmp.sql
[0m12:42:43.803312 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_merged_tmp.sql
[0m12:42:43.804312 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_tmp.sql
[0m12:42:43.807307 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_tmp.sql
[0m12:42:43.808307 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__label_tmp.sql
[0m12:42:43.811819 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__label_tmp.sql
[0m12:42:43.813325 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_review_tmp.sql
[0m12:42:43.817332 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_review_tmp.sql
[0m12:42:43.818837 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_tmp.sql
[0m12:42:43.821850 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_tmp.sql
[0m12:42:43.823850 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repository_tmp.sql
[0m12:42:43.828355 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repository_tmp.sql
[0m12:42:43.829362 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repo_team_tmp.sql
[0m12:42:43.833392 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repo_team_tmp.sql
[0m12:42:43.834395 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:42:43.837394 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:42:43.838365 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__team_tmp.sql
[0m12:42:43.841514 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__team_tmp.sql
[0m12:42:43.842518 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__user_tmp.sql
[0m12:42:43.846559 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__user_tmp.sql
[0m12:42:44.066748 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A78F3FBE80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A78F4FE7F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001A78F4FEA90>]}


============================== 2022-08-07 12:43:56.731410 | eb21ad19-29a5-418b-9a98-4c7b722be25d ==============================
[0m12:43:56.731410 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:43:56.732412 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:43:56.732412 [debug] [MainThread]: Tracking: tracking
[0m12:43:56.743509 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD4D271C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD4D278B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD4D277F0>]}
[0m12:43:56.780808 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m12:43:56.781809 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'eb21ad19-29a5-418b-9a98-4c7b722be25d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD4D275B0>]}
[0m12:43:56.898352 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m12:43:56.899681 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:43:56.918267 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:43:56.923331 [debug] [MainThread]: Parsing macros\etc.sql
[0m12:43:56.924336 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:43:56.927332 [debug] [MainThread]: Parsing macros\materializations\copy.sql
[0m12:43:56.928881 [debug] [MainThread]: Parsing macros\materializations\incremental.sql
[0m12:43:56.942550 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:43:56.944548 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:43:56.945551 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:43:56.949091 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:43:56.951132 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:43:56.952098 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:43:56.952098 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:43:56.953233 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:43:56.953233 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:43:56.954265 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:43:56.954265 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:43:56.955260 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:43:56.955260 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:43:56.956267 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:43:56.956267 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:43:56.957265 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:43:56.957265 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:43:56.958805 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:43:56.969224 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:43:56.977227 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:43:56.979320 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:43:56.981311 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:43:56.987350 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:43:56.991265 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:43:57.003251 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:43:57.004356 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:43:57.010965 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:43:57.014498 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:43:57.015490 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:43:57.016995 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:43:57.016995 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:43:57.016995 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:43:57.018579 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:43:57.019643 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:43:57.021682 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:43:57.023681 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:43:57.026656 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:43:57.030689 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:43:57.038275 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:43:57.039311 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:43:57.052224 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:43:57.066773 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:43:57.069451 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:43:57.073767 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:43:57.076739 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:43:57.078280 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:43:57.079779 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:43:57.083786 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:43:57.098261 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:43:57.103771 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:43:57.112778 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:43:57.122298 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:43:57.123297 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:43:57.137202 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:43:57.138233 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:43:57.142235 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:43:57.143238 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:43:57.144240 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:43:57.144240 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:43:57.145201 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:43:57.146199 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:43:57.150723 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:43:57.151723 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:43:57.152748 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:43:57.153745 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:43:57.154718 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:43:57.155753 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:43:57.156755 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:43:57.156755 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:43:57.158295 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:43:57.159362 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:43:57.161329 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:43:57.162333 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:43:57.163350 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:43:57.164349 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:43:57.165346 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:43:57.166346 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:43:57.167373 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:43:57.170288 [debug] [MainThread]: Parsing macros\cross_db_utils\any_value.sql
[0m12:43:57.171329 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m12:43:57.173329 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m12:43:57.174326 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m12:43:57.176295 [debug] [MainThread]: Parsing macros\cross_db_utils\bool_or.sql
[0m12:43:57.177302 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m12:43:57.180340 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_bool_to_text.sql
[0m12:43:57.181340 [debug] [MainThread]: Parsing macros\cross_db_utils\concat.sql
[0m12:43:57.182349 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m12:43:57.184307 [debug] [MainThread]: Parsing macros\cross_db_utils\datatypes.sql
[0m12:43:57.189819 [debug] [MainThread]: Parsing macros\cross_db_utils\dateadd.sql
[0m12:43:57.191818 [debug] [MainThread]: Parsing macros\cross_db_utils\datediff.sql
[0m12:43:57.199707 [debug] [MainThread]: Parsing macros\cross_db_utils\date_trunc.sql
[0m12:43:57.200752 [debug] [MainThread]: Parsing macros\cross_db_utils\escape_single_quotes.sql
[0m12:43:57.202746 [debug] [MainThread]: Parsing macros\cross_db_utils\except.sql
[0m12:43:57.203715 [debug] [MainThread]: Parsing macros\cross_db_utils\hash.sql
[0m12:43:57.204750 [debug] [MainThread]: Parsing macros\cross_db_utils\identifier.sql
[0m12:43:57.205754 [debug] [MainThread]: Parsing macros\cross_db_utils\intersect.sql
[0m12:43:57.206747 [debug] [MainThread]: Parsing macros\cross_db_utils\last_day.sql
[0m12:43:57.209239 [debug] [MainThread]: Parsing macros\cross_db_utils\length.sql
[0m12:43:57.210272 [debug] [MainThread]: Parsing macros\cross_db_utils\listagg.sql
[0m12:43:57.217246 [debug] [MainThread]: Parsing macros\cross_db_utils\literal.sql
[0m12:43:57.218352 [debug] [MainThread]: Parsing macros\cross_db_utils\position.sql
[0m12:43:57.219362 [debug] [MainThread]: Parsing macros\cross_db_utils\replace.sql
[0m12:43:57.220365 [debug] [MainThread]: Parsing macros\cross_db_utils\right.sql
[0m12:43:57.221355 [debug] [MainThread]: Parsing macros\cross_db_utils\safe_cast.sql
[0m12:43:57.223373 [debug] [MainThread]: Parsing macros\cross_db_utils\split_part.sql
[0m12:43:57.228376 [debug] [MainThread]: Parsing macros\cross_db_utils\width_bucket.sql
[0m12:43:57.231946 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_ephemeral.sql
[0m12:43:57.233952 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_relation.sql
[0m12:43:57.234942 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m12:43:57.236943 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m12:43:57.236943 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m12:43:57.239505 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m12:43:57.242478 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m12:43:57.244507 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m12:43:57.245506 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m12:43:57.246512 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m12:43:57.253501 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m12:43:57.255541 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m12:43:57.256543 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m12:43:57.257540 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m12:43:57.259699 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m12:43:57.260739 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m12:43:57.263738 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m12:43:57.264739 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m12:43:57.265740 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m12:43:57.267818 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m12:43:57.268857 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m12:43:57.269857 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m12:43:57.270826 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m12:43:57.271858 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m12:43:57.290948 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m12:43:57.294945 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m12:43:57.300477 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m12:43:57.304477 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m12:43:57.308981 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m12:43:57.310986 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m12:43:57.313491 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m12:43:57.315497 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m12:43:57.318506 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m12:43:57.323532 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m12:43:57.325533 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m12:43:57.326533 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m12:43:57.327533 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m12:43:57.332538 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m12:43:57.333539 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m12:43:57.335538 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m12:43:57.338538 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m12:43:57.340048 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m12:43:57.342047 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m12:43:57.345557 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m12:43:57.355070 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m12:43:57.360579 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m12:43:57.362580 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m12:43:57.363580 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m12:43:57.366579 [debug] [MainThread]: Parsing macros\add_dbt_source_relation.sql
[0m12:43:57.366579 [debug] [MainThread]: Parsing macros\add_pass_through_columns.sql
[0m12:43:57.368579 [debug] [MainThread]: Parsing macros\array_agg.sql
[0m12:43:57.369083 [debug] [MainThread]: Parsing macros\calculated_fields.sql
[0m12:43:57.370089 [debug] [MainThread]: Parsing macros\ceiling.sql
[0m12:43:57.371089 [debug] [MainThread]: Parsing macros\collect_freshness.sql
[0m12:43:57.374599 [debug] [MainThread]: Parsing macros\dummy_coalesce_value.sql
[0m12:43:57.376599 [debug] [MainThread]: Parsing macros\empty_variable_warning.sql
[0m12:43:57.377599 [debug] [MainThread]: Parsing macros\enabled_vars.sql
[0m12:43:57.379104 [debug] [MainThread]: Parsing macros\enabled_vars_one_true.sql
[0m12:43:57.380109 [debug] [MainThread]: Parsing macros\fill_pass_through_columns.sql
[0m12:43:57.381114 [debug] [MainThread]: Parsing macros\fill_staging_columns.sql
[0m12:43:57.384114 [debug] [MainThread]: Parsing macros\first_value.sql
[0m12:43:57.386114 [debug] [MainThread]: Parsing macros\json_extract.sql
[0m12:43:57.388619 [debug] [MainThread]: Parsing macros\json_parse.sql
[0m12:43:57.393625 [debug] [MainThread]: Parsing macros\max_bool.sql
[0m12:43:57.394626 [debug] [MainThread]: Parsing macros\percentile.sql
[0m12:43:57.397626 [debug] [MainThread]: Parsing macros\persist_pass_through_columns.sql
[0m12:43:57.398662 [debug] [MainThread]: Parsing macros\pivot_json_extract.sql
[0m12:43:57.399834 [debug] [MainThread]: Parsing macros\remove_prefix_from_columns.sql
[0m12:43:57.400827 [debug] [MainThread]: Parsing macros\seed_data_helper.sql
[0m12:43:57.401829 [debug] [MainThread]: Parsing macros\snowflake_seed_data.sql
[0m12:43:57.403369 [debug] [MainThread]: Parsing macros\source_relation.sql
[0m12:43:57.405408 [debug] [MainThread]: Parsing macros\string_agg.sql
[0m12:43:57.407408 [debug] [MainThread]: Parsing macros\timestamp_add.sql
[0m12:43:57.410006 [debug] [MainThread]: Parsing macros\timestamp_diff.sql
[0m12:43:57.417552 [debug] [MainThread]: Parsing macros\try_cast.sql
[0m12:43:57.420611 [debug] [MainThread]: Parsing macros\union_data.sql
[0m12:43:57.425612 [debug] [MainThread]: Parsing macros\union_relations.sql
[0m12:43:57.432167 [debug] [MainThread]: Parsing macros\get_issue_assignee_columns.sql
[0m12:43:57.433534 [debug] [MainThread]: Parsing macros\get_issue_closed_history_columns.sql
[0m12:43:57.435573 [debug] [MainThread]: Parsing macros\get_issue_columns.sql
[0m12:43:57.438575 [debug] [MainThread]: Parsing macros\get_issue_comment_columns.sql
[0m12:43:57.439752 [debug] [MainThread]: Parsing macros\get_issue_label_columns.sql
[0m12:43:57.440792 [debug] [MainThread]: Parsing macros\get_issue_merged_columns.sql
[0m12:43:57.441798 [debug] [MainThread]: Parsing macros\get_label_columns.sql
[0m12:43:57.443794 [debug] [MainThread]: Parsing macros\get_pull_request_columns.sql
[0m12:43:57.446793 [debug] [MainThread]: Parsing macros\get_pull_request_review_columns.sql
[0m12:43:57.448335 [debug] [MainThread]: Parsing macros\get_repository_columns.sql
[0m12:43:57.451377 [debug] [MainThread]: Parsing macros\get_repo_team_columns.sql
[0m12:43:57.452374 [debug] [MainThread]: Parsing macros\get_requested_reviewer_history_columns.sql
[0m12:43:57.453375 [debug] [MainThread]: Parsing macros\get_team_columns.sql
[0m12:43:57.455374 [debug] [MainThread]: Parsing macros\get_user_columns.sql
[0m12:43:57.858296 [debug] [MainThread]: 1699: static parser successfully parsed prod\agg_transactions.sql
[0m12:43:57.869146 [debug] [MainThread]: 1699: static parser successfully parsed prod\dim_customers.sql
[0m12:43:57.870179 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m12:43:57.874147 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m12:43:57.875178 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_customers.sql
[0m12:43:57.877146 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_orders.sql
[0m12:43:57.879709 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m12:43:57.883715 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m12:43:57.910316 [debug] [MainThread]: 1603: static parser failed on github__daily_metrics.sql
[0m12:43:57.917826 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__daily_metrics.sql
[0m12:43:57.918445 [debug] [MainThread]: 1603: static parser failed on github__issues.sql
[0m12:43:57.921456 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__issues.sql
[0m12:43:57.922457 [debug] [MainThread]: 1603: static parser failed on github__monthly_metrics.sql
[0m12:43:57.926457 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__monthly_metrics.sql
[0m12:43:57.927456 [debug] [MainThread]: 1699: static parser successfully parsed github__pull_requests.sql
[0m12:43:57.928462 [debug] [MainThread]: 1603: static parser failed on github__quarterly_metrics.sql
[0m12:43:57.932462 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__quarterly_metrics.sql
[0m12:43:57.933462 [debug] [MainThread]: 1603: static parser failed on github__weekly_metrics.sql
[0m12:43:57.937462 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__weekly_metrics.sql
[0m12:43:57.938462 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_assignees.sql
[0m12:43:57.945508 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_assignees.sql
[0m12:43:57.946507 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_comments.sql
[0m12:43:57.949767 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_comments.sql
[0m12:43:57.951778 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_joined.sql
[0m12:43:57.961304 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_joined.sql
[0m12:43:57.962304 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_labels.sql
[0m12:43:57.967303 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_labels.sql
[0m12:43:57.968304 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_label_joined.sql
[0m12:43:57.972733 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_label_joined.sql
[0m12:43:57.973771 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_open_length.sql
[0m12:43:57.982777 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_open_length.sql
[0m12:43:57.983774 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_reviewers.sql
[0m12:43:57.988310 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_reviewers.sql
[0m12:43:57.989309 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_times.sql
[0m12:43:57.997446 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_times.sql
[0m12:43:57.998595 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__repository_teams.sql
[0m12:43:58.004639 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__repository_teams.sql
[0m12:43:58.014861 [debug] [MainThread]: 1603: static parser failed on stg_github__issue.sql
[0m12:43:58.074808 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue.sql
[0m12:43:58.075808 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_assignee.sql
[0m12:43:58.083316 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_assignee.sql
[0m12:43:58.084316 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_closed_history.sql
[0m12:43:58.093332 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_closed_history.sql
[0m12:43:58.094339 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_comment.sql
[0m12:43:58.103853 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_comment.sql
[0m12:43:58.104853 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_label.sql
[0m12:43:58.113365 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_label.sql
[0m12:43:58.114364 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_merged.sql
[0m12:43:58.122495 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_merged.sql
[0m12:43:58.123501 [debug] [MainThread]: 1603: static parser failed on stg_github__label.sql
[0m12:43:58.132016 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__label.sql
[0m12:43:58.133016 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request.sql
[0m12:43:58.146527 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request.sql
[0m12:43:58.147527 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request_review.sql
[0m12:43:58.157548 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request_review.sql
[0m12:43:58.159054 [debug] [MainThread]: 1603: static parser failed on stg_github__repository.sql
[0m12:43:58.170578 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repository.sql
[0m12:43:58.171578 [debug] [MainThread]: 1603: static parser failed on stg_github__repo_team.sql
[0m12:43:58.180136 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repo_team.sql
[0m12:43:58.181169 [debug] [MainThread]: 1603: static parser failed on stg_github__requested_reviewer_history.sql
[0m12:43:58.190356 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__requested_reviewer_history.sql
[0m12:43:58.191398 [debug] [MainThread]: 1603: static parser failed on stg_github__team.sql
[0m12:43:58.201795 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__team.sql
[0m12:43:58.202803 [debug] [MainThread]: 1603: static parser failed on stg_github__user.sql
[0m12:43:58.210402 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__user.sql
[0m12:43:58.211410 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_assignee_tmp.sql
[0m12:43:58.215831 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_assignee_tmp.sql
[0m12:43:58.216827 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:43:58.219722 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:43:58.220763 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_comment_tmp.sql
[0m12:43:58.224763 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_comment_tmp.sql
[0m12:43:58.225763 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_label_tmp.sql
[0m12:43:58.230285 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_label_tmp.sql
[0m12:43:58.231294 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_merged_tmp.sql
[0m12:43:58.234280 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_merged_tmp.sql
[0m12:43:58.235277 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_tmp.sql
[0m12:43:58.238819 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_tmp.sql
[0m12:43:58.239859 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__label_tmp.sql
[0m12:43:58.243401 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__label_tmp.sql
[0m12:43:58.244443 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_review_tmp.sql
[0m12:43:58.247440 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_review_tmp.sql
[0m12:43:58.248439 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_tmp.sql
[0m12:43:58.252049 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_tmp.sql
[0m12:43:58.253049 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repository_tmp.sql
[0m12:43:58.257040 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repository_tmp.sql
[0m12:43:58.258347 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repo_team_tmp.sql
[0m12:43:58.262352 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repo_team_tmp.sql
[0m12:43:58.263346 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:43:58.267350 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:43:58.268352 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__team_tmp.sql
[0m12:43:58.271933 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__team_tmp.sql
[0m12:43:58.273232 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__user_tmp.sql
[0m12:43:58.276244 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__user_tmp.sql
[0m12:43:58.534438 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'eb21ad19-29a5-418b-9a98-4c7b722be25d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD62E1F70>]}
[0m12:43:58.596732 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'eb21ad19-29a5-418b-9a98-4c7b722be25d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD62B4DF0>]}
[0m12:43:58.598237 [info ] [MainThread]: Found 49 models, 41 tests, 0 snapshots, 0 analyses, 615 macros, 0 operations, 0 seed files, 17 sources, 1 exposure, 0 metrics
[0m12:43:58.598237 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'eb21ad19-29a5-418b-9a98-4c7b722be25d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD62E1D90>]}
[0m12:43:58.602268 [info ] [MainThread]: 
[0m12:43:58.603273 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:43:58.605274 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m12:43:58.606274 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:43:59.208374 [debug] [ThreadPool]: Acquiring new bigquery connection "create_airflow-docker-352518_dbt-x-airflow"
[0m12:43:59.208374 [debug] [ThreadPool]: Acquiring new bigquery connection "create_airflow-docker-352518_dbt-x-airflow"
[0m12:43:59.208374 [debug] [ThreadPool]: BigQuery adapter: Creating schema "airflow-docker-352518.dbt-x-airflow".
[0m12:43:59.208374 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:43:59.639046 [debug] [ThreadPool]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('POST https://bigquery.googleapis.com/bigquery/v2/projects/airflow-docker-352518/datasets?prettyPrint=false: Invalid dataset ID "dbt-x-airflow". Dataset IDs must be alphanumeric (plus underscores) and must be at most 1024 characters long.')
[0m12:44:00.603410 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:44:00.603410 [debug] [MainThread]: Connection 'create_airflow-docker-352518_dbt-x-airflow' was properly closed.
[0m12:44:00.603410 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD6296E50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD62964F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020FD62949D0>]}


============================== 2022-08-07 12:45:44.330004 | 904a873b-138e-4422-8963-0ec851418d17 ==============================
[0m12:45:44.330004 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:45:44.331012 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:45:44.331012 [debug] [MainThread]: Tracking: tracking
[0m12:45:44.343441 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206943BAA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206943BA970>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206943BA370>]}
[0m12:45:44.409920 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m12:45:44.410920 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206943C5BE0>]}
[0m12:45:44.518268 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m12:45:44.519271 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:45:44.538441 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:45:44.542064 [debug] [MainThread]: Parsing macros\etc.sql
[0m12:45:44.544066 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:45:44.546030 [debug] [MainThread]: Parsing macros\materializations\copy.sql
[0m12:45:44.548289 [debug] [MainThread]: Parsing macros\materializations\incremental.sql
[0m12:45:44.561505 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:45:44.563478 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:45:44.564479 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:45:44.568505 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:45:44.570305 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:45:44.571305 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:45:44.571305 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:45:44.572279 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:45:44.573270 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:45:44.573270 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:45:44.574304 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:45:44.574304 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:45:44.574304 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:45:44.575306 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:45:44.575306 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:45:44.576306 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:45:44.576306 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:45:44.578312 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:45:44.588862 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:45:44.595830 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:45:44.598587 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:45:44.600669 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:45:44.606673 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:45:44.609918 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:45:44.621536 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:45:44.622860 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:45:44.629542 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:45:44.633545 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:45:44.634581 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:45:44.635549 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:45:44.635549 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:45:44.636548 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:45:44.637548 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:45:44.638553 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:45:44.640553 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:45:44.642587 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:45:44.646553 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:45:44.649764 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:45:44.656805 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:45:44.658350 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:45:44.670856 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:45:44.682904 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:45:44.684942 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:45:44.688483 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:45:44.692805 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:45:44.694805 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:45:44.695805 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:45:44.700363 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:45:44.714158 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:45:44.718700 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:45:44.729239 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:45:44.738271 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:45:44.739853 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:45:44.753034 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:45:44.754062 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:45:44.758317 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:45:44.759283 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:45:44.760320 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:45:44.761285 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:45:44.761285 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:45:44.762316 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:45:44.767291 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:45:44.768853 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:45:44.769894 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:45:44.770862 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:45:44.771896 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:45:44.771896 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:45:44.773342 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:45:44.774306 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:45:44.776314 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:45:44.776314 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:45:44.778316 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:45:44.779819 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:45:44.779819 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:45:44.780859 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:45:44.781859 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:45:44.782858 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:45:44.784861 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:45:44.786856 [debug] [MainThread]: Parsing macros\cross_db_utils\any_value.sql
[0m12:45:44.788258 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m12:45:44.790255 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m12:45:44.791254 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m12:45:44.793222 [debug] [MainThread]: Parsing macros\cross_db_utils\bool_or.sql
[0m12:45:44.794221 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m12:45:44.796860 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_bool_to_text.sql
[0m12:45:44.798471 [debug] [MainThread]: Parsing macros\cross_db_utils\concat.sql
[0m12:45:44.798471 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m12:45:44.801474 [debug] [MainThread]: Parsing macros\cross_db_utils\datatypes.sql
[0m12:45:44.806552 [debug] [MainThread]: Parsing macros\cross_db_utils\dateadd.sql
[0m12:45:44.808553 [debug] [MainThread]: Parsing macros\cross_db_utils\datediff.sql
[0m12:45:44.816737 [debug] [MainThread]: Parsing macros\cross_db_utils\date_trunc.sql
[0m12:45:44.818114 [debug] [MainThread]: Parsing macros\cross_db_utils\escape_single_quotes.sql
[0m12:45:44.819153 [debug] [MainThread]: Parsing macros\cross_db_utils\except.sql
[0m12:45:44.820154 [debug] [MainThread]: Parsing macros\cross_db_utils\hash.sql
[0m12:45:44.821154 [debug] [MainThread]: Parsing macros\cross_db_utils\identifier.sql
[0m12:45:44.822155 [debug] [MainThread]: Parsing macros\cross_db_utils\intersect.sql
[0m12:45:44.823155 [debug] [MainThread]: Parsing macros\cross_db_utils\last_day.sql
[0m12:45:44.826151 [debug] [MainThread]: Parsing macros\cross_db_utils\length.sql
[0m12:45:44.827154 [debug] [MainThread]: Parsing macros\cross_db_utils\listagg.sql
[0m12:45:44.833874 [debug] [MainThread]: Parsing macros\cross_db_utils\literal.sql
[0m12:45:44.834871 [debug] [MainThread]: Parsing macros\cross_db_utils\position.sql
[0m12:45:44.835971 [debug] [MainThread]: Parsing macros\cross_db_utils\replace.sql
[0m12:45:44.837007 [debug] [MainThread]: Parsing macros\cross_db_utils\right.sql
[0m12:45:44.838569 [debug] [MainThread]: Parsing macros\cross_db_utils\safe_cast.sql
[0m12:45:44.840808 [debug] [MainThread]: Parsing macros\cross_db_utils\split_part.sql
[0m12:45:44.844774 [debug] [MainThread]: Parsing macros\cross_db_utils\width_bucket.sql
[0m12:45:44.849294 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_ephemeral.sql
[0m12:45:44.851333 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_relation.sql
[0m12:45:44.852325 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m12:45:44.854323 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m12:45:44.855328 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m12:45:44.856289 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m12:45:44.859474 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m12:45:44.861472 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m12:45:44.862475 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m12:45:44.863480 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m12:45:44.871150 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m12:45:44.872151 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m12:45:44.873118 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m12:45:44.875149 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m12:45:44.876150 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m12:45:44.878410 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m12:45:44.880408 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m12:45:44.881411 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m12:45:44.883408 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m12:45:44.885409 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m12:45:44.886409 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m12:45:44.887408 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m12:45:44.888413 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m12:45:44.888413 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m12:45:44.907204 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m12:45:44.910784 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m12:45:44.916754 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m12:45:44.919363 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m12:45:44.924411 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m12:45:44.926408 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m12:45:44.928377 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m12:45:44.931477 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m12:45:44.934473 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m12:45:44.938871 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m12:45:44.939870 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m12:45:44.941875 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m12:45:44.942869 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m12:45:44.946835 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m12:45:44.948373 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m12:45:44.950430 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m12:45:44.953956 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m12:45:44.954954 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m12:45:44.958492 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m12:45:44.960791 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m12:45:44.969319 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m12:45:44.975326 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m12:45:44.977287 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m12:45:44.978319 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m12:45:44.980479 [debug] [MainThread]: Parsing macros\add_dbt_source_relation.sql
[0m12:45:44.981472 [debug] [MainThread]: Parsing macros\add_pass_through_columns.sql
[0m12:45:44.982887 [debug] [MainThread]: Parsing macros\array_agg.sql
[0m12:45:44.983927 [debug] [MainThread]: Parsing macros\calculated_fields.sql
[0m12:45:44.983927 [debug] [MainThread]: Parsing macros\ceiling.sql
[0m12:45:44.984925 [debug] [MainThread]: Parsing macros\collect_freshness.sql
[0m12:45:44.988470 [debug] [MainThread]: Parsing macros\dummy_coalesce_value.sql
[0m12:45:44.990580 [debug] [MainThread]: Parsing macros\empty_variable_warning.sql
[0m12:45:44.991576 [debug] [MainThread]: Parsing macros\enabled_vars.sql
[0m12:45:44.992578 [debug] [MainThread]: Parsing macros\enabled_vars_one_true.sql
[0m12:45:44.993574 [debug] [MainThread]: Parsing macros\fill_pass_through_columns.sql
[0m12:45:44.994545 [debug] [MainThread]: Parsing macros\fill_staging_columns.sql
[0m12:45:44.998522 [debug] [MainThread]: Parsing macros\first_value.sql
[0m12:45:45.000518 [debug] [MainThread]: Parsing macros\json_extract.sql
[0m12:45:45.002544 [debug] [MainThread]: Parsing macros\json_parse.sql
[0m12:45:45.007546 [debug] [MainThread]: Parsing macros\max_bool.sql
[0m12:45:45.009060 [debug] [MainThread]: Parsing macros\percentile.sql
[0m12:45:45.012099 [debug] [MainThread]: Parsing macros\persist_pass_through_columns.sql
[0m12:45:45.012099 [debug] [MainThread]: Parsing macros\pivot_json_extract.sql
[0m12:45:45.013486 [debug] [MainThread]: Parsing macros\remove_prefix_from_columns.sql
[0m12:45:45.014488 [debug] [MainThread]: Parsing macros\seed_data_helper.sql
[0m12:45:45.015477 [debug] [MainThread]: Parsing macros\snowflake_seed_data.sql
[0m12:45:45.016484 [debug] [MainThread]: Parsing macros\source_relation.sql
[0m12:45:45.019006 [debug] [MainThread]: Parsing macros\string_agg.sql
[0m12:45:45.021054 [debug] [MainThread]: Parsing macros\timestamp_add.sql
[0m12:45:45.024076 [debug] [MainThread]: Parsing macros\timestamp_diff.sql
[0m12:45:45.031573 [debug] [MainThread]: Parsing macros\try_cast.sql
[0m12:45:45.034601 [debug] [MainThread]: Parsing macros\union_data.sql
[0m12:45:45.040185 [debug] [MainThread]: Parsing macros\union_relations.sql
[0m12:45:45.046730 [debug] [MainThread]: Parsing macros\get_issue_assignee_columns.sql
[0m12:45:45.047731 [debug] [MainThread]: Parsing macros\get_issue_closed_history_columns.sql
[0m12:45:45.049847 [debug] [MainThread]: Parsing macros\get_issue_columns.sql
[0m12:45:45.052888 [debug] [MainThread]: Parsing macros\get_issue_comment_columns.sql
[0m12:45:45.053886 [debug] [MainThread]: Parsing macros\get_issue_label_columns.sql
[0m12:45:45.054888 [debug] [MainThread]: Parsing macros\get_issue_merged_columns.sql
[0m12:45:45.056886 [debug] [MainThread]: Parsing macros\get_label_columns.sql
[0m12:45:45.058397 [debug] [MainThread]: Parsing macros\get_pull_request_columns.sql
[0m12:45:45.060398 [debug] [MainThread]: Parsing macros\get_pull_request_review_columns.sql
[0m12:45:45.062403 [debug] [MainThread]: Parsing macros\get_repository_columns.sql
[0m12:45:45.065365 [debug] [MainThread]: Parsing macros\get_repo_team_columns.sql
[0m12:45:45.066397 [debug] [MainThread]: Parsing macros\get_requested_reviewer_history_columns.sql
[0m12:45:45.067398 [debug] [MainThread]: Parsing macros\get_team_columns.sql
[0m12:45:45.069977 [debug] [MainThread]: Parsing macros\get_user_columns.sql
[0m12:45:45.465224 [debug] [MainThread]: 1699: static parser successfully parsed prod\agg_transactions.sql
[0m12:45:45.474872 [debug] [MainThread]: 1699: static parser successfully parsed prod\dim_customers.sql
[0m12:45:45.476870 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m12:45:45.481380 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m12:45:45.482419 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_customers.sql
[0m12:45:45.484411 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_orders.sql
[0m12:45:45.486418 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m12:45:45.490722 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m12:45:45.549479 [debug] [MainThread]: 1603: static parser failed on github__daily_metrics.sql
[0m12:45:45.557483 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__daily_metrics.sql
[0m12:45:45.559018 [debug] [MainThread]: 1603: static parser failed on github__issues.sql
[0m12:45:45.562104 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__issues.sql
[0m12:45:45.563105 [debug] [MainThread]: 1603: static parser failed on github__monthly_metrics.sql
[0m12:45:45.567071 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__monthly_metrics.sql
[0m12:45:45.568211 [debug] [MainThread]: 1699: static parser successfully parsed github__pull_requests.sql
[0m12:45:45.570252 [debug] [MainThread]: 1603: static parser failed on github__quarterly_metrics.sql
[0m12:45:45.573250 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__quarterly_metrics.sql
[0m12:45:45.574252 [debug] [MainThread]: 1603: static parser failed on github__weekly_metrics.sql
[0m12:45:45.577219 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__weekly_metrics.sql
[0m12:45:45.578760 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_assignees.sql
[0m12:45:45.586272 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_assignees.sql
[0m12:45:45.587241 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_comments.sql
[0m12:45:45.590845 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_comments.sql
[0m12:45:45.591846 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_joined.sql
[0m12:45:45.600831 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_joined.sql
[0m12:45:45.602832 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_labels.sql
[0m12:45:45.606796 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_labels.sql
[0m12:45:45.606796 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_label_joined.sql
[0m12:45:45.612366 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_label_joined.sql
[0m12:45:45.613420 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_open_length.sql
[0m12:45:45.621746 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_open_length.sql
[0m12:45:45.622743 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_reviewers.sql
[0m12:45:45.627841 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_reviewers.sql
[0m12:45:45.629880 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_times.sql
[0m12:45:45.639466 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_times.sql
[0m12:45:45.640467 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__repository_teams.sql
[0m12:45:45.646510 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__repository_teams.sql
[0m12:45:45.656287 [debug] [MainThread]: 1603: static parser failed on stg_github__issue.sql
[0m12:45:45.680913 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue.sql
[0m12:45:45.681913 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_assignee.sql
[0m12:45:45.690512 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_assignee.sql
[0m12:45:45.691511 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_closed_history.sql
[0m12:45:45.699605 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_closed_history.sql
[0m12:45:45.700605 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_comment.sql
[0m12:45:45.710146 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_comment.sql
[0m12:45:45.711157 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_label.sql
[0m12:45:45.718672 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_label.sql
[0m12:45:45.719684 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_merged.sql
[0m12:45:45.728683 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_merged.sql
[0m12:45:45.729196 [debug] [MainThread]: 1603: static parser failed on stg_github__label.sql
[0m12:45:45.738789 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__label.sql
[0m12:45:45.739803 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request.sql
[0m12:45:45.753335 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request.sql
[0m12:45:45.754336 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request_review.sql
[0m12:45:45.764382 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request_review.sql
[0m12:45:45.765382 [debug] [MainThread]: 1603: static parser failed on stg_github__repository.sql
[0m12:45:45.776920 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repository.sql
[0m12:45:45.778432 [debug] [MainThread]: 1603: static parser failed on stg_github__repo_team.sql
[0m12:45:45.786444 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repo_team.sql
[0m12:45:45.787444 [debug] [MainThread]: 1603: static parser failed on stg_github__requested_reviewer_history.sql
[0m12:45:45.796492 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__requested_reviewer_history.sql
[0m12:45:45.797006 [debug] [MainThread]: 1603: static parser failed on stg_github__team.sql
[0m12:45:45.806593 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__team.sql
[0m12:45:45.808607 [debug] [MainThread]: 1603: static parser failed on stg_github__user.sql
[0m12:45:45.815608 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__user.sql
[0m12:45:45.816609 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_assignee_tmp.sql
[0m12:45:45.821129 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_assignee_tmp.sql
[0m12:45:45.822129 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:45:45.826655 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:45:45.826655 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_comment_tmp.sql
[0m12:45:45.830182 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_comment_tmp.sql
[0m12:45:45.832193 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_label_tmp.sql
[0m12:45:45.835193 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_label_tmp.sql
[0m12:45:45.836193 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_merged_tmp.sql
[0m12:45:45.839718 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_merged_tmp.sql
[0m12:45:45.840718 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_tmp.sql
[0m12:45:45.843718 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_tmp.sql
[0m12:45:45.844718 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__label_tmp.sql
[0m12:45:45.849232 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__label_tmp.sql
[0m12:45:45.850244 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_review_tmp.sql
[0m12:45:45.854770 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_review_tmp.sql
[0m12:45:45.855770 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_tmp.sql
[0m12:45:45.859282 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_tmp.sql
[0m12:45:45.860294 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repository_tmp.sql
[0m12:45:45.863294 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repository_tmp.sql
[0m12:45:45.864294 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repo_team_tmp.sql
[0m12:45:45.868308 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repo_team_tmp.sql
[0m12:45:45.869309 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:45:45.873308 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:45:45.875310 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__team_tmp.sql
[0m12:45:45.878821 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__team_tmp.sql
[0m12:45:45.879832 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__user_tmp.sql
[0m12:45:45.883345 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__user_tmp.sql
[0m12:45:46.184536 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206957C8A00>]}
[0m12:45:46.204630 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206957C81C0>]}
[0m12:45:46.204630 [info ] [MainThread]: Found 49 models, 41 tests, 0 snapshots, 0 analyses, 615 macros, 0 operations, 0 seed files, 17 sources, 1 exposure, 0 metrics
[0m12:45:46.205632 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020695927790>]}
[0m12:45:46.208631 [info ] [MainThread]: 
[0m12:45:46.209762 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:45:46.212766 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m12:45:46.212766 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:45:46.688268 [debug] [ThreadPool]: Acquiring new bigquery connection "create_airflow-docker-352518_dbt_x_airflow"
[0m12:45:46.688268 [debug] [ThreadPool]: Acquiring new bigquery connection "create_airflow-docker-352518_dbt_x_airflow"
[0m12:45:46.688268 [debug] [ThreadPool]: BigQuery adapter: Creating schema "airflow-docker-352518.dbt_x_airflow".
[0m12:45:46.688268 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m12:45:47.680977 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m12:45:47.680977 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:45:48.169793 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206957815B0>]}
[0m12:45:48.169793 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m12:45:48.170809 [info ] [MainThread]: 
[0m12:45:48.176799 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m12:45:48.176799 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m12:45:48.178453 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m12:45:48.178453 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m12:45:48.178453 [info ] [Thread-1  ]: 1 of 40 START table model dbt_x_airflow.agg_transactions ....................... [RUN]
[0m12:45:48.178453 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_assignee_tmp
[0m12:45:48.179460 [info ] [Thread-2  ]: 2 of 40 START view model dbt_x_airflow.stg_customers ........................... [RUN]
[0m12:45:48.179460 [info ] [Thread-3  ]: 3 of 40 START view model dbt_x_airflow.stg_orders .............................. [RUN]
[0m12:45:48.179460 [info ] [Thread-4  ]: 4 of 40 START view model dbt_x_airflow.stg_payments ............................ [RUN]
[0m12:45:48.180460 [info ] [Thread-5  ]: 5 of 40 START view model dbt_x_airflow.stg_github__issue_assignee_tmp .......... [RUN]
[0m12:45:48.180460 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m12:45:48.181461 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m12:45:48.181461 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m12:45:48.182477 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m12:45:48.182477 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m12:45:48.182477 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m12:45:48.182477 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m12:45:48.182477 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m12:45:48.183465 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m12:45:48.187470 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:45:48.190981 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:45:48.190981 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_assignee_tmp"
[0m12:45:48.191983 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m12:45:48.191983 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m12:45:48.191983 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_assignee_tmp
[0m12:45:48.194225 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m12:45:48.195215 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_assignee_tmp
[0m12:45:48.197730 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:45:48.201072 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:45:48.205080 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_assignee_tmp"
[0m12:45:48.206072 [debug] [Thread-1  ]: finished collecting timing info
[0m12:45:48.206072 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m12:45:48.238351 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:45:48.239351 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:48.239351 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:48.239351 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m12:45:48.240319 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:48.240319 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m12:45:48.245351 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m12:45:48.267007 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:45:48.269539 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:45:48.270539 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:45:48.270539 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m12:45:48.270539 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:48.271509 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_assignee_tmp
[0m12:45:48.274514 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_assignee_tmp"
[0m12:45:48.274514 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m12:45:48.275506 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m12:45:48.275506 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m12:45:48.276506 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m12:45:48.279005 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m12:45:48.280012 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m12:45:48.280012 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    CONCAT('$', ROUND(amount/100, 1)) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m12:45:48.282010 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m12:45:48.284527 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_assignee_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_assignee_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_assignee_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_assignee`;


[0m12:45:48.924693 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_assignee_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_assignee_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_assignee`;


[0m12:45:48.924693 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 22f73c48-c6e5-4989-a644-0b35fc5d17d2

[0m12:45:48.925694 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:48.925694 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_assignee_tmp (models\tmp\stg_github__issue_assignee_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 22f73c48-c6e5-4989-a644-0b35fc5d17d2
  
[0m12:45:48.925694 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020695AF55E0>]}
[0m12:45:48.925694 [error] [Thread-5  ]: 5 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_assignee_tmp . [[31mERROR[0m in 0.73s]
[0m12:45:48.926693 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_assignee_tmp
[0m12:45:48.927818 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_closed_history_tmp
[0m12:45:48.927818 [info ] [Thread-5  ]: 6 of 40 START view model dbt_x_airflow.stg_github__issue_closed_history_tmp .... [RUN]
[0m12:45:48.928823 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:45:48.928823 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_closed_history_tmp
[0m12:45:48.928823 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_closed_history_tmp
[0m12:45:48.932827 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:45:48.933829 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:48.933829 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_closed_history_tmp
[0m12:45:48.936838 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:45:48.938439 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:45:48.944452 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_closed_history_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_closed_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_closed_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_closed_history`;


[0m12:45:49.630990 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_closed_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_closed_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_closed_history`;


[0m12:45:49.630990 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: b612eba4-c91a-49ff-b60f-f3b2884544fd

[0m12:45:49.631958 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:49.631958 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_closed_history_tmp (models\tmp\stg_github__issue_closed_history_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: b612eba4-c91a-49ff-b60f-f3b2884544fd
  
[0m12:45:49.633525 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020695B1BD60>]}
[0m12:45:49.634550 [error] [Thread-5  ]: 6 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_closed_history_tmp  [[31mERROR[0m in 0.70s]
[0m12:45:49.636534 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_closed_history_tmp
[0m12:45:49.636534 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_comment_tmp
[0m12:45:49.636534 [info ] [Thread-5  ]: 7 of 40 START view model dbt_x_airflow.stg_github__issue_comment_tmp ........... [RUN]
[0m12:45:49.637534 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_comment_tmp"
[0m12:45:49.637534 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_comment_tmp
[0m12:45:49.637534 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_comment_tmp
[0m12:45:49.642052 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_comment_tmp"
[0m12:45:49.645050 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:49.646049 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_comment_tmp
[0m12:45:49.648559 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_comment_tmp"
[0m12:45:49.650605 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:45:49.654565 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_comment_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_comment_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_comment_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_comment`;


[0m12:45:49.704585 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:49.705585 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696B57130>]}
[0m12:45:49.705585 [info ] [Thread-2  ]: 2 of 40 OK created view model dbt_x_airflow.stg_customers ...................... [[32mOK[0m in 1.52s]
[0m12:45:49.706606 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m12:45:49.706606 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue_label_tmp
[0m12:45:49.706606 [info ] [Thread-2  ]: 8 of 40 START view model dbt_x_airflow.stg_github__issue_label_tmp ............. [RUN]
[0m12:45:49.707854 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_label_tmp"
[0m12:45:49.707854 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__issue_label_tmp
[0m12:45:49.707854 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__issue_label_tmp
[0m12:45:49.711862 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__issue_label_tmp"
[0m12:45:49.712859 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:49.712859 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__issue_label_tmp
[0m12:45:49.715866 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_label_tmp"
[0m12:45:49.716870 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:45:49.721439 [debug] [Thread-2  ]: On model.github_source.stg_github__issue_label_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_label`;


[0m12:45:49.799621 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:49.800613 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206956B23D0>]}
[0m12:45:49.800613 [info ] [Thread-3  ]: 3 of 40 OK created view model dbt_x_airflow.stg_orders ......................... [[32mOK[0m in 1.62s]
[0m12:45:49.801612 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m12:45:49.801612 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__issue_merged_tmp
[0m12:45:49.802613 [info ] [Thread-3  ]: 9 of 40 START view model dbt_x_airflow.stg_github__issue_merged_tmp ............ [RUN]
[0m12:45:49.802613 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_merged_tmp"
[0m12:45:49.803621 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__issue_merged_tmp
[0m12:45:49.803621 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__issue_merged_tmp
[0m12:45:49.806620 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__issue_merged_tmp"
[0m12:45:49.807615 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:49.807615 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__issue_merged_tmp
[0m12:45:49.809926 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_merged_tmp"
[0m12:45:49.810927 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:45:49.815013 [debug] [Thread-3  ]: On model.github_source.stg_github__issue_merged_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_merged_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_merged_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_merged`;


[0m12:45:49.902343 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:49.903358 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206956B2160>]}
[0m12:45:49.903358 [info ] [Thread-4  ]: 4 of 40 OK created view model dbt_x_airflow.stg_payments ....................... [[32mOK[0m in 1.72s]
[0m12:45:49.904348 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m12:45:49.905349 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_tmp
[0m12:45:49.905349 [info ] [Thread-4  ]: 10 of 40 START view model dbt_x_airflow.stg_github__issue_tmp .................. [RUN]
[0m12:45:49.906350 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_tmp"
[0m12:45:49.906350 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__issue_tmp
[0m12:45:49.906350 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__issue_tmp
[0m12:45:49.910572 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__issue_tmp"
[0m12:45:49.912573 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:49.912573 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__issue_tmp
[0m12:45:49.915571 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_tmp"
[0m12:45:49.915571 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:45:49.920583 [debug] [Thread-4  ]: On model.github_source.stg_github__issue_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue`;


[0m12:45:50.204596 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_comment_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_comment_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_comment`;


[0m12:45:50.205603 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 9c171a2f-dde5-48dd-a49f-5f82751e6b7f

[0m12:45:50.205603 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:50.205603 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_comment_tmp (models\tmp\stg_github__issue_comment_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 9c171a2f-dde5-48dd-a49f-5f82751e6b7f
  
[0m12:45:50.205603 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696BC7130>]}
[0m12:45:50.206601 [error] [Thread-5  ]: 7 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_comment_tmp .. [[31mERROR[0m in 0.57s]
[0m12:45:50.207588 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_comment_tmp
[0m12:45:50.207588 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__label_tmp
[0m12:45:50.207588 [info ] [Thread-5  ]: 11 of 40 START view model dbt_x_airflow.stg_github__label_tmp .................. [RUN]
[0m12:45:50.208590 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__label_tmp"
[0m12:45:50.208590 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__label_tmp
[0m12:45:50.208590 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__label_tmp
[0m12:45:50.213762 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__label_tmp"
[0m12:45:50.214762 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:50.214762 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__label_tmp
[0m12:45:50.218291 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__label_tmp"
[0m12:45:50.222297 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:45:50.227296 [debug] [Thread-5  ]: On model.github_source.stg_github__label_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`label`;


[0m12:45:50.248354 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_label`;


[0m12:45:50.249361 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 5f5d5b67-fefa-4c01-a869-41ef82a9bd02

[0m12:45:50.249361 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:50.250371 [debug] [Thread-2  ]: Runtime Error in model stg_github__issue_label_tmp (models\tmp\stg_github__issue_label_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 5f5d5b67-fefa-4c01-a869-41ef82a9bd02
  
[0m12:45:50.250371 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696BEAA30>]}
[0m12:45:50.250371 [error] [Thread-2  ]: 8 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_label_tmp .... [[31mERROR[0m in 0.54s]
[0m12:45:50.251372 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue_label_tmp
[0m12:45:50.252360 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__pull_request_review_tmp
[0m12:45:50.253364 [info ] [Thread-2  ]: 12 of 40 START view model dbt_x_airflow.stg_github__pull_request_review_tmp .... [RUN]
[0m12:45:50.254362 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__pull_request_review_tmp"
[0m12:45:50.254362 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__pull_request_review_tmp
[0m12:45:50.254362 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__pull_request_review_tmp
[0m12:45:50.258371 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__pull_request_review_tmp"
[0m12:45:50.258371 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:50.258371 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__pull_request_review_tmp
[0m12:45:50.261531 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__pull_request_review_tmp"
[0m12:45:50.261531 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:45:50.266936 [debug] [Thread-2  ]: On model.github_source.stg_github__pull_request_review_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_review_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_review_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request_review`;


[0m12:45:50.307736 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_merged_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_merged_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_merged`;


[0m12:45:50.307736 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 5bcb34d3-aed7-44b4-a923-d268a68ebc4a

[0m12:45:50.307736 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:50.308759 [debug] [Thread-3  ]: Runtime Error in model stg_github__issue_merged_tmp (models\tmp\stg_github__issue_merged_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 5bcb34d3-aed7-44b4-a923-d268a68ebc4a
  
[0m12:45:50.308759 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696B807C0>]}
[0m12:45:50.308759 [error] [Thread-3  ]: 9 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_merged_tmp ... [[31mERROR[0m in 0.51s]
[0m12:45:50.310744 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__issue_merged_tmp
[0m12:45:50.310744 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__pull_request_tmp
[0m12:45:50.310744 [info ] [Thread-3  ]: 13 of 40 START view model dbt_x_airflow.stg_github__pull_request_tmp ........... [RUN]
[0m12:45:50.311756 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__pull_request_tmp"
[0m12:45:50.311756 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__pull_request_tmp
[0m12:45:50.311756 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__pull_request_tmp
[0m12:45:50.315746 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__pull_request_tmp"
[0m12:45:50.318253 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:50.319298 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__pull_request_tmp
[0m12:45:50.322297 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__pull_request_tmp"
[0m12:45:50.324304 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:45:50.328840 [debug] [Thread-3  ]: On model.github_source.stg_github__pull_request_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request`;


[0m12:45:50.479450 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue`;


[0m12:45:50.479450 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: f8556a73-24c3-49c0-b095-3fb26133144e

[0m12:45:50.480457 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:50.480457 [debug] [Thread-4  ]: Runtime Error in model stg_github__issue_tmp (models\tmp\stg_github__issue_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: f8556a73-24c3-49c0-b095-3fb26133144e
  
[0m12:45:50.480457 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696C03A90>]}
[0m12:45:50.480457 [error] [Thread-4  ]: 10 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_tmp ......... [[31mERROR[0m in 0.57s]
[0m12:45:50.482457 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_tmp
[0m12:45:50.482457 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__repo_team_tmp
[0m12:45:50.534828 [info ] [Thread-4  ]: 14 of 40 START view model dbt_x_airflow.stg_github__repo_team_tmp .............. [RUN]
[0m12:45:50.535800 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__repo_team_tmp"
[0m12:45:50.535800 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__repo_team_tmp
[0m12:45:50.535800 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__repo_team_tmp
[0m12:45:50.540457 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__repo_team_tmp"
[0m12:45:50.541421 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:50.541421 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__repo_team_tmp
[0m12:45:50.543434 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__repo_team_tmp"
[0m12:45:50.544429 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:45:50.548435 [debug] [Thread-4  ]: On model.github_source.stg_github__repo_team_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repo_team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repo_team_tmp`
  OPTIONS()
  as 

select * 
from `airflow-docker-352518`.`github`.`repo_team`;


[0m12:45:50.737194 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`label`;


[0m12:45:50.737194 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 71990424-57fe-419e-9ce1-7ab09f9c32cb

[0m12:45:50.737194 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:50.737194 [debug] [Thread-5  ]: Runtime Error in model stg_github__label_tmp (models\tmp\stg_github__label_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 71990424-57fe-419e-9ce1-7ab09f9c32cb
  
[0m12:45:50.737194 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696CD0FA0>]}
[0m12:45:50.737194 [error] [Thread-5  ]: 11 of 40 ERROR creating view model dbt_x_airflow.stg_github__label_tmp ......... [[31mERROR[0m in 0.53s]
[0m12:45:50.739718 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__label_tmp
[0m12:45:50.739718 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__repository_tmp
[0m12:45:50.739718 [info ] [Thread-5  ]: 15 of 40 START view model dbt_x_airflow.stg_github__repository_tmp ............. [RUN]
[0m12:45:50.740717 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__repository_tmp"
[0m12:45:50.740717 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__repository_tmp
[0m12:45:50.740717 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__repository_tmp
[0m12:45:50.744917 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__repository_tmp"
[0m12:45:50.745983 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:50.745983 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__repository_tmp
[0m12:45:50.748446 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__repository_tmp"
[0m12:45:50.753495 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:45:50.758506 [debug] [Thread-5  ]: On model.github_source.stg_github__repository_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repository_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repository_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`repository`;


[0m12:45:50.828731 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request`;


[0m12:45:50.828731 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 36e4a8f6-cc19-4c59-a8e2-c7598b3d2fdd

[0m12:45:50.829734 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:50.829734 [debug] [Thread-3  ]: Runtime Error in model stg_github__pull_request_tmp (models\tmp\stg_github__pull_request_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 36e4a8f6-cc19-4c59-a8e2-c7598b3d2fdd
  
[0m12:45:50.829734 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696C421F0>]}
[0m12:45:50.829734 [error] [Thread-3  ]: 13 of 40 ERROR creating view model dbt_x_airflow.stg_github__pull_request_tmp .. [[31mERROR[0m in 0.52s]
[0m12:45:50.831732 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__pull_request_tmp
[0m12:45:50.831732 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:45:50.831732 [info ] [Thread-3  ]: 16 of 40 START view model dbt_x_airflow.stg_github__requested_reviewer_history_tmp  [RUN]
[0m12:45:50.832832 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:45:50.832832 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:45:50.832832 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:45:50.838422 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:45:50.838422 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_review_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_review_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request_review`;


[0m12:45:50.839428 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 74d0fa7e-9a44-445f-a3ef-741d31abcc25

[0m12:45:50.839428 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:50.840436 [debug] [Thread-2  ]: Runtime Error in model stg_github__pull_request_review_tmp (models\tmp\stg_github__pull_request_review_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 74d0fa7e-9a44-445f-a3ef-741d31abcc25
  
[0m12:45:50.840436 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696C6D700>]}
[0m12:45:50.840436 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:50.840436 [error] [Thread-2  ]: 12 of 40 ERROR creating view model dbt_x_airflow.stg_github__pull_request_review_tmp  [[31mERROR[0m in 0.59s]
[0m12:45:50.841437 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:45:50.844449 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:45:50.845443 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__pull_request_review_tmp
[0m12:45:50.846436 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__team_tmp
[0m12:45:50.846436 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:45:50.847436 [info ] [Thread-2  ]: 17 of 40 START view model dbt_x_airflow.stg_github__team_tmp ................... [RUN]
[0m12:45:50.848443 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__team_tmp"
[0m12:45:50.848443 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__team_tmp
[0m12:45:50.848443 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__team_tmp
[0m12:45:50.852443 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__team_tmp"
[0m12:45:50.853444 [debug] [Thread-3  ]: On model.github_source.stg_github__requested_reviewer_history_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__requested_reviewer_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__requested_reviewer_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`requested_reviewer_history`;


[0m12:45:50.855448 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:50.855448 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__team_tmp
[0m12:45:50.858948 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__team_tmp"
[0m12:45:50.859955 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:45:50.864387 [debug] [Thread-2  ]: On model.github_source.stg_github__team_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__team_tmp`
  OPTIONS()
  as select * 
from `airflow-docker-352518`.`github`.`team`;


[0m12:45:51.078281 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repo_team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repo_team_tmp`
  OPTIONS()
  as 

select * 
from `airflow-docker-352518`.`github`.`repo_team`;


[0m12:45:51.079549 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 288b6f0a-a6cc-43da-8040-1857bc65db17

[0m12:45:51.079549 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:51.079549 [debug] [Thread-4  ]: Runtime Error in model stg_github__repo_team_tmp (models\tmp\stg_github__repo_team_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 288b6f0a-a6cc-43da-8040-1857bc65db17
  
[0m12:45:51.079549 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020696CCA820>]}
[0m12:45:51.080564 [error] [Thread-4  ]: 14 of 40 ERROR creating view model dbt_x_airflow.stg_github__repo_team_tmp ..... [[31mERROR[0m in 0.54s]
[0m12:45:51.081555 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__repo_team_tmp
[0m12:45:51.081555 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__user_tmp
[0m12:45:51.081555 [info ] [Thread-4  ]: 18 of 40 START view model dbt_x_airflow.stg_github__user_tmp ................... [RUN]
[0m12:45:51.082565 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__user_tmp"
[0m12:45:51.082565 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__user_tmp
[0m12:45:51.082565 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__user_tmp
[0m12:45:51.086557 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__user_tmp"
[0m12:45:51.087561 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:51.087561 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__user_tmp
[0m12:45:51.091586 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__user_tmp"
[0m12:45:51.093571 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:45:51.098075 [debug] [Thread-4  ]: On model.github_source.stg_github__user_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__user_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__user_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`user`;


[0m12:45:51.276388 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repository_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repository_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`repository`;


[0m12:45:51.276388 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: d7061879-df0e-40c2-8a3d-d22c84e42d16

[0m12:45:51.276388 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:51.277389 [debug] [Thread-5  ]: Runtime Error in model stg_github__repository_tmp (models\tmp\stg_github__repository_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: d7061879-df0e-40c2-8a3d-d22c84e42d16
  
[0m12:45:51.277389 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020695AF5730>]}
[0m12:45:51.277389 [error] [Thread-5  ]: 15 of 40 ERROR creating view model dbt_x_airflow.stg_github__repository_tmp .... [[31mERROR[0m in 0.54s]
[0m12:45:51.279453 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__repository_tmp
[0m12:45:51.279453 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_assignee
[0m12:45:51.279453 [info ] [Thread-5  ]: 19 of 40 SKIP relation dbt_x_airflow.stg_github__issue_assignee ................ [[33mSKIP[0m]
[0m12:45:51.280455 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_assignee
[0m12:45:51.281455 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_closed_history
[0m12:45:51.281455 [info ] [Thread-5  ]: 20 of 40 SKIP relation dbt_x_airflow.stg_github__issue_closed_history .......... [[33mSKIP[0m]
[0m12:45:51.282459 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_closed_history
[0m12:45:51.282459 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.dim_customers
[0m12:45:51.282459 [info ] [Thread-5  ]: 21 of 40 START table model dbt_x_airflow.dim_customers ......................... [RUN]
[0m12:45:51.283465 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m12:45:51.283465 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m12:45:51.284471 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.dim_customers
[0m12:45:51.287477 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:45:51.288471 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:51.288471 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m12:45:51.291728 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:45:51.292833 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:45:51.296891 [debug] [Thread-5  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m12:45:51.350564 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__requested_reviewer_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__requested_reviewer_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`requested_reviewer_history`;


[0m12:45:51.351471 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: a36bf6b0-6603-4541-9e55-4e458d0bff76

[0m12:45:51.351471 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:51.351471 [debug] [Thread-3  ]: Runtime Error in model stg_github__requested_reviewer_history_tmp (models\tmp\stg_github__requested_reviewer_history_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: a36bf6b0-6603-4541-9e55-4e458d0bff76
  
[0m12:45:51.351471 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020697D47FA0>]}
[0m12:45:51.352474 [error] [Thread-3  ]: 16 of 40 ERROR creating view model dbt_x_airflow.stg_github__requested_reviewer_history_tmp  [[31mERROR[0m in 0.52s]
[0m12:45:51.353474 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:45:51.353474 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m12:45:51.353474 [info ] [Thread-3  ]: 22 of 40 START table model dbt_x_airflow.pivoted_orders ........................ [RUN]
[0m12:45:51.354473 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m12:45:51.354473 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m12:45:51.354473 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m12:45:51.358482 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:45:51.362478 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:51.362478 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m12:45:51.365479 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:45:51.367487 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:45:51.370638 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__team_tmp`
  OPTIONS()
  as select * 
from `airflow-docker-352518`.`github`.`team`;


[0m12:45:51.370638 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 4a3df965-05eb-4d23-8ca1-dab48bd5ef4f

[0m12:45:51.370638 [debug] [Thread-2  ]: finished collecting timing info
[0m12:45:51.370638 [debug] [Thread-2  ]: Runtime Error in model stg_github__team_tmp (models\tmp\stg_github__team_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 4a3df965-05eb-4d23-8ca1-dab48bd5ef4f
  
[0m12:45:51.371636 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020697D42B20>]}
[0m12:45:51.371636 [error] [Thread-2  ]: 17 of 40 ERROR creating view model dbt_x_airflow.stg_github__team_tmp .......... [[31mERROR[0m in 0.52s]
[0m12:45:51.372878 [debug] [Thread-3  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m12:45:51.372878 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__team_tmp
[0m12:45:51.373888 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue_comment
[0m12:45:51.373888 [info ] [Thread-2  ]: 23 of 40 SKIP relation dbt_x_airflow.stg_github__issue_comment ................. [[33mSKIP[0m]
[0m12:45:51.374890 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue_comment
[0m12:45:51.375891 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue_label
[0m12:45:51.375891 [info ] [Thread-2  ]: 24 of 40 SKIP relation dbt_x_airflow.stg_github__issue_label ................... [[33mSKIP[0m]
[0m12:45:51.376899 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue_label
[0m12:45:51.376899 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue_merged
[0m12:45:51.376899 [info ] [Thread-2  ]: 25 of 40 SKIP relation dbt_x_airflow.stg_github__issue_merged .................. [[33mSKIP[0m]
[0m12:45:51.378410 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue_merged
[0m12:45:51.378410 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue
[0m12:45:51.378410 [info ] [Thread-2  ]: 26 of 40 SKIP relation dbt_x_airflow.stg_github__issue ......................... [[33mSKIP[0m]
[0m12:45:51.379700 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue
[0m12:45:51.379700 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__label
[0m12:45:51.380713 [info ] [Thread-2  ]: 27 of 40 SKIP relation dbt_x_airflow.stg_github__label ......................... [[33mSKIP[0m]
[0m12:45:51.380713 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__label
[0m12:45:51.381712 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__pull_request
[0m12:45:51.381712 [info ] [Thread-2  ]: 28 of 40 SKIP relation dbt_x_airflow.stg_github__pull_request .................. [[33mSKIP[0m]
[0m12:45:51.382712 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__pull_request
[0m12:45:51.382712 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__pull_request_review
[0m12:45:51.382712 [info ] [Thread-2  ]: 29 of 40 SKIP relation dbt_x_airflow.stg_github__pull_request_review ........... [[33mSKIP[0m]
[0m12:45:51.383710 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__pull_request_review
[0m12:45:51.383710 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__repo_team
[0m12:45:51.383710 [info ] [Thread-2  ]: 30 of 40 SKIP relation dbt_x_airflow.stg_github__repo_team ..................... [[33mSKIP[0m]
[0m12:45:51.384709 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__repo_team
[0m12:45:51.384709 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__repository
[0m12:45:51.384709 [info ] [Thread-2  ]: 31 of 40 SKIP relation dbt_x_airflow.stg_github__repository .................... [[33mSKIP[0m]
[0m12:45:51.385710 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__repository
[0m12:45:51.385710 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__requested_reviewer_history
[0m12:45:51.385710 [info ] [Thread-2  ]: 32 of 40 SKIP relation dbt_x_airflow.stg_github__requested_reviewer_history .... [[33mSKIP[0m]
[0m12:45:51.386708 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__requested_reviewer_history
[0m12:45:51.386708 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__team
[0m12:45:51.386708 [info ] [Thread-2  ]: 33 of 40 SKIP relation dbt_x_airflow.stg_github__team .......................... [[33mSKIP[0m]
[0m12:45:51.388006 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__team
[0m12:45:51.388006 [debug] [Thread-2  ]: Began running node model.github.int_github__issue_comments
[0m12:45:51.389017 [debug] [Thread-2  ]: Finished running node model.github.int_github__issue_comments
[0m12:45:51.389017 [debug] [Thread-2  ]: Began running node model.github.int_github__issue_open_length
[0m12:45:51.389017 [debug] [Thread-2  ]: Finished running node model.github.int_github__issue_open_length
[0m12:45:51.390029 [debug] [Thread-2  ]: Began running node model.github.int_github__issue_label_joined
[0m12:45:51.390029 [debug] [Thread-2  ]: Finished running node model.github.int_github__issue_label_joined
[0m12:45:51.390029 [debug] [Thread-2  ]: Began running node model.github.int_github__pull_request_times
[0m12:45:51.390029 [debug] [Thread-2  ]: Finished running node model.github.int_github__pull_request_times
[0m12:45:51.390029 [debug] [Thread-2  ]: Began running node model.github.int_github__repository_teams
[0m12:45:51.390029 [debug] [Thread-2  ]: Finished running node model.github.int_github__repository_teams
[0m12:45:51.391013 [debug] [Thread-2  ]: Began running node model.github.int_github__issue_labels
[0m12:45:51.391013 [debug] [Thread-2  ]: Finished running node model.github.int_github__issue_labels
[0m12:45:51.570045 [debug] [Thread-1  ]: finished collecting timing info
[0m12:45:51.571060 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000206956B2D60>]}
[0m12:45:51.571060 [info ] [Thread-1  ]: 1 of 40 OK created table model dbt_x_airflow.agg_transactions .................. [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.39s]
[0m12:45:51.572047 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m12:45:51.595673 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__user_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__user_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`user`;


[0m12:45:51.595673 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: c4dd080a-8b2f-49a9-b909-464a950a48ec

[0m12:45:51.595673 [debug] [Thread-4  ]: finished collecting timing info
[0m12:45:51.595673 [debug] [Thread-4  ]: Runtime Error in model stg_github__user_tmp (models\tmp\stg_github__user_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: c4dd080a-8b2f-49a9-b909-464a950a48ec
  
[0m12:45:51.596688 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020697D472B0>]}
[0m12:45:51.596688 [error] [Thread-4  ]: 18 of 40 ERROR creating view model dbt_x_airflow.stg_github__user_tmp .......... [[31mERROR[0m in 0.51s]
[0m12:45:51.596688 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__user_tmp
[0m12:45:51.598197 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__user
[0m12:45:51.598197 [info ] [Thread-2  ]: 34 of 40 SKIP relation dbt_x_airflow.stg_github__user .......................... [[33mSKIP[0m]
[0m12:45:51.599249 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__user
[0m12:45:51.600242 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_assignees
[0m12:45:51.600242 [debug] [Thread-1  ]: Began running node model.github.int_github__pull_request_reviewers
[0m12:45:51.600242 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_assignees
[0m12:45:51.601239 [debug] [Thread-1  ]: Finished running node model.github.int_github__pull_request_reviewers
[0m12:45:51.601239 [debug] [Thread-2  ]: Began running node model.github.int_github__issue_joined
[0m12:45:51.601239 [debug] [Thread-2  ]: Finished running node model.github.int_github__issue_joined
[0m12:45:51.602237 [debug] [Thread-1  ]: Began running node model.github.github__issues
[0m12:45:51.602237 [info ] [Thread-1  ]: 35 of 40 SKIP relation dbt_x_airflow.github__issues ............................ [[33mSKIP[0m]
[0m12:45:51.602237 [debug] [Thread-4  ]: Began running node model.github.github__pull_requests
[0m12:45:51.602237 [info ] [Thread-4  ]: 36 of 40 SKIP relation dbt_x_airflow.github__pull_requests ..................... [[33mSKIP[0m]
[0m12:45:51.603238 [debug] [Thread-1  ]: Finished running node model.github.github__issues
[0m12:45:51.604238 [debug] [Thread-4  ]: Finished running node model.github.github__pull_requests
[0m12:45:51.604238 [debug] [Thread-2  ]: Began running node model.github.github__daily_metrics
[0m12:45:51.604238 [info ] [Thread-2  ]: 37 of 40 SKIP relation dbt_x_airflow.github__daily_metrics ..................... [[33mSKIP[0m]
[0m12:45:51.605237 [debug] [Thread-2  ]: Finished running node model.github.github__daily_metrics
[0m12:45:51.605237 [debug] [Thread-4  ]: Began running node model.github.github__monthly_metrics
[0m12:45:51.606239 [debug] [Thread-1  ]: Began running node model.github.github__quarterly_metrics
[0m12:45:51.606239 [debug] [Thread-2  ]: Began running node model.github.github__weekly_metrics
[0m12:45:51.606239 [info ] [Thread-4  ]: 38 of 40 SKIP relation dbt_x_airflow.github__monthly_metrics ................... [[33mSKIP[0m]
[0m12:45:51.606239 [info ] [Thread-1  ]: 39 of 40 SKIP relation dbt_x_airflow.github__quarterly_metrics ................. [[33mSKIP[0m]
[0m12:45:51.606239 [info ] [Thread-2  ]: 40 of 40 SKIP relation dbt_x_airflow.github__weekly_metrics .................... [[33mSKIP[0m]
[0m12:45:51.607239 [debug] [Thread-4  ]: Finished running node model.github.github__monthly_metrics
[0m12:45:51.608757 [debug] [Thread-1  ]: Finished running node model.github.github__quarterly_metrics
[0m12:45:51.608757 [debug] [Thread-2  ]: Finished running node model.github.github__weekly_metrics
[0m12:45:52.777604 [debug] [Thread-3  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]')
[0m12:45:54.818580 [debug] [Thread-3  ]: finished collecting timing info
[0m12:45:54.818580 [debug] [Thread-3  ]: Database Error in model pivoted_orders (models\prod\pivoted_orders.sql)
  No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
  compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:45:54.818580 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020697DE5A30>]}
[0m12:45:54.819591 [error] [Thread-3  ]: 22 of 40 ERROR creating table model dbt_x_airflow.pivoted_orders ............... [[31mERROR[0m in 3.46s]
[0m12:45:54.820590 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m12:45:54.973544 [debug] [Thread-5  ]: finished collecting timing info
[0m12:45:54.973544 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '904a873b-138e-4422-8963-0ec851418d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020695B06F40>]}
[0m12:45:54.974544 [info ] [Thread-5  ]: 21 of 40 OK created table model dbt_x_airflow.dim_customers .................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.69s]
[0m12:45:54.975535 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m12:45:54.976537 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:45:54.977537 [info ] [MainThread]: 
[0m12:45:54.978544 [info ] [MainThread]: Finished running 17 view models, 23 table models in 0 hours 0 minutes and 8.77 seconds (8.77s).
[0m12:45:54.978544 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:45:54.978544 [debug] [MainThread]: Connection 'create_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m12:45:54.978544 [debug] [MainThread]: Connection 'list_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m12:45:54.978544 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m12:45:54.979633 [debug] [MainThread]: Connection 'model.github_source.stg_github__team_tmp' was properly closed.
[0m12:45:54.979633 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m12:45:54.979633 [debug] [MainThread]: Connection 'model.github_source.stg_github__user_tmp' was properly closed.
[0m12:45:54.979633 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m12:45:54.995668 [info ] [MainThread]: 
[0m12:45:54.996667 [info ] [MainThread]: [31mCompleted with 15 errors and 0 warnings:[0m
[0m12:45:54.997667 [info ] [MainThread]: 
[0m12:45:54.997667 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_assignee_tmp (models\tmp\stg_github__issue_assignee_tmp.sql)[0m
[0m12:45:54.998667 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:54.999841 [error] [MainThread]:   
[0m12:45:54.999841 [error] [MainThread]:   Location: US
[0m12:45:54.999841 [error] [MainThread]:   Job ID: 22f73c48-c6e5-4989-a644-0b35fc5d17d2
[0m12:45:54.999841 [error] [MainThread]:   
[0m12:45:55.001349 [info ] [MainThread]: 
[0m12:45:55.001349 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_closed_history_tmp (models\tmp\stg_github__issue_closed_history_tmp.sql)[0m
[0m12:45:55.002694 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.003703 [error] [MainThread]:   
[0m12:45:55.003703 [error] [MainThread]:   Location: US
[0m12:45:55.003703 [error] [MainThread]:   Job ID: b612eba4-c91a-49ff-b60f-f3b2884544fd
[0m12:45:55.004700 [error] [MainThread]:   
[0m12:45:55.004700 [info ] [MainThread]: 
[0m12:45:55.005700 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_comment_tmp (models\tmp\stg_github__issue_comment_tmp.sql)[0m
[0m12:45:55.005700 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.006702 [error] [MainThread]:   
[0m12:45:55.006702 [error] [MainThread]:   Location: US
[0m12:45:55.007702 [error] [MainThread]:   Job ID: 9c171a2f-dde5-48dd-a49f-5f82751e6b7f
[0m12:45:55.007702 [error] [MainThread]:   
[0m12:45:55.007702 [info ] [MainThread]: 
[0m12:45:55.008701 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_label_tmp (models\tmp\stg_github__issue_label_tmp.sql)[0m
[0m12:45:55.008701 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.010204 [error] [MainThread]:   
[0m12:45:55.010204 [error] [MainThread]:   Location: US
[0m12:45:55.010204 [error] [MainThread]:   Job ID: 5f5d5b67-fefa-4c01-a869-41ef82a9bd02
[0m12:45:55.011204 [error] [MainThread]:   
[0m12:45:55.011204 [info ] [MainThread]: 
[0m12:45:55.012204 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_merged_tmp (models\tmp\stg_github__issue_merged_tmp.sql)[0m
[0m12:45:55.012204 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.013206 [error] [MainThread]:   
[0m12:45:55.013206 [error] [MainThread]:   Location: US
[0m12:45:55.014206 [error] [MainThread]:   Job ID: 5bcb34d3-aed7-44b4-a923-d268a68ebc4a
[0m12:45:55.014206 [error] [MainThread]:   
[0m12:45:55.015205 [info ] [MainThread]: 
[0m12:45:55.015205 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_tmp (models\tmp\stg_github__issue_tmp.sql)[0m
[0m12:45:55.015205 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.016205 [error] [MainThread]:   
[0m12:45:55.016205 [error] [MainThread]:   Location: US
[0m12:45:55.017205 [error] [MainThread]:   Job ID: f8556a73-24c3-49c0-b095-3fb26133144e
[0m12:45:55.017205 [error] [MainThread]:   
[0m12:45:55.018522 [info ] [MainThread]: 
[0m12:45:55.018522 [error] [MainThread]: [33mRuntime Error in model stg_github__label_tmp (models\tmp\stg_github__label_tmp.sql)[0m
[0m12:45:55.019529 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.019529 [error] [MainThread]:   
[0m12:45:55.020530 [error] [MainThread]:   Location: US
[0m12:45:55.020530 [error] [MainThread]:   Job ID: 71990424-57fe-419e-9ce1-7ab09f9c32cb
[0m12:45:55.020530 [error] [MainThread]:   
[0m12:45:55.021531 [info ] [MainThread]: 
[0m12:45:55.021531 [error] [MainThread]: [33mRuntime Error in model stg_github__pull_request_tmp (models\tmp\stg_github__pull_request_tmp.sql)[0m
[0m12:45:55.022529 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.022529 [error] [MainThread]:   
[0m12:45:55.022529 [error] [MainThread]:   Location: US
[0m12:45:55.023530 [error] [MainThread]:   Job ID: 36e4a8f6-cc19-4c59-a8e2-c7598b3d2fdd
[0m12:45:55.023530 [error] [MainThread]:   
[0m12:45:55.024531 [info ] [MainThread]: 
[0m12:45:55.024531 [error] [MainThread]: [33mRuntime Error in model stg_github__pull_request_review_tmp (models\tmp\stg_github__pull_request_review_tmp.sql)[0m
[0m12:45:55.025528 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.025528 [error] [MainThread]:   
[0m12:45:55.026528 [error] [MainThread]:   Location: US
[0m12:45:55.026528 [error] [MainThread]:   Job ID: 74d0fa7e-9a44-445f-a3ef-741d31abcc25
[0m12:45:55.026528 [error] [MainThread]:   
[0m12:45:55.027530 [info ] [MainThread]: 
[0m12:45:55.027530 [error] [MainThread]: [33mRuntime Error in model stg_github__repo_team_tmp (models\tmp\stg_github__repo_team_tmp.sql)[0m
[0m12:45:55.028534 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.028534 [error] [MainThread]:   
[0m12:45:55.028534 [error] [MainThread]:   Location: US
[0m12:45:55.029593 [error] [MainThread]:   Job ID: 288b6f0a-a6cc-43da-8040-1857bc65db17
[0m12:45:55.029593 [error] [MainThread]:   
[0m12:45:55.030594 [info ] [MainThread]: 
[0m12:45:55.030594 [error] [MainThread]: [33mRuntime Error in model stg_github__repository_tmp (models\tmp\stg_github__repository_tmp.sql)[0m
[0m12:45:55.031593 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.031593 [error] [MainThread]:   
[0m12:45:55.031593 [error] [MainThread]:   Location: US
[0m12:45:55.032789 [error] [MainThread]:   Job ID: d7061879-df0e-40c2-8a3d-d22c84e42d16
[0m12:45:55.032789 [error] [MainThread]:   
[0m12:45:55.032789 [info ] [MainThread]: 
[0m12:45:55.033797 [error] [MainThread]: [33mRuntime Error in model stg_github__requested_reviewer_history_tmp (models\tmp\stg_github__requested_reviewer_history_tmp.sql)[0m
[0m12:45:55.033797 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.034798 [error] [MainThread]:   
[0m12:45:55.034798 [error] [MainThread]:   Location: US
[0m12:45:55.035800 [error] [MainThread]:   Job ID: a36bf6b0-6603-4541-9e55-4e458d0bff76
[0m12:45:55.035800 [error] [MainThread]:   
[0m12:45:55.035800 [info ] [MainThread]: 
[0m12:45:55.036797 [error] [MainThread]: [33mRuntime Error in model stg_github__team_tmp (models\tmp\stg_github__team_tmp.sql)[0m
[0m12:45:55.036797 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.036797 [error] [MainThread]:   
[0m12:45:55.038304 [error] [MainThread]:   Location: US
[0m12:45:55.038304 [error] [MainThread]:   Job ID: 4a3df965-05eb-4d23-8ca1-dab48bd5ef4f
[0m12:45:55.038304 [error] [MainThread]:   
[0m12:45:55.039351 [info ] [MainThread]: 
[0m12:45:55.039351 [error] [MainThread]: [33mRuntime Error in model stg_github__user_tmp (models\tmp\stg_github__user_tmp.sql)[0m
[0m12:45:55.040358 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:45:55.040358 [error] [MainThread]:   
[0m12:45:55.040358 [error] [MainThread]:   Location: US
[0m12:45:55.041358 [error] [MainThread]:   Job ID: c4dd080a-8b2f-49a9-b909-464a950a48ec
[0m12:45:55.041358 [error] [MainThread]:   
[0m12:45:55.041358 [info ] [MainThread]: 
[0m12:45:55.042359 [error] [MainThread]: [33mDatabase Error in model pivoted_orders (models\prod\pivoted_orders.sql)[0m
[0m12:45:55.042359 [error] [MainThread]:   No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
[0m12:45:55.043357 [error] [MainThread]:   compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:45:55.043357 [info ] [MainThread]: 
[0m12:45:55.044358 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=15 SKIP=20 TOTAL=40
[0m12:45:55.044358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002069579BA30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020695781F70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000020695781790>]}


============================== 2022-08-07 12:47:39.240345 | d5515177-b909-4a09-9b70-d3ebbc14ef9b ==============================
[0m12:47:39.240345 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:47:39.241348 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m12:47:39.242344 [debug] [MainThread]: Tracking: tracking
[0m12:47:39.263921 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2F49370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2F49730>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2F49430>]}
[0m12:47:39.264933 [debug] [MainThread]: Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-6uf174cr'
[0m12:47:39.265927 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m12:47:39.410845 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m12:47:39.410845 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m12:47:39.510584 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m12:47:39.546286 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m12:47:40.448748 [info ] [MainThread]:   Installed from version 0.8.6
[0m12:47:40.448748 [info ] [MainThread]:   Up to date!
[0m12:47:40.449789 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'd5515177-b909-4a09-9b70-d3ebbc14ef9b', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2EE6400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2EE6250>]}
[0m12:47:40.450802 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2EB6D90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2EC45B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000015AF2EE6EE0>]}


============================== 2022-08-07 12:47:47.792816 | cdf5e7c8-8587-46b8-aef2-3666bf5fbe43 ==============================
[0m12:47:47.792816 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:47:47.793817 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:47:47.793817 [debug] [MainThread]: Tracking: tracking
[0m12:47:47.806372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2DBE6130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2DBE6D30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2DBE6A60>]}
[0m12:47:48.046473 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:47:48.046473 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:47:48.055295 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2EF973D0>]}
[0m12:47:48.074779 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2EC30760>]}
[0m12:47:48.075747 [info ] [MainThread]: Found 49 models, 41 tests, 0 snapshots, 0 analyses, 615 macros, 0 operations, 0 seed files, 17 sources, 1 exposure, 0 metrics
[0m12:47:48.075747 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2DBF0F70>]}
[0m12:47:48.079454 [info ] [MainThread]: 
[0m12:47:48.080458 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:47:48.082459 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m12:47:48.083459 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:48.616658 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m12:47:48.616658 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:47:49.111909 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2DC06760>]}
[0m12:47:49.112903 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m12:47:49.112903 [info ] [MainThread]: 
[0m12:47:49.120452 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m12:47:49.120452 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m12:47:49.120452 [info ] [Thread-1  ]: 1 of 40 START table model dbt_x_airflow.agg_transactions ....................... [RUN]
[0m12:47:49.120452 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m12:47:49.121442 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m12:47:49.121442 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_assignee_tmp
[0m12:47:49.121442 [info ] [Thread-2  ]: 2 of 40 START view model dbt_x_airflow.stg_customers ........................... [RUN]
[0m12:47:49.122446 [info ] [Thread-3  ]: 3 of 40 START view model dbt_x_airflow.stg_orders .............................. [RUN]
[0m12:47:49.122446 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m12:47:49.122446 [info ] [Thread-4  ]: 4 of 40 START view model dbt_x_airflow.stg_payments ............................ [RUN]
[0m12:47:49.123453 [info ] [Thread-5  ]: 5 of 40 START view model dbt_x_airflow.stg_github__issue_assignee_tmp .......... [RUN]
[0m12:47:49.124453 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m12:47:49.124453 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m12:47:49.124453 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m12:47:49.125452 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m12:47:49.125452 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m12:47:49.125452 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m12:47:49.125452 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m12:47:49.126452 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_assignee_tmp"
[0m12:47:49.126452 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m12:47:49.129966 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:47:49.129966 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m12:47:49.129966 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m12:47:49.129966 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_assignee_tmp
[0m12:47:49.132970 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:47:49.133982 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m12:47:49.136973 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:47:49.138474 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_assignee_tmp
[0m12:47:49.142481 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:47:49.142481 [debug] [Thread-1  ]: finished collecting timing info
[0m12:47:49.148482 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_assignee_tmp"
[0m12:47:49.148482 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m12:47:49.149610 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:49.149610 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:49.164982 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m12:47:49.171732 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m12:47:49.171732 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:49.172732 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m12:47:49.172732 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:49.194469 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m12:47:49.201922 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:47:49.204931 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:47:49.204931 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_assignee_tmp
[0m12:47:49.206955 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:47:49.210531 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_assignee_tmp"
[0m12:47:49.211529 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m12:47:49.212529 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m12:47:49.212529 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m12:47:49.213535 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m12:47:49.217533 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    CONCAT('$', ROUND(amount/100, 1)) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m12:47:49.219041 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m12:47:49.219041 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m12:47:49.221187 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_assignee_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_assignee_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_assignee_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_assignee`;


[0m12:47:49.680537 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:47:49.682504 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m12:47:49.740343 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_assignee_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_assignee_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_assignee`;


[0m12:47:49.741343 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 6d409af2-72de-463a-a9e4-e40971af67b8

[0m12:47:49.741343 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:49.741343 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_assignee_tmp (models\tmp\stg_github__issue_assignee_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 6d409af2-72de-463a-a9e4-e40971af67b8
  
[0m12:47:49.741343 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F1435E0>]}
[0m12:47:49.741343 [error] [Thread-5  ]: 5 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_assignee_tmp . [[31mERROR[0m in 0.61s]
[0m12:47:49.743339 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_assignee_tmp
[0m12:47:49.743339 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_closed_history_tmp
[0m12:47:49.743339 [info ] [Thread-5  ]: 6 of 40 START view model dbt_x_airflow.stg_github__issue_closed_history_tmp .... [RUN]
[0m12:47:49.744337 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:47:49.744337 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_closed_history_tmp
[0m12:47:49.745337 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_closed_history_tmp
[0m12:47:49.748878 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:47:49.749886 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:49.749886 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_closed_history_tmp
[0m12:47:49.753395 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:47:49.754404 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:47:49.758949 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_closed_history_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_closed_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_closed_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_closed_history`;


[0m12:47:50.275862 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_closed_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_closed_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_closed_history`;


[0m12:47:50.276863 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: a5c25812-4284-4a17-9600-2f114a7ff203

[0m12:47:50.276863 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:50.276863 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_closed_history_tmp (models\tmp\stg_github__issue_closed_history_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: a5c25812-4284-4a17-9600-2f114a7ff203
  
[0m12:47:50.276863 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F1AA220>]}
[0m12:47:50.277933 [error] [Thread-5  ]: 6 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_closed_history_tmp  [[31mERROR[0m in 0.53s]
[0m12:47:50.277933 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_closed_history_tmp
[0m12:47:50.278938 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_comment_tmp
[0m12:47:50.278938 [info ] [Thread-5  ]: 7 of 40 START view model dbt_x_airflow.stg_github__issue_comment_tmp ........... [RUN]
[0m12:47:50.279940 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_comment_tmp"
[0m12:47:50.279940 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_comment_tmp
[0m12:47:50.279940 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_comment_tmp
[0m12:47:50.282937 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_comment_tmp"
[0m12:47:50.283938 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:50.284941 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_comment_tmp
[0m12:47:50.286938 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_comment_tmp"
[0m12:47:50.288528 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:47:50.292947 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_comment_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_comment_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_comment_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_comment`;


[0m12:47:50.338889 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:50.339889 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F0D2E50>]}
[0m12:47:50.339889 [info ] [Thread-3  ]: 3 of 40 OK created view model dbt_x_airflow.stg_orders ......................... [[32mOK[0m in 1.22s]
[0m12:47:50.340933 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m12:47:50.341888 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__issue_label_tmp
[0m12:47:50.341888 [info ] [Thread-3  ]: 8 of 40 START view model dbt_x_airflow.stg_github__issue_label_tmp ............. [RUN]
[0m12:47:50.342888 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_label_tmp"
[0m12:47:50.342888 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__issue_label_tmp
[0m12:47:50.342888 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__issue_label_tmp
[0m12:47:50.345890 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__issue_label_tmp"
[0m12:47:50.346897 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:50.346897 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__issue_label_tmp
[0m12:47:50.349472 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_label_tmp"
[0m12:47:50.351461 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:47:50.357774 [debug] [Thread-3  ]: On model.github_source.stg_github__issue_label_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_label`;


[0m12:47:50.361286 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:50.362283 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F029880>]}
[0m12:47:50.362283 [info ] [Thread-4  ]: 4 of 40 OK created view model dbt_x_airflow.stg_payments ....................... [[32mOK[0m in 1.24s]
[0m12:47:50.364325 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m12:47:50.365285 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_merged_tmp
[0m12:47:50.365285 [info ] [Thread-4  ]: 9 of 40 START view model dbt_x_airflow.stg_github__issue_merged_tmp ............ [RUN]
[0m12:47:50.366284 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_merged_tmp"
[0m12:47:50.366284 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__issue_merged_tmp
[0m12:47:50.366284 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__issue_merged_tmp
[0m12:47:50.370522 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__issue_merged_tmp"
[0m12:47:50.371555 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:50.371555 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__issue_merged_tmp
[0m12:47:50.373520 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_merged_tmp"
[0m12:47:50.374552 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:47:50.379618 [debug] [Thread-4  ]: On model.github_source.stg_github__issue_merged_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_merged_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_merged_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_merged`;


[0m12:47:50.416423 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:50.417413 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F023D90>]}
[0m12:47:50.417413 [info ] [Thread-2  ]: 2 of 40 OK created view model dbt_x_airflow.stg_customers ...................... [[32mOK[0m in 1.29s]
[0m12:47:50.418921 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m12:47:50.419929 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue_tmp
[0m12:47:50.419929 [info ] [Thread-2  ]: 10 of 40 START view model dbt_x_airflow.stg_github__issue_tmp .................. [RUN]
[0m12:47:50.420964 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_tmp"
[0m12:47:50.420964 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__issue_tmp
[0m12:47:50.420964 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__issue_tmp
[0m12:47:50.425961 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__issue_tmp"
[0m12:47:50.426961 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:50.426961 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__issue_tmp
[0m12:47:50.478628 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_tmp"
[0m12:47:50.478628 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:47:50.482913 [debug] [Thread-2  ]: On model.github_source.stg_github__issue_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue`;


[0m12:47:50.768330 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_comment_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_comment_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_comment`;


[0m12:47:50.768392 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: bee4fd94-4f34-46eb-b174-dd97641e18bb

[0m12:47:50.768392 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:50.768392 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_comment_tmp (models\tmp\stg_github__issue_comment_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: bee4fd94-4f34-46eb-b174-dd97641e18bb
  
[0m12:47:50.768392 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2DC20730>]}
[0m12:47:50.769398 [error] [Thread-5  ]: 7 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_comment_tmp .. [[31mERROR[0m in 0.49s]
[0m12:47:50.770397 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_comment_tmp
[0m12:47:50.770397 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__label_tmp
[0m12:47:50.770397 [info ] [Thread-5  ]: 11 of 40 START view model dbt_x_airflow.stg_github__label_tmp .................. [RUN]
[0m12:47:50.771406 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__label_tmp"
[0m12:47:50.771406 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__label_tmp
[0m12:47:50.771406 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__label_tmp
[0m12:47:50.774420 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__label_tmp"
[0m12:47:50.775417 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:50.775417 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__label_tmp
[0m12:47:50.777409 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__label_tmp"
[0m12:47:50.778419 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:47:50.782507 [debug] [Thread-5  ]: On model.github_source.stg_github__label_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`label`;


[0m12:47:50.867766 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_label`;


[0m12:47:50.867766 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: fe1a5a1a-5636-444b-a4cc-3e852ca4b705

[0m12:47:50.867766 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:50.867766 [debug] [Thread-3  ]: Runtime Error in model stg_github__issue_label_tmp (models\tmp\stg_github__issue_label_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: fe1a5a1a-5636-444b-a4cc-3e852ca4b705
  
[0m12:47:50.868767 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F1C5D90>]}
[0m12:47:50.868767 [error] [Thread-3  ]: 8 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_label_tmp .... [[31mERROR[0m in 0.53s]
[0m12:47:50.870284 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__issue_label_tmp
[0m12:47:50.870284 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__pull_request_review_tmp
[0m12:47:50.870284 [info ] [Thread-3  ]: 12 of 40 START view model dbt_x_airflow.stg_github__pull_request_review_tmp .... [RUN]
[0m12:47:50.871284 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__pull_request_review_tmp"
[0m12:47:50.871284 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__pull_request_review_tmp
[0m12:47:50.871284 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__pull_request_review_tmp
[0m12:47:50.874285 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__pull_request_review_tmp"
[0m12:47:50.875287 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:50.875287 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__pull_request_review_tmp
[0m12:47:50.878586 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__pull_request_review_tmp"
[0m12:47:50.879587 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:47:50.883582 [debug] [Thread-3  ]: On model.github_source.stg_github__pull_request_review_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_review_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_review_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request_review`;


[0m12:47:50.936965 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_merged_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_merged_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_merged`;


[0m12:47:50.938326 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: ff3a9f5a-5234-4546-b56f-c062c2f29bd4

[0m12:47:50.938326 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:50.938326 [debug] [Thread-4  ]: Runtime Error in model stg_github__issue_merged_tmp (models\tmp\stg_github__issue_merged_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: ff3a9f5a-5234-4546-b56f-c062c2f29bd4
  
[0m12:47:50.938326 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C3023BBB0>]}
[0m12:47:50.939321 [error] [Thread-4  ]: 9 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_merged_tmp ... [[31mERROR[0m in 0.57s]
[0m12:47:50.940319 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_merged_tmp
[0m12:47:50.940319 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__pull_request_tmp
[0m12:47:50.940319 [info ] [Thread-4  ]: 13 of 40 START view model dbt_x_airflow.stg_github__pull_request_tmp ........... [RUN]
[0m12:47:50.941327 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__pull_request_tmp"
[0m12:47:50.942321 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__pull_request_tmp
[0m12:47:50.942321 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__pull_request_tmp
[0m12:47:50.945333 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__pull_request_tmp"
[0m12:47:50.945333 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:50.946330 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__pull_request_tmp
[0m12:47:50.948329 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__pull_request_tmp"
[0m12:47:50.949530 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:47:50.953527 [debug] [Thread-4  ]: On model.github_source.stg_github__pull_request_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request`;


[0m12:47:51.010846 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue`;


[0m12:47:51.010846 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: fdda45d7-635e-49f5-b007-9864e11ba250

[0m12:47:51.011854 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:51.011854 [debug] [Thread-2  ]: Runtime Error in model stg_github__issue_tmp (models\tmp\stg_github__issue_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: fdda45d7-635e-49f5-b007-9864e11ba250
  
[0m12:47:51.011854 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2DC21850>]}
[0m12:47:51.011854 [error] [Thread-2  ]: 10 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_tmp ......... [[31mERROR[0m in 0.59s]
[0m12:47:51.013476 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue_tmp
[0m12:47:51.013476 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__repo_team_tmp
[0m12:47:51.014481 [info ] [Thread-2  ]: 14 of 40 START view model dbt_x_airflow.stg_github__repo_team_tmp .............. [RUN]
[0m12:47:51.014481 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__repo_team_tmp"
[0m12:47:51.015481 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__repo_team_tmp
[0m12:47:51.015481 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__repo_team_tmp
[0m12:47:51.019017 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__repo_team_tmp"
[0m12:47:51.020053 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:51.020053 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__repo_team_tmp
[0m12:47:51.023060 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__repo_team_tmp"
[0m12:47:51.023060 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:47:51.027059 [debug] [Thread-2  ]: On model.github_source.stg_github__repo_team_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repo_team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repo_team_tmp`
  OPTIONS()
  as 

select * 
from `airflow-docker-352518`.`github`.`repo_team`;


[0m12:47:51.267942 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`label`;


[0m12:47:51.267942 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 2ad9f424-3b30-4fbd-a26a-86fe10cc94cf

[0m12:47:51.267942 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:51.267942 [debug] [Thread-5  ]: Runtime Error in model stg_github__label_tmp (models\tmp\stg_github__label_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 2ad9f424-3b30-4fbd-a26a-86fe10cc94cf
  
[0m12:47:51.267942 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C30203580>]}
[0m12:47:51.268954 [error] [Thread-5  ]: 11 of 40 ERROR creating view model dbt_x_airflow.stg_github__label_tmp ......... [[31mERROR[0m in 0.50s]
[0m12:47:51.269947 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__label_tmp
[0m12:47:51.269947 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__repository_tmp
[0m12:47:51.270946 [info ] [Thread-5  ]: 15 of 40 START view model dbt_x_airflow.stg_github__repository_tmp ............. [RUN]
[0m12:47:51.271946 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__repository_tmp"
[0m12:47:51.271946 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__repository_tmp
[0m12:47:51.271946 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__repository_tmp
[0m12:47:51.275950 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__repository_tmp"
[0m12:47:51.276959 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:51.276959 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__repository_tmp
[0m12:47:51.280498 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__repository_tmp"
[0m12:47:51.281502 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:47:51.285536 [debug] [Thread-5  ]: On model.github_source.stg_github__repository_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repository_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repository_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`repository`;


[0m12:47:51.496923 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_review_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_review_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request_review`;


[0m12:47:51.496923 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: ebbd20c3-6db6-4161-a42c-fc141e94f1c5

[0m12:47:51.498502 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:51.498502 [debug] [Thread-3  ]: Runtime Error in model stg_github__pull_request_review_tmp (models\tmp\stg_github__pull_request_review_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: ebbd20c3-6db6-4161-a42c-fc141e94f1c5
  
[0m12:47:51.498502 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C302AFF10>]}
[0m12:47:51.498502 [error] [Thread-3  ]: 12 of 40 ERROR creating view model dbt_x_airflow.stg_github__pull_request_review_tmp  [[31mERROR[0m in 0.63s]
[0m12:47:51.499638 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__pull_request_review_tmp
[0m12:47:51.500647 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:47:51.500647 [info ] [Thread-3  ]: 16 of 40 START view model dbt_x_airflow.stg_github__requested_reviewer_history_tmp  [RUN]
[0m12:47:51.501670 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:47:51.501670 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:47:51.501670 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:47:51.505672 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:47:51.506681 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:51.506681 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:47:51.509712 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:47:51.510707 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:47:51.514713 [debug] [Thread-3  ]: On model.github_source.stg_github__requested_reviewer_history_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__requested_reviewer_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__requested_reviewer_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`requested_reviewer_history`;


[0m12:47:51.527596 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request`;


[0m12:47:51.527596 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 95045f74-e14a-49a0-8349-c0b3afd05a58

[0m12:47:51.527596 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:51.527596 [debug] [Thread-4  ]: Runtime Error in model stg_github__pull_request_tmp (models\tmp\stg_github__pull_request_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 95045f74-e14a-49a0-8349-c0b3afd05a58
  
[0m12:47:51.528604 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C302C8220>]}
[0m12:47:51.528604 [error] [Thread-4  ]: 13 of 40 ERROR creating view model dbt_x_airflow.stg_github__pull_request_tmp .. [[31mERROR[0m in 0.59s]
[0m12:47:51.528604 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__pull_request_tmp
[0m12:47:51.529928 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__team_tmp
[0m12:47:51.529928 [info ] [Thread-4  ]: 17 of 40 START view model dbt_x_airflow.stg_github__team_tmp ................... [RUN]
[0m12:47:51.530934 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__team_tmp"
[0m12:47:51.530934 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__team_tmp
[0m12:47:51.530934 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__team_tmp
[0m12:47:51.534936 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__team_tmp"
[0m12:47:51.535937 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:51.535937 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__team_tmp
[0m12:47:51.541468 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__team_tmp"
[0m12:47:51.542467 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:47:51.546466 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repo_team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repo_team_tmp`
  OPTIONS()
  as 

select * 
from `airflow-docker-352518`.`github`.`repo_team`;


[0m12:47:51.546466 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 52adf460-f75e-4527-8598-2296c1e2f1cb

[0m12:47:51.547467 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:51.547467 [debug] [Thread-2  ]: Runtime Error in model stg_github__repo_team_tmp (models\tmp\stg_github__repo_team_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 52adf460-f75e-4527-8598-2296c1e2f1cb
  
[0m12:47:51.547467 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C303166A0>]}
[0m12:47:51.547467 [error] [Thread-2  ]: 14 of 40 ERROR creating view model dbt_x_airflow.stg_github__repo_team_tmp ..... [[31mERROR[0m in 0.53s]
[0m12:47:51.548501 [debug] [Thread-4  ]: On model.github_source.stg_github__team_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__team_tmp`
  OPTIONS()
  as select * 
from `airflow-docker-352518`.`github`.`team`;


[0m12:47:51.549698 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__repo_team_tmp
[0m12:47:51.549698 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__user_tmp
[0m12:47:51.549698 [info ] [Thread-2  ]: 18 of 40 START view model dbt_x_airflow.stg_github__user_tmp ................... [RUN]
[0m12:47:51.550668 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__user_tmp"
[0m12:47:51.551667 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__user_tmp
[0m12:47:51.551667 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__user_tmp
[0m12:47:51.555710 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__user_tmp"
[0m12:47:51.556712 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:51.557712 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__user_tmp
[0m12:47:51.560131 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__user_tmp"
[0m12:47:51.561140 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:47:51.565171 [debug] [Thread-2  ]: On model.github_source.stg_github__user_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__user_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__user_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`user`;


[0m12:47:51.856937 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repository_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repository_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`repository`;


[0m12:47:51.856937 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 1e864c51-ee50-4cbb-a78c-1b4b5ebacdef

[0m12:47:51.858531 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:51.858531 [debug] [Thread-5  ]: Runtime Error in model stg_github__repository_tmp (models\tmp\stg_github__repository_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 1e864c51-ee50-4cbb-a78c-1b4b5ebacdef
  
[0m12:47:51.858531 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C302E0220>]}
[0m12:47:51.858531 [error] [Thread-5  ]: 15 of 40 ERROR creating view model dbt_x_airflow.stg_github__repository_tmp .... [[31mERROR[0m in 0.59s]
[0m12:47:51.860557 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__repository_tmp
[0m12:47:51.860557 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_assignee
[0m12:47:51.860557 [info ] [Thread-5  ]: 19 of 40 SKIP relation dbt_x_airflow.stg_github__issue_assignee ................ [[33mSKIP[0m]
[0m12:47:51.861563 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_assignee
[0m12:47:51.861563 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_closed_history
[0m12:47:51.862557 [info ] [Thread-5  ]: 20 of 40 SKIP relation dbt_x_airflow.stg_github__issue_closed_history .......... [[33mSKIP[0m]
[0m12:47:51.862557 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_closed_history
[0m12:47:51.862557 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m12:47:51.863556 [info ] [Thread-5  ]: 21 of 40 START table model dbt_x_airflow.pivoted_orders ........................ [RUN]
[0m12:47:51.863556 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m12:47:51.863556 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m12:47:51.864554 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m12:47:51.868572 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:47:51.869576 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:51.869576 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m12:47:51.872604 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:47:51.873570 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:47:51.878568 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m12:47:52.267334 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__requested_reviewer_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__requested_reviewer_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`requested_reviewer_history`;


[0m12:47:52.267334 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: a5af35c0-993b-4af6-ad3d-9a953686be1a

[0m12:47:52.268341 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:52.268341 [debug] [Thread-3  ]: Runtime Error in model stg_github__requested_reviewer_history_tmp (models\tmp\stg_github__requested_reviewer_history_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: a5af35c0-993b-4af6-ad3d-9a953686be1a
  
[0m12:47:52.268867 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C3134C700>]}
[0m12:47:52.268867 [error] [Thread-3  ]: 16 of 40 ERROR creating view model dbt_x_airflow.stg_github__requested_reviewer_history_tmp  [[31mERROR[0m in 0.77s]
[0m12:47:52.270883 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:47:52.270883 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.dim_customers
[0m12:47:52.271872 [info ] [Thread-3  ]: 22 of 40 START table model dbt_x_airflow.dim_customers ......................... [RUN]
[0m12:47:52.273105 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m12:47:52.273105 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m12:47:52.273105 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.dim_customers
[0m12:47:52.277110 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:47:52.278615 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:52.278615 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m12:47:52.280666 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:47:52.286667 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__team_tmp`
  OPTIONS()
  as select * 
from `airflow-docker-352518`.`github`.`team`;


[0m12:47:52.286667 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: ea0d9249-13a5-4d0d-982c-e5f26bb52473

[0m12:47:52.286667 [debug] [Thread-4  ]: finished collecting timing info
[0m12:47:52.287665 [debug] [Thread-4  ]: Runtime Error in model stg_github__team_tmp (models\tmp\stg_github__team_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: ea0d9249-13a5-4d0d-982c-e5f26bb52473
  
[0m12:47:52.287665 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C31391430>]}
[0m12:47:52.287665 [error] [Thread-4  ]: 17 of 40 ERROR creating view model dbt_x_airflow.stg_github__team_tmp .......... [[31mERROR[0m in 0.76s]
[0m12:47:52.289674 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__team_tmp
[0m12:47:52.290679 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_comment
[0m12:47:52.290679 [info ] [Thread-4  ]: 23 of 40 SKIP relation dbt_x_airflow.stg_github__issue_comment ................. [[33mSKIP[0m]
[0m12:47:52.291678 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_comment
[0m12:47:52.291678 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_label
[0m12:47:52.291678 [info ] [Thread-4  ]: 24 of 40 SKIP relation dbt_x_airflow.stg_github__issue_label ................... [[33mSKIP[0m]
[0m12:47:52.292785 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_label
[0m12:47:52.292785 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_merged
[0m12:47:52.292785 [info ] [Thread-4  ]: 25 of 40 SKIP relation dbt_x_airflow.stg_github__issue_merged .................. [[33mSKIP[0m]
[0m12:47:52.293782 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_merged
[0m12:47:52.294788 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue
[0m12:47:52.294788 [info ] [Thread-4  ]: 26 of 40 SKIP relation dbt_x_airflow.stg_github__issue ......................... [[33mSKIP[0m]
[0m12:47:52.294788 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue
[0m12:47:52.295782 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__label
[0m12:47:52.295782 [info ] [Thread-4  ]: 27 of 40 SKIP relation dbt_x_airflow.stg_github__label ......................... [[33mSKIP[0m]
[0m12:47:52.295782 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__label
[0m12:47:52.296781 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__pull_request_review
[0m12:47:52.296781 [info ] [Thread-4  ]: 28 of 40 SKIP relation dbt_x_airflow.stg_github__pull_request_review ........... [[33mSKIP[0m]
[0m12:47:52.297781 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__pull_request_review
[0m12:47:52.297781 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__pull_request
[0m12:47:52.297781 [info ] [Thread-4  ]: 29 of 40 SKIP relation dbt_x_airflow.stg_github__pull_request .................. [[33mSKIP[0m]
[0m12:47:52.297781 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__pull_request
[0m12:47:52.298781 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__repo_team
[0m12:47:52.298781 [info ] [Thread-4  ]: 30 of 40 SKIP relation dbt_x_airflow.stg_github__repo_team ..................... [[33mSKIP[0m]
[0m12:47:52.299822 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__repo_team
[0m12:47:52.299822 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__repository
[0m12:47:52.299822 [info ] [Thread-4  ]: 31 of 40 SKIP relation dbt_x_airflow.stg_github__repository .................... [[33mSKIP[0m]
[0m12:47:52.300823 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__repository
[0m12:47:52.300823 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__requested_reviewer_history
[0m12:47:52.301825 [info ] [Thread-4  ]: 32 of 40 SKIP relation dbt_x_airflow.stg_github__requested_reviewer_history .... [[33mSKIP[0m]
[0m12:47:52.301825 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__requested_reviewer_history
[0m12:47:52.301825 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__team
[0m12:47:52.301825 [info ] [Thread-4  ]: 33 of 40 SKIP relation dbt_x_airflow.stg_github__team .......................... [[33mSKIP[0m]
[0m12:47:52.303334 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__team
[0m12:47:52.304345 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_comments
[0m12:47:52.305346 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_comments
[0m12:47:52.305346 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_open_length
[0m12:47:52.305346 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_open_length
[0m12:47:52.305346 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_label_joined
[0m12:47:52.305346 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_label_joined
[0m12:47:52.306339 [debug] [Thread-4  ]: Began running node model.github.int_github__pull_request_times
[0m12:47:52.306339 [debug] [Thread-4  ]: Finished running node model.github.int_github__pull_request_times
[0m12:47:52.306339 [debug] [Thread-4  ]: Began running node model.github.int_github__repository_teams
[0m12:47:52.306339 [debug] [Thread-4  ]: Finished running node model.github.int_github__repository_teams
[0m12:47:52.307341 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_labels
[0m12:47:52.307341 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_labels
[0m12:47:52.319236 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__user_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__user_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`user`;


[0m12:47:52.319236 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 152f1dc3-9332-43ce-a779-14c1eedd32be

[0m12:47:52.320231 [debug] [Thread-2  ]: finished collecting timing info
[0m12:47:52.320231 [debug] [Thread-2  ]: Runtime Error in model stg_github__user_tmp (models\tmp\stg_github__user_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 152f1dc3-9332-43ce-a779-14c1eedd32be
  
[0m12:47:52.321232 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C313FFE80>]}
[0m12:47:52.321232 [error] [Thread-2  ]: 18 of 40 ERROR creating view model dbt_x_airflow.stg_github__user_tmp .......... [[31mERROR[0m in 0.77s]
[0m12:47:52.322228 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__user_tmp
[0m12:47:52.323229 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__user
[0m12:47:52.323229 [info ] [Thread-4  ]: 34 of 40 SKIP relation dbt_x_airflow.stg_github__user .......................... [[33mSKIP[0m]
[0m12:47:52.324228 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__user
[0m12:47:52.324228 [debug] [Thread-2  ]: Began running node model.github.int_github__issue_assignees
[0m12:47:52.324228 [debug] [Thread-2  ]: Finished running node model.github.int_github__issue_assignees
[0m12:47:52.325238 [debug] [Thread-4  ]: Began running node model.github.int_github__pull_request_reviewers
[0m12:47:52.325238 [debug] [Thread-4  ]: Finished running node model.github.int_github__pull_request_reviewers
[0m12:47:52.325238 [debug] [Thread-2  ]: Began running node model.github.int_github__issue_joined
[0m12:47:52.326231 [debug] [Thread-2  ]: Finished running node model.github.int_github__issue_joined
[0m12:47:52.326231 [debug] [Thread-4  ]: Began running node model.github.github__issues
[0m12:47:52.326231 [debug] [Thread-2  ]: Began running node model.github.github__pull_requests
[0m12:47:52.326231 [info ] [Thread-4  ]: 35 of 40 SKIP relation dbt_x_airflow.github__issues ............................ [[33mSKIP[0m]
[0m12:47:52.326231 [info ] [Thread-2  ]: 36 of 40 SKIP relation dbt_x_airflow.github__pull_requests ..................... [[33mSKIP[0m]
[0m12:47:52.327229 [debug] [Thread-4  ]: Finished running node model.github.github__issues
[0m12:47:52.328736 [debug] [Thread-2  ]: Finished running node model.github.github__pull_requests
[0m12:47:52.328736 [debug] [Thread-4  ]: Began running node model.github.github__daily_metrics
[0m12:47:52.328736 [info ] [Thread-4  ]: 37 of 40 SKIP relation dbt_x_airflow.github__daily_metrics ..................... [[33mSKIP[0m]
[0m12:47:52.329776 [debug] [Thread-4  ]: Finished running node model.github.github__daily_metrics
[0m12:47:52.330775 [debug] [Thread-2  ]: Began running node model.github.github__monthly_metrics
[0m12:47:52.330775 [info ] [Thread-2  ]: 38 of 40 SKIP relation dbt_x_airflow.github__monthly_metrics ................... [[33mSKIP[0m]
[0m12:47:52.330775 [debug] [Thread-4  ]: Began running node model.github.github__quarterly_metrics
[0m12:47:52.330775 [info ] [Thread-4  ]: 39 of 40 SKIP relation dbt_x_airflow.github__quarterly_metrics ................. [[33mSKIP[0m]
[0m12:47:52.331776 [debug] [Thread-2  ]: Finished running node model.github.github__monthly_metrics
[0m12:47:52.331776 [debug] [Thread-2  ]: Began running node model.github.github__weekly_metrics
[0m12:47:52.331776 [info ] [Thread-2  ]: 40 of 40 SKIP relation dbt_x_airflow.github__weekly_metrics .................... [[33mSKIP[0m]
[0m12:47:52.332783 [debug] [Thread-4  ]: Finished running node model.github.github__quarterly_metrics
[0m12:47:52.333789 [debug] [Thread-2  ]: Finished running node model.github.github__weekly_metrics
[0m12:47:52.721817 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:47:52.721817 [debug] [Thread-3  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m12:47:52.802008 [debug] [Thread-1  ]: finished collecting timing info
[0m12:47:52.802997 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F023DC0>]}
[0m12:47:52.802997 [info ] [Thread-1  ]: 1 of 40 OK created table model dbt_x_airflow.agg_transactions .................. [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.68s]
[0m12:47:52.804962 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m12:47:53.074002 [debug] [Thread-5  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]')
[0m12:47:54.481362 [debug] [Thread-5  ]: finished collecting timing info
[0m12:47:54.481362 [debug] [Thread-5  ]: Database Error in model pivoted_orders (models\prod\pivoted_orders.sql)
  No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
  compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:47:54.481362 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C31463C10>]}
[0m12:47:54.482362 [error] [Thread-5  ]: 21 of 40 ERROR creating table model dbt_x_airflow.pivoted_orders ............... [[31mERROR[0m in 2.62s]
[0m12:47:54.483364 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m12:47:55.894367 [debug] [Thread-3  ]: finished collecting timing info
[0m12:47:55.894367 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'cdf5e7c8-8587-46b8-aef2-3666bf5fbe43', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C3140A670>]}
[0m12:47:55.894367 [info ] [Thread-3  ]: 22 of 40 OK created table model dbt_x_airflow.dim_customers .................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.62s]
[0m12:47:55.895884 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m12:47:55.896960 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:47:55.898533 [info ] [MainThread]: 
[0m12:47:55.898533 [info ] [MainThread]: Finished running 17 view models, 23 table models in 0 hours 0 minutes and 7.82 seconds (7.82s).
[0m12:47:55.899540 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:47:55.899540 [debug] [MainThread]: Connection 'list_airflow-docker-352518' was properly closed.
[0m12:47:55.899540 [debug] [MainThread]: Connection 'list_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m12:47:55.899540 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m12:47:55.899540 [debug] [MainThread]: Connection 'model.github_source.stg_github__user_tmp' was properly closed.
[0m12:47:55.899540 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m12:47:55.900540 [debug] [MainThread]: Connection 'model.github_source.stg_github__team_tmp' was properly closed.
[0m12:47:55.900540 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m12:47:55.916020 [info ] [MainThread]: 
[0m12:47:55.917005 [info ] [MainThread]: [31mCompleted with 15 errors and 0 warnings:[0m
[0m12:47:55.917005 [info ] [MainThread]: 
[0m12:47:55.917005 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_assignee_tmp (models\tmp\stg_github__issue_assignee_tmp.sql)[0m
[0m12:47:55.918480 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.919479 [error] [MainThread]:   
[0m12:47:55.919479 [error] [MainThread]:   Location: US
[0m12:47:55.920476 [error] [MainThread]:   Job ID: 6d409af2-72de-463a-a9e4-e40971af67b8
[0m12:47:55.920476 [error] [MainThread]:   
[0m12:47:55.921476 [info ] [MainThread]: 
[0m12:47:55.921476 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_closed_history_tmp (models\tmp\stg_github__issue_closed_history_tmp.sql)[0m
[0m12:47:55.922477 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.922477 [error] [MainThread]:   
[0m12:47:55.923478 [error] [MainThread]:   Location: US
[0m12:47:55.923478 [error] [MainThread]:   Job ID: a5c25812-4284-4a17-9600-2f114a7ff203
[0m12:47:55.924481 [error] [MainThread]:   
[0m12:47:55.924481 [info ] [MainThread]: 
[0m12:47:55.925476 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_comment_tmp (models\tmp\stg_github__issue_comment_tmp.sql)[0m
[0m12:47:55.925476 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.926477 [error] [MainThread]:   
[0m12:47:55.926477 [error] [MainThread]:   Location: US
[0m12:47:55.927477 [error] [MainThread]:   Job ID: bee4fd94-4f34-46eb-b174-dd97641e18bb
[0m12:47:55.927477 [error] [MainThread]:   
[0m12:47:55.928475 [info ] [MainThread]: 
[0m12:47:55.928475 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_label_tmp (models\tmp\stg_github__issue_label_tmp.sql)[0m
[0m12:47:55.928475 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.928475 [error] [MainThread]:   
[0m12:47:55.929972 [error] [MainThread]:   Location: US
[0m12:47:55.929972 [error] [MainThread]:   Job ID: fe1a5a1a-5636-444b-a4cc-3e852ca4b705
[0m12:47:55.929972 [error] [MainThread]:   
[0m12:47:55.930972 [info ] [MainThread]: 
[0m12:47:55.930972 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_merged_tmp (models\tmp\stg_github__issue_merged_tmp.sql)[0m
[0m12:47:55.931970 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.931970 [error] [MainThread]:   
[0m12:47:55.931970 [error] [MainThread]:   Location: US
[0m12:47:55.931970 [error] [MainThread]:   Job ID: ff3a9f5a-5234-4546-b56f-c062c2f29bd4
[0m12:47:55.933211 [error] [MainThread]:   
[0m12:47:55.933211 [info ] [MainThread]: 
[0m12:47:55.933211 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_tmp (models\tmp\stg_github__issue_tmp.sql)[0m
[0m12:47:55.934212 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.935209 [error] [MainThread]:   
[0m12:47:55.935209 [error] [MainThread]:   Location: US
[0m12:47:55.935209 [error] [MainThread]:   Job ID: fdda45d7-635e-49f5-b007-9864e11ba250
[0m12:47:55.936212 [error] [MainThread]:   
[0m12:47:55.936212 [info ] [MainThread]: 
[0m12:47:55.937212 [error] [MainThread]: [33mRuntime Error in model stg_github__label_tmp (models\tmp\stg_github__label_tmp.sql)[0m
[0m12:47:55.937212 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.937212 [error] [MainThread]:   
[0m12:47:55.938719 [error] [MainThread]:   Location: US
[0m12:47:55.938719 [error] [MainThread]:   Job ID: 2ad9f424-3b30-4fbd-a26a-86fe10cc94cf
[0m12:47:55.938719 [error] [MainThread]:   
[0m12:47:55.938719 [info ] [MainThread]: 
[0m12:47:55.940107 [error] [MainThread]: [33mRuntime Error in model stg_github__pull_request_review_tmp (models\tmp\stg_github__pull_request_review_tmp.sql)[0m
[0m12:47:55.940107 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.941112 [error] [MainThread]:   
[0m12:47:55.941112 [error] [MainThread]:   Location: US
[0m12:47:55.941112 [error] [MainThread]:   Job ID: ebbd20c3-6db6-4161-a42c-fc141e94f1c5
[0m12:47:55.941112 [error] [MainThread]:   
[0m12:47:55.942114 [info ] [MainThread]: 
[0m12:47:55.942114 [error] [MainThread]: [33mRuntime Error in model stg_github__pull_request_tmp (models\tmp\stg_github__pull_request_tmp.sql)[0m
[0m12:47:55.943116 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.944117 [error] [MainThread]:   
[0m12:47:55.944117 [error] [MainThread]:   Location: US
[0m12:47:55.945112 [error] [MainThread]:   Job ID: 95045f74-e14a-49a0-8349-c0b3afd05a58
[0m12:47:55.945112 [error] [MainThread]:   
[0m12:47:55.946113 [info ] [MainThread]: 
[0m12:47:55.946113 [error] [MainThread]: [33mRuntime Error in model stg_github__repo_team_tmp (models\tmp\stg_github__repo_team_tmp.sql)[0m
[0m12:47:55.947112 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.947112 [error] [MainThread]:   
[0m12:47:55.947112 [error] [MainThread]:   Location: US
[0m12:47:55.947112 [error] [MainThread]:   Job ID: 52adf460-f75e-4527-8598-2296c1e2f1cb
[0m12:47:55.948554 [error] [MainThread]:   
[0m12:47:55.948554 [info ] [MainThread]: 
[0m12:47:55.948554 [error] [MainThread]: [33mRuntime Error in model stg_github__repository_tmp (models\tmp\stg_github__repository_tmp.sql)[0m
[0m12:47:55.949557 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.949557 [error] [MainThread]:   
[0m12:47:55.950556 [error] [MainThread]:   Location: US
[0m12:47:55.950556 [error] [MainThread]:   Job ID: 1e864c51-ee50-4cbb-a78c-1b4b5ebacdef
[0m12:47:55.951561 [error] [MainThread]:   
[0m12:47:55.951561 [info ] [MainThread]: 
[0m12:47:55.952558 [error] [MainThread]: [33mRuntime Error in model stg_github__requested_reviewer_history_tmp (models\tmp\stg_github__requested_reviewer_history_tmp.sql)[0m
[0m12:47:55.952558 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.953558 [error] [MainThread]:   
[0m12:47:55.953558 [error] [MainThread]:   Location: US
[0m12:47:55.953558 [error] [MainThread]:   Job ID: a5af35c0-993b-4af6-ad3d-9a953686be1a
[0m12:47:55.953558 [error] [MainThread]:   
[0m12:47:55.954558 [info ] [MainThread]: 
[0m12:47:55.954558 [error] [MainThread]: [33mRuntime Error in model stg_github__team_tmp (models\tmp\stg_github__team_tmp.sql)[0m
[0m12:47:55.955558 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.955558 [error] [MainThread]:   
[0m12:47:55.956558 [error] [MainThread]:   Location: US
[0m12:47:55.956558 [error] [MainThread]:   Job ID: ea0d9249-13a5-4d0d-982c-e5f26bb52473
[0m12:47:55.956558 [error] [MainThread]:   
[0m12:47:55.957556 [info ] [MainThread]: 
[0m12:47:55.957556 [error] [MainThread]: [33mRuntime Error in model stg_github__user_tmp (models\tmp\stg_github__user_tmp.sql)[0m
[0m12:47:55.958557 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:47:55.958557 [error] [MainThread]:   
[0m12:47:55.958557 [error] [MainThread]:   Location: US
[0m12:47:55.958557 [error] [MainThread]:   Job ID: 152f1dc3-9332-43ce-a779-14c1eedd32be
[0m12:47:55.959751 [error] [MainThread]:   
[0m12:47:55.959751 [info ] [MainThread]: 
[0m12:47:55.959751 [error] [MainThread]: [33mDatabase Error in model pivoted_orders (models\prod\pivoted_orders.sql)[0m
[0m12:47:55.960751 [error] [MainThread]:   No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
[0m12:47:55.960751 [error] [MainThread]:   compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:47:55.961751 [info ] [MainThread]: 
[0m12:47:55.961751 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=15 SKIP=20 TOTAL=40
[0m12:47:55.962752 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C313A9670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F1B6370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000021C2F1B66D0>]}


============================== 2022-08-07 12:48:47.255362 | 3b6e3753-4a4f-46e0-8f07-9ac136e43419 ==============================
[0m12:48:47.255362 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:48:47.256359 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m12:48:47.257361 [debug] [MainThread]: Tracking: tracking
[0m12:48:47.286426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B8527C92B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B8527C9670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B8527C9370>]}
[0m12:48:47.287428 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The default package install path has changed from `dbt_modules` to
`dbt_packages`. Please update `clean-targets` in `dbt_project.yml` and check
`.gitignore` as well. Or, set `packages-install-path: dbt_modules` if you'd like
to keep the current value.
[0m12:48:47.288473 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '3b6e3753-4a4f-46e0-8f07-9ac136e43419', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B8527C92B0>]}
[0m12:48:47.391739 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B852767400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B852767CD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002B852767F10>]}


============================== 2022-08-07 12:48:56.302756 | 6c2f5054-4a6e-4050-88d8-c02142e37772 ==============================
[0m12:48:56.302756 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:48:56.303759 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:48:56.303759 [debug] [MainThread]: Tracking: tracking
[0m12:48:56.316766 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E3B772E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E3B77580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E3B77790>]}
[0m12:48:56.357381 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m12:48:56.358380 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E3B77790>]}
[0m12:48:56.468420 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m12:48:56.469901 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:48:56.488481 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:48:56.492527 [debug] [MainThread]: Parsing macros\etc.sql
[0m12:48:56.494529 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:48:56.496527 [debug] [MainThread]: Parsing macros\materializations\copy.sql
[0m12:48:56.498528 [debug] [MainThread]: Parsing macros\materializations\incremental.sql
[0m12:48:56.511370 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:48:56.513364 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:48:56.515334 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:48:56.518470 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:48:56.521437 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:48:56.521437 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:48:56.522470 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:48:56.523469 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:48:56.523469 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:48:56.523469 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:48:56.524436 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:48:56.524436 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:48:56.525455 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:48:56.525455 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:48:56.526455 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:48:56.526455 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:48:56.527454 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:48:56.528452 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:48:56.539085 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:48:56.547390 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:48:56.549435 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:48:56.551435 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:48:56.557403 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:48:56.561034 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:48:56.572471 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:48:56.574474 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:48:56.580494 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:48:56.584492 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:48:56.585495 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:48:56.586497 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:48:56.586497 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:48:56.587503 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:48:56.589040 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:48:56.590090 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:48:56.592083 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:48:56.593723 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:48:56.597744 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:48:56.600982 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:48:56.609264 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:48:56.610285 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:48:56.621838 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:48:56.634966 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:48:56.636622 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:48:56.640971 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:48:56.643969 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:48:56.645967 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:48:56.646967 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:48:56.650615 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:48:56.664932 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:48:56.670528 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:48:56.681132 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:48:56.689509 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:48:56.691557 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:48:56.704574 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:48:56.705573 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:48:56.709120 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:48:56.711169 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:48:56.712126 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:48:56.713635 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:48:56.714643 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:48:56.715643 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:48:56.723165 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:48:56.725164 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:48:56.726162 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:48:56.727178 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:48:56.728702 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:48:56.728702 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:48:56.729722 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:48:56.730722 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:48:56.731723 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:48:56.732722 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:48:56.734723 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:48:56.735722 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:48:56.736723 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:48:56.737245 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:48:56.738833 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:48:56.738833 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:48:56.740848 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:48:56.743376 [debug] [MainThread]: Parsing macros\cross_db_utils\any_value.sql
[0m12:48:56.744400 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m12:48:56.746397 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m12:48:56.748396 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m12:48:56.749941 [debug] [MainThread]: Parsing macros\cross_db_utils\bool_or.sql
[0m12:48:56.751960 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m12:48:56.753960 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_bool_to_text.sql
[0m12:48:56.754965 [debug] [MainThread]: Parsing macros\cross_db_utils\concat.sql
[0m12:48:56.755962 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m12:48:56.758487 [debug] [MainThread]: Parsing macros\cross_db_utils\datatypes.sql
[0m12:48:56.763506 [debug] [MainThread]: Parsing macros\cross_db_utils\dateadd.sql
[0m12:48:56.766506 [debug] [MainThread]: Parsing macros\cross_db_utils\datediff.sql
[0m12:48:56.773558 [debug] [MainThread]: Parsing macros\cross_db_utils\date_trunc.sql
[0m12:48:56.774578 [debug] [MainThread]: Parsing macros\cross_db_utils\escape_single_quotes.sql
[0m12:48:56.776578 [debug] [MainThread]: Parsing macros\cross_db_utils\except.sql
[0m12:48:56.777579 [debug] [MainThread]: Parsing macros\cross_db_utils\hash.sql
[0m12:48:56.778578 [debug] [MainThread]: Parsing macros\cross_db_utils\identifier.sql
[0m12:48:56.780123 [debug] [MainThread]: Parsing macros\cross_db_utils\intersect.sql
[0m12:48:56.781143 [debug] [MainThread]: Parsing macros\cross_db_utils\last_day.sql
[0m12:48:56.784143 [debug] [MainThread]: Parsing macros\cross_db_utils\length.sql
[0m12:48:56.785143 [debug] [MainThread]: Parsing macros\cross_db_utils\listagg.sql
[0m12:48:56.792321 [debug] [MainThread]: Parsing macros\cross_db_utils\literal.sql
[0m12:48:56.793285 [debug] [MainThread]: Parsing macros\cross_db_utils\position.sql
[0m12:48:56.794285 [debug] [MainThread]: Parsing macros\cross_db_utils\replace.sql
[0m12:48:56.795317 [debug] [MainThread]: Parsing macros\cross_db_utils\right.sql
[0m12:48:56.797322 [debug] [MainThread]: Parsing macros\cross_db_utils\safe_cast.sql
[0m12:48:56.798317 [debug] [MainThread]: Parsing macros\cross_db_utils\split_part.sql
[0m12:48:56.802952 [debug] [MainThread]: Parsing macros\cross_db_utils\width_bucket.sql
[0m12:48:56.806995 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_ephemeral.sql
[0m12:48:56.808544 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_relation.sql
[0m12:48:56.809622 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m12:48:56.811672 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m12:48:56.812668 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m12:48:56.813668 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m12:48:56.816668 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m12:48:56.817945 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m12:48:56.819951 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m12:48:56.820990 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m12:48:56.826991 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m12:48:56.829687 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m12:48:56.830688 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m12:48:56.831684 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m12:48:56.833699 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m12:48:56.835734 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m12:48:56.837224 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m12:48:56.839769 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m12:48:56.840774 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m12:48:56.842774 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m12:48:56.843774 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m12:48:56.844774 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m12:48:56.845774 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m12:48:56.846774 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m12:48:56.865795 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m12:48:56.869300 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m12:48:56.875311 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m12:48:56.878814 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m12:48:56.883819 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m12:48:56.885819 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m12:48:56.888324 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m12:48:56.890386 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m12:48:56.893390 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m12:48:56.898390 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m12:48:56.899900 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m12:48:56.900905 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m12:48:56.901905 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m12:48:56.906904 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m12:48:56.908409 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m12:48:56.909415 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m12:48:56.912414 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m12:48:56.913415 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m12:48:56.917414 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m12:48:56.919924 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m12:48:56.929650 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m12:48:56.935653 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m12:48:56.937160 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m12:48:56.938665 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m12:48:56.940671 [debug] [MainThread]: Parsing macros\add_dbt_source_relation.sql
[0m12:48:56.941671 [debug] [MainThread]: Parsing macros\add_pass_through_columns.sql
[0m12:48:56.943671 [debug] [MainThread]: Parsing macros\array_agg.sql
[0m12:48:56.944671 [debug] [MainThread]: Parsing macros\calculated_fields.sql
[0m12:48:56.944671 [debug] [MainThread]: Parsing macros\ceiling.sql
[0m12:48:56.945671 [debug] [MainThread]: Parsing macros\collect_freshness.sql
[0m12:48:56.949175 [debug] [MainThread]: Parsing macros\dummy_coalesce_value.sql
[0m12:48:56.952180 [debug] [MainThread]: Parsing macros\empty_variable_warning.sql
[0m12:48:56.952180 [debug] [MainThread]: Parsing macros\enabled_vars.sql
[0m12:48:56.953685 [debug] [MainThread]: Parsing macros\enabled_vars_one_true.sql
[0m12:48:56.954690 [debug] [MainThread]: Parsing macros\fill_pass_through_columns.sql
[0m12:48:56.955691 [debug] [MainThread]: Parsing macros\fill_staging_columns.sql
[0m12:48:56.959193 [debug] [MainThread]: Parsing macros\first_value.sql
[0m12:48:56.961203 [debug] [MainThread]: Parsing macros\json_extract.sql
[0m12:48:56.963204 [debug] [MainThread]: Parsing macros\json_parse.sql
[0m12:48:56.968712 [debug] [MainThread]: Parsing macros\max_bool.sql
[0m12:48:56.969717 [debug] [MainThread]: Parsing macros\percentile.sql
[0m12:48:56.972717 [debug] [MainThread]: Parsing macros\persist_pass_through_columns.sql
[0m12:48:56.973718 [debug] [MainThread]: Parsing macros\pivot_json_extract.sql
[0m12:48:56.974718 [debug] [MainThread]: Parsing macros\remove_prefix_from_columns.sql
[0m12:48:56.975717 [debug] [MainThread]: Parsing macros\seed_data_helper.sql
[0m12:48:56.976718 [debug] [MainThread]: Parsing macros\snowflake_seed_data.sql
[0m12:48:56.977719 [debug] [MainThread]: Parsing macros\source_relation.sql
[0m12:48:56.979222 [debug] [MainThread]: Parsing macros\string_agg.sql
[0m12:48:56.982227 [debug] [MainThread]: Parsing macros\timestamp_add.sql
[0m12:48:56.984737 [debug] [MainThread]: Parsing macros\timestamp_diff.sql
[0m12:48:56.992250 [debug] [MainThread]: Parsing macros\try_cast.sql
[0m12:48:56.996251 [debug] [MainThread]: Parsing macros\union_data.sql
[0m12:48:57.001761 [debug] [MainThread]: Parsing macros\union_relations.sql
[0m12:48:57.008761 [debug] [MainThread]: Parsing macros\get_issue_assignee_columns.sql
[0m12:48:57.009266 [debug] [MainThread]: Parsing macros\get_issue_closed_history_columns.sql
[0m12:48:57.011271 [debug] [MainThread]: Parsing macros\get_issue_columns.sql
[0m12:48:57.013775 [debug] [MainThread]: Parsing macros\get_issue_comment_columns.sql
[0m12:48:57.015780 [debug] [MainThread]: Parsing macros\get_issue_label_columns.sql
[0m12:48:57.016780 [debug] [MainThread]: Parsing macros\get_issue_merged_columns.sql
[0m12:48:57.017780 [debug] [MainThread]: Parsing macros\get_label_columns.sql
[0m12:48:57.019284 [debug] [MainThread]: Parsing macros\get_pull_request_columns.sql
[0m12:48:57.022299 [debug] [MainThread]: Parsing macros\get_pull_request_review_columns.sql
[0m12:48:57.024300 [debug] [MainThread]: Parsing macros\get_repository_columns.sql
[0m12:48:57.026300 [debug] [MainThread]: Parsing macros\get_repo_team_columns.sql
[0m12:48:57.027300 [debug] [MainThread]: Parsing macros\get_requested_reviewer_history_columns.sql
[0m12:48:57.028804 [debug] [MainThread]: Parsing macros\get_team_columns.sql
[0m12:48:57.030810 [debug] [MainThread]: Parsing macros\get_user_columns.sql
[0m12:48:57.435397 [debug] [MainThread]: 1699: static parser successfully parsed prod\agg_transactions.sql
[0m12:48:57.445856 [debug] [MainThread]: 1699: static parser successfully parsed prod\dim_customers.sql
[0m12:48:57.446889 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m12:48:57.451376 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m12:48:57.452375 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_customers.sql
[0m12:48:57.454342 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_orders.sql
[0m12:48:57.455376 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m12:48:57.459945 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m12:48:57.485538 [debug] [MainThread]: 1603: static parser failed on github__daily_metrics.sql
[0m12:48:57.492144 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__daily_metrics.sql
[0m12:48:57.493396 [debug] [MainThread]: 1603: static parser failed on github__issues.sql
[0m12:48:57.496398 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__issues.sql
[0m12:48:57.497363 [debug] [MainThread]: 1603: static parser failed on github__monthly_metrics.sql
[0m12:48:57.500399 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__monthly_metrics.sql
[0m12:48:57.501396 [debug] [MainThread]: 1699: static parser successfully parsed github__pull_requests.sql
[0m12:48:57.503395 [debug] [MainThread]: 1603: static parser failed on github__quarterly_metrics.sql
[0m12:48:57.507874 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__quarterly_metrics.sql
[0m12:48:57.507874 [debug] [MainThread]: 1603: static parser failed on github__weekly_metrics.sql
[0m12:48:57.511916 [debug] [MainThread]: 1602: parser fallback to jinja rendering on github__weekly_metrics.sql
[0m12:48:57.512918 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_assignees.sql
[0m12:48:57.518512 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_assignees.sql
[0m12:48:57.519551 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_comments.sql
[0m12:48:57.523994 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_comments.sql
[0m12:48:57.524962 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_joined.sql
[0m12:48:57.534627 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_joined.sql
[0m12:48:57.535623 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_labels.sql
[0m12:48:57.540692 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_labels.sql
[0m12:48:57.541691 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_label_joined.sql
[0m12:48:57.546697 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_label_joined.sql
[0m12:48:57.547697 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__issue_open_length.sql
[0m12:48:57.555756 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__issue_open_length.sql
[0m12:48:57.556756 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_reviewers.sql
[0m12:48:57.562279 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_reviewers.sql
[0m12:48:57.563279 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__pull_request_times.sql
[0m12:48:57.571293 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__pull_request_times.sql
[0m12:48:57.572294 [debug] [MainThread]: 1603: static parser failed on intermediate\int_github__repository_teams.sql
[0m12:48:57.579855 [debug] [MainThread]: 1602: parser fallback to jinja rendering on intermediate\int_github__repository_teams.sql
[0m12:48:57.589423 [debug] [MainThread]: 1603: static parser failed on stg_github__issue.sql
[0m12:48:57.650267 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue.sql
[0m12:48:57.651277 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_assignee.sql
[0m12:48:57.658791 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_assignee.sql
[0m12:48:57.659803 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_closed_history.sql
[0m12:48:57.667802 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_closed_history.sql
[0m12:48:57.669315 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_comment.sql
[0m12:48:57.677849 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_comment.sql
[0m12:48:57.679362 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_label.sql
[0m12:48:57.686373 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_label.sql
[0m12:48:57.687378 [debug] [MainThread]: 1603: static parser failed on stg_github__issue_merged.sql
[0m12:48:57.695389 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__issue_merged.sql
[0m12:48:57.696389 [debug] [MainThread]: 1603: static parser failed on stg_github__label.sql
[0m12:48:57.706435 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__label.sql
[0m12:48:57.707436 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request.sql
[0m12:48:57.720501 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request.sql
[0m12:48:57.721501 [debug] [MainThread]: 1603: static parser failed on stg_github__pull_request_review.sql
[0m12:48:57.732022 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__pull_request_review.sql
[0m12:48:57.733536 [debug] [MainThread]: 1603: static parser failed on stg_github__repository.sql
[0m12:48:57.744700 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repository.sql
[0m12:48:57.745701 [debug] [MainThread]: 1603: static parser failed on stg_github__repo_team.sql
[0m12:48:57.754714 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__repo_team.sql
[0m12:48:57.755715 [debug] [MainThread]: 1603: static parser failed on stg_github__requested_reviewer_history.sql
[0m12:48:57.763751 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__requested_reviewer_history.sql
[0m12:48:57.764762 [debug] [MainThread]: 1603: static parser failed on stg_github__team.sql
[0m12:48:57.775299 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__team.sql
[0m12:48:57.776299 [debug] [MainThread]: 1603: static parser failed on stg_github__user.sql
[0m12:48:57.784316 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stg_github__user.sql
[0m12:48:57.785316 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_assignee_tmp.sql
[0m12:48:57.788900 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_assignee_tmp.sql
[0m12:48:57.789906 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:48:57.793584 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_closed_history_tmp.sql
[0m12:48:57.794596 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_comment_tmp.sql
[0m12:48:57.799107 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_comment_tmp.sql
[0m12:48:57.800123 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_label_tmp.sql
[0m12:48:57.805135 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_label_tmp.sql
[0m12:48:57.806136 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_merged_tmp.sql
[0m12:48:57.809661 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_merged_tmp.sql
[0m12:48:57.810660 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__issue_tmp.sql
[0m12:48:57.814661 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__issue_tmp.sql
[0m12:48:57.815661 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__label_tmp.sql
[0m12:48:57.819174 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__label_tmp.sql
[0m12:48:57.820185 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_review_tmp.sql
[0m12:48:57.823699 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_review_tmp.sql
[0m12:48:57.823699 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__pull_request_tmp.sql
[0m12:48:57.827710 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__pull_request_tmp.sql
[0m12:48:57.829223 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repository_tmp.sql
[0m12:48:57.834248 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repository_tmp.sql
[0m12:48:57.835248 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__repo_team_tmp.sql
[0m12:48:57.838793 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__repo_team_tmp.sql
[0m12:48:57.839828 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:48:57.843826 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__requested_reviewer_history_tmp.sql
[0m12:48:57.844828 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__team_tmp.sql
[0m12:48:57.847827 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__team_tmp.sql
[0m12:48:57.848824 [debug] [MainThread]: 1603: static parser failed on tmp\stg_github__user_tmp.sql
[0m12:48:57.852243 [debug] [MainThread]: 1602: parser fallback to jinja rendering on tmp\stg_github__user_tmp.sql
[0m12:48:58.108454 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E5131F70>]}
[0m12:48:58.169472 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E51050D0>]}
[0m12:48:58.169472 [info ] [MainThread]: Found 49 models, 41 tests, 0 snapshots, 0 analyses, 615 macros, 0 operations, 0 seed files, 17 sources, 1 exposure, 0 metrics
[0m12:48:58.170507 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E5131FA0>]}
[0m12:48:58.173480 [info ] [MainThread]: 
[0m12:48:58.174470 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:48:58.177471 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m12:48:58.177471 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:48:58.985767 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m12:48:58.986759 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:48:59.845178 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E50E5520>]}
[0m12:48:59.846167 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m12:48:59.846167 [info ] [MainThread]: 
[0m12:48:59.852469 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m12:48:59.853470 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m12:48:59.853470 [info ] [Thread-1  ]: 1 of 40 START table model dbt_x_airflow.agg_transactions ....................... [RUN]
[0m12:48:59.853470 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m12:48:59.854470 [info ] [Thread-2  ]: 2 of 40 START view model dbt_x_airflow.stg_customers ........................... [RUN]
[0m12:48:59.854470 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m12:48:59.854470 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_assignee_tmp
[0m12:48:59.855470 [info ] [Thread-3  ]: 3 of 40 START view model dbt_x_airflow.stg_orders .............................. [RUN]
[0m12:48:59.855470 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m12:48:59.856473 [info ] [Thread-4  ]: 4 of 40 START view model dbt_x_airflow.stg_payments ............................ [RUN]
[0m12:48:59.856473 [info ] [Thread-5  ]: 5 of 40 START view model dbt_x_airflow.stg_github__issue_assignee_tmp .......... [RUN]
[0m12:48:59.857472 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m12:48:59.857472 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m12:48:59.858473 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m12:48:59.858473 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m12:48:59.858473 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m12:48:59.858473 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m12:48:59.861971 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:48:59.861971 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m12:48:59.861971 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m12:48:59.865342 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:48:59.866343 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_assignee_tmp"
[0m12:48:59.866343 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m12:48:59.866343 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m12:48:59.867341 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_assignee_tmp
[0m12:48:59.867341 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m12:48:59.870903 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:48:59.870903 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_assignee_tmp
[0m12:48:59.873904 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:48:59.876889 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_assignee_tmp"
[0m12:48:59.879560 [debug] [Thread-1  ]: finished collecting timing info
[0m12:48:59.879560 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m12:48:59.896032 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m12:48:59.896032 [debug] [Thread-3  ]: finished collecting timing info
[0m12:48:59.896999 [debug] [Thread-5  ]: finished collecting timing info
[0m12:48:59.896999 [debug] [Thread-2  ]: finished collecting timing info
[0m12:48:59.896999 [debug] [Thread-4  ]: finished collecting timing info
[0m12:48:59.896999 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m12:48:59.896999 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_assignee_tmp
[0m12:48:59.896999 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m12:48:59.898506 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m12:48:59.940372 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_assignee_tmp"
[0m12:48:59.942445 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:48:59.943404 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:48:59.947409 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:48:59.949840 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m12:48:59.950839 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m12:48:59.950839 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m12:48:59.951842 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m12:48:59.955232 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m12:48:59.955232 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    CONCAT('$', ROUND(amount/100, 1)) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m12:48:59.956237 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_assignee_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_assignee_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_assignee_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_assignee`;


[0m12:48:59.956237 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m12:49:00.819525 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:49:00.820522 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m12:49:00.867023 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_assignee_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_assignee_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_assignee`;


[0m12:49:00.867023 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: a3681aac-cb0b-410b-9e70-0fd965acc7c2

[0m12:49:00.867023 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:00.868551 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_assignee_tmp (models\tmp\stg_github__issue_assignee_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: a3681aac-cb0b-410b-9e70-0fd965acc7c2
  
[0m12:49:00.868551 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E3B5BEB0>]}
[0m12:49:00.868551 [error] [Thread-5  ]: 5 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_assignee_tmp . [[31mERROR[0m in 1.00s]
[0m12:49:00.869566 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_assignee_tmp
[0m12:49:00.869566 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_closed_history_tmp
[0m12:49:00.870556 [info ] [Thread-5  ]: 6 of 40 START view model dbt_x_airflow.stg_github__issue_closed_history_tmp .... [RUN]
[0m12:49:00.870556 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:49:00.871557 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_closed_history_tmp
[0m12:49:00.871557 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_closed_history_tmp
[0m12:49:00.875558 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:49:00.876557 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:00.876557 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_closed_history_tmp
[0m12:49:00.880092 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_closed_history_tmp"
[0m12:49:00.881090 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:49:00.885375 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_closed_history_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_closed_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_closed_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_closed_history`;


[0m12:49:01.987416 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_closed_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_closed_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_closed_history`;


[0m12:49:01.991975 [debug] [Thread-3  ]: finished collecting timing info
[0m12:49:01.991975 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 44ced79e-9529-44a4-ac30-c337fbae097f

[0m12:49:01.994701 [debug] [Thread-4  ]: finished collecting timing info
[0m12:49:01.994701 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E501DFD0>]}
[0m12:49:01.995734 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:01.995734 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E501D520>]}
[0m12:49:01.995734 [info ] [Thread-3  ]: 3 of 40 OK created view model dbt_x_airflow.stg_orders ......................... [[32mOK[0m in 2.14s]
[0m12:49:01.996736 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_closed_history_tmp (models\tmp\stg_github__issue_closed_history_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 44ced79e-9529-44a4-ac30-c337fbae097f
  
[0m12:49:01.996736 [info ] [Thread-4  ]: 4 of 40 OK created view model dbt_x_airflow.stg_payments ....................... [[32mOK[0m in 2.13s]
[0m12:49:01.997723 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E4FD35E0>]}
[0m12:49:01.997723 [error] [Thread-5  ]: 6 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_closed_history_tmp  [[31mERROR[0m in 1.13s]
[0m12:49:01.998701 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m12:49:01.998701 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__issue_comment_tmp
[0m12:49:01.998701 [info ] [Thread-3  ]: 7 of 40 START view model dbt_x_airflow.stg_github__issue_comment_tmp ........... [RUN]
[0m12:49:01.999997 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m12:49:02.001010 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_label_tmp
[0m12:49:02.001010 [info ] [Thread-4  ]: 8 of 40 START view model dbt_x_airflow.stg_github__issue_label_tmp ............. [RUN]
[0m12:49:02.002002 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_closed_history_tmp
[0m12:49:02.002002 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__issue_merged_tmp
[0m12:49:02.002002 [info ] [Thread-5  ]: 9 of 40 START view model dbt_x_airflow.stg_github__issue_merged_tmp ............ [RUN]
[0m12:49:02.003002 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_comment_tmp"
[0m12:49:02.003002 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__issue_comment_tmp
[0m12:49:02.003002 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_label_tmp"
[0m12:49:02.004002 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__issue_comment_tmp
[0m12:49:02.004002 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__issue_label_tmp
[0m12:49:02.004002 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_merged_tmp"
[0m12:49:02.007017 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__issue_label_tmp
[0m12:49:02.007017 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__issue_merged_tmp
[0m12:49:02.011366 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__issue_label_tmp"
[0m12:49:02.011366 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__issue_merged_tmp
[0m12:49:02.016369 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__issue_merged_tmp"
[0m12:49:02.021888 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__issue_comment_tmp"
[0m12:49:02.021888 [debug] [Thread-4  ]: finished collecting timing info
[0m12:49:02.022907 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__issue_label_tmp
[0m12:49:02.025916 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_label_tmp"
[0m12:49:02.025916 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:02.026914 [debug] [Thread-3  ]: finished collecting timing info
[0m12:49:02.026914 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__issue_merged_tmp
[0m12:49:02.026914 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__issue_comment_tmp
[0m12:49:02.031474 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_merged_tmp"
[0m12:49:02.034475 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_comment_tmp"
[0m12:49:02.036474 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:49:02.036474 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:49:02.038488 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:49:02.042995 [debug] [Thread-5  ]: On model.github_source.stg_github__issue_merged_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_merged_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_merged_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_merged`;


[0m12:49:02.044060 [debug] [Thread-4  ]: On model.github_source.stg_github__issue_label_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_label`;


[0m12:49:02.045056 [debug] [Thread-3  ]: On model.github_source.stg_github__issue_comment_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_comment_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_comment_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_comment`;


[0m12:49:02.473879 [debug] [Thread-2  ]: finished collecting timing info
[0m12:49:02.473879 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E501DCD0>]}
[0m12:49:02.474877 [info ] [Thread-2  ]: 2 of 40 OK created view model dbt_x_airflow.stg_customers ...................... [[32mOK[0m in 2.62s]
[0m12:49:02.475880 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m12:49:02.475880 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue_tmp
[0m12:49:02.475880 [info ] [Thread-2  ]: 10 of 40 START view model dbt_x_airflow.stg_github__issue_tmp .................. [RUN]
[0m12:49:02.476870 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__issue_tmp"
[0m12:49:02.476870 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__issue_tmp
[0m12:49:02.476870 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__issue_tmp
[0m12:49:02.480436 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__issue_tmp"
[0m12:49:02.484436 [debug] [Thread-2  ]: finished collecting timing info
[0m12:49:02.484436 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__issue_tmp
[0m12:49:02.488453 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__issue_tmp"
[0m12:49:02.489445 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:49:02.493444 [debug] [Thread-2  ]: On model.github_source.stg_github__issue_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue`;


[0m12:49:02.903954 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_merged_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_merged_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_merged`;


[0m12:49:02.903954 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: b4388b0d-2568-466d-a323-caf586ed7cf4

[0m12:49:02.903954 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:02.903954 [debug] [Thread-5  ]: Runtime Error in model stg_github__issue_merged_tmp (models\tmp\stg_github__issue_merged_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: b4388b0d-2568-466d-a323-caf586ed7cf4
  
[0m12:49:02.904951 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E4F94100>]}
[0m12:49:02.904951 [error] [Thread-5  ]: 9 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_merged_tmp ... [[31mERROR[0m in 0.90s]
[0m12:49:02.905943 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__issue_merged_tmp
[0m12:49:02.905943 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__label_tmp
[0m12:49:02.906943 [info ] [Thread-5  ]: 11 of 40 START view model dbt_x_airflow.stg_github__label_tmp .................. [RUN]
[0m12:49:02.907972 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__label_tmp"
[0m12:49:02.907972 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__label_tmp
[0m12:49:02.907972 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__label_tmp
[0m12:49:02.911992 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__label_tmp"
[0m12:49:02.912979 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:02.912979 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__label_tmp
[0m12:49:02.916980 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__label_tmp"
[0m12:49:02.918489 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:49:02.923546 [debug] [Thread-5  ]: On model.github_source.stg_github__label_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`label`;


[0m12:49:02.938873 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_label`;


[0m12:49:02.938873 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 8df124bc-2a19-4133-9a6c-ab5569b2f09b

[0m12:49:02.938873 [debug] [Thread-4  ]: finished collecting timing info
[0m12:49:02.939862 [debug] [Thread-4  ]: Runtime Error in model stg_github__issue_label_tmp (models\tmp\stg_github__issue_label_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 8df124bc-2a19-4133-9a6c-ab5569b2f09b
  
[0m12:49:02.939862 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E4F76A90>]}
[0m12:49:02.939862 [error] [Thread-4  ]: 8 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_label_tmp .... [[31mERROR[0m in 0.94s]
[0m12:49:02.941859 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_label_tmp
[0m12:49:02.941859 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__pull_request_review_tmp
[0m12:49:02.941859 [info ] [Thread-4  ]: 12 of 40 START view model dbt_x_airflow.stg_github__pull_request_review_tmp .... [RUN]
[0m12:49:02.942860 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__pull_request_review_tmp"
[0m12:49:02.942860 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__pull_request_review_tmp
[0m12:49:02.942860 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__pull_request_review_tmp
[0m12:49:02.946372 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__pull_request_review_tmp"
[0m12:49:02.947371 [debug] [Thread-4  ]: finished collecting timing info
[0m12:49:02.947371 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__pull_request_review_tmp
[0m12:49:02.950882 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__pull_request_review_tmp"
[0m12:49:02.951882 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:49:02.956950 [debug] [Thread-4  ]: On model.github_source.stg_github__pull_request_review_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_review_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_review_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request_review`;


[0m12:49:03.004581 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_comment_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_comment_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue_comment`;


[0m12:49:03.004581 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: bf7dbffb-7cdc-4207-8542-04c0c4a8446b

[0m12:49:03.005532 [debug] [Thread-3  ]: finished collecting timing info
[0m12:49:03.005532 [debug] [Thread-3  ]: Runtime Error in model stg_github__issue_comment_tmp (models\tmp\stg_github__issue_comment_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: bf7dbffb-7cdc-4207-8542-04c0c4a8446b
  
[0m12:49:03.005532 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E63567C0>]}
[0m12:49:03.006567 [error] [Thread-3  ]: 7 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_comment_tmp .. [[31mERROR[0m in 1.00s]
[0m12:49:03.007570 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__issue_comment_tmp
[0m12:49:03.007570 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__pull_request_tmp
[0m12:49:03.007570 [info ] [Thread-3  ]: 13 of 40 START view model dbt_x_airflow.stg_github__pull_request_tmp ........... [RUN]
[0m12:49:03.008532 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__pull_request_tmp"
[0m12:49:03.008532 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__pull_request_tmp
[0m12:49:03.010665 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__pull_request_tmp
[0m12:49:03.013895 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__pull_request_tmp"
[0m12:49:03.014860 [debug] [Thread-3  ]: finished collecting timing info
[0m12:49:03.014860 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__pull_request_tmp
[0m12:49:03.016862 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__pull_request_tmp"
[0m12:49:03.018401 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:49:03.022849 [debug] [Thread-3  ]: On model.github_source.stg_github__pull_request_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request`;


[0m12:49:03.208594 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__issue_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__issue_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`issue`;


[0m12:49:03.208594 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: bf1192cf-3ea0-4a11-b214-0c682721d0b8

[0m12:49:03.208594 [debug] [Thread-2  ]: finished collecting timing info
[0m12:49:03.209592 [debug] [Thread-2  ]: Runtime Error in model stg_github__issue_tmp (models\tmp\stg_github__issue_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: bf1192cf-3ea0-4a11-b214-0c682721d0b8
  
[0m12:49:03.209592 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E51B5700>]}
[0m12:49:03.209592 [error] [Thread-2  ]: 10 of 40 ERROR creating view model dbt_x_airflow.stg_github__issue_tmp ......... [[31mERROR[0m in 0.73s]
[0m12:49:03.210596 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue_tmp
[0m12:49:03.211586 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__repo_team_tmp
[0m12:49:03.211586 [info ] [Thread-2  ]: 14 of 40 START view model dbt_x_airflow.stg_github__repo_team_tmp .............. [RUN]
[0m12:49:03.212582 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.github_source.stg_github__repo_team_tmp"
[0m12:49:03.212582 [debug] [Thread-2  ]: Began compiling node model.github_source.stg_github__repo_team_tmp
[0m12:49:03.212582 [debug] [Thread-2  ]: Compiling model.github_source.stg_github__repo_team_tmp
[0m12:49:03.216585 [debug] [Thread-2  ]: Writing injected SQL for node "model.github_source.stg_github__repo_team_tmp"
[0m12:49:03.219843 [debug] [Thread-2  ]: finished collecting timing info
[0m12:49:03.220843 [debug] [Thread-2  ]: Began executing node model.github_source.stg_github__repo_team_tmp
[0m12:49:03.224895 [debug] [Thread-2  ]: Writing runtime SQL for node "model.github_source.stg_github__repo_team_tmp"
[0m12:49:03.226863 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:49:03.231439 [debug] [Thread-2  ]: On model.github_source.stg_github__repo_team_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repo_team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repo_team_tmp`
  OPTIONS()
  as 

select * 
from `airflow-docker-352518`.`github`.`repo_team`;


[0m12:49:03.384756 [debug] [Thread-1  ]: finished collecting timing info
[0m12:49:03.385724 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E3C9EBE0>]}
[0m12:49:03.386732 [info ] [Thread-1  ]: 1 of 40 OK created table model dbt_x_airflow.agg_transactions .................. [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.53s]
[0m12:49:03.389178 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m12:49:03.389178 [debug] [Thread-1  ]: Began running node model.github_source.stg_github__repository_tmp
[0m12:49:03.389178 [info ] [Thread-1  ]: 15 of 40 START view model dbt_x_airflow.stg_github__repository_tmp ............. [RUN]
[0m12:49:03.390179 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.github_source.stg_github__repository_tmp"
[0m12:49:03.390179 [debug] [Thread-1  ]: Began compiling node model.github_source.stg_github__repository_tmp
[0m12:49:03.390179 [debug] [Thread-1  ]: Compiling model.github_source.stg_github__repository_tmp
[0m12:49:03.395178 [debug] [Thread-1  ]: Writing injected SQL for node "model.github_source.stg_github__repository_tmp"
[0m12:49:03.396177 [debug] [Thread-1  ]: finished collecting timing info
[0m12:49:03.396177 [debug] [Thread-1  ]: Began executing node model.github_source.stg_github__repository_tmp
[0m12:49:03.398750 [debug] [Thread-1  ]: Writing runtime SQL for node "model.github_source.stg_github__repository_tmp"
[0m12:49:03.399796 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m12:49:03.403799 [debug] [Thread-1  ]: On model.github_source.stg_github__repository_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repository_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repository_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`repository`;


[0m12:49:03.607627 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__label_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__label_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`label`;


[0m12:49:03.607627 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 21c59455-a0be-4674-9e79-619b8a2965b3

[0m12:49:03.608633 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:03.608633 [debug] [Thread-5  ]: Runtime Error in model stg_github__label_tmp (models\tmp\stg_github__label_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 21c59455-a0be-4674-9e79-619b8a2965b3
  
[0m12:49:03.609154 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E634C970>]}
[0m12:49:03.609154 [error] [Thread-5  ]: 11 of 40 ERROR creating view model dbt_x_airflow.stg_github__label_tmp ......... [[31mERROR[0m in 0.70s]
[0m12:49:03.610195 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__label_tmp
[0m12:49:03.610195 [debug] [Thread-5  ]: Began running node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:49:03.610195 [info ] [Thread-5  ]: 16 of 40 START view model dbt_x_airflow.stg_github__requested_reviewer_history_tmp  [RUN]
[0m12:49:03.611164 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:49:03.612159 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_review_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_review_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request_review`;


[0m12:49:03.613665 [debug] [Thread-5  ]: Began compiling node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:49:03.613665 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 18fddf6c-f98f-4216-b38d-ff7f1707620b

[0m12:49:03.613665 [debug] [Thread-5  ]: Compiling model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:49:03.614714 [debug] [Thread-4  ]: finished collecting timing info
[0m12:49:03.618678 [debug] [Thread-5  ]: Writing injected SQL for node "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:49:03.618678 [debug] [Thread-4  ]: Runtime Error in model stg_github__pull_request_review_tmp (models\tmp\stg_github__pull_request_review_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 18fddf6c-f98f-4216-b38d-ff7f1707620b
  
[0m12:49:03.619885 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E643D040>]}
[0m12:49:03.620900 [error] [Thread-4  ]: 12 of 40 ERROR creating view model dbt_x_airflow.stg_github__pull_request_review_tmp  [[31mERROR[0m in 0.68s]
[0m12:49:03.622902 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:03.622902 [debug] [Thread-5  ]: Began executing node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:49:03.626895 [debug] [Thread-5  ]: Writing runtime SQL for node "model.github_source.stg_github__requested_reviewer_history_tmp"
[0m12:49:03.626895 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__pull_request_review_tmp
[0m12:49:03.626895 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__team_tmp
[0m12:49:03.626895 [info ] [Thread-4  ]: 17 of 40 START view model dbt_x_airflow.stg_github__team_tmp ................... [RUN]
[0m12:49:03.628407 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m12:49:03.629415 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.github_source.stg_github__team_tmp"
[0m12:49:03.629415 [debug] [Thread-4  ]: Began compiling node model.github_source.stg_github__team_tmp
[0m12:49:03.629415 [debug] [Thread-4  ]: Compiling model.github_source.stg_github__team_tmp
[0m12:49:03.633413 [debug] [Thread-4  ]: Writing injected SQL for node "model.github_source.stg_github__team_tmp"
[0m12:49:03.634415 [debug] [Thread-4  ]: finished collecting timing info
[0m12:49:03.634415 [debug] [Thread-4  ]: Began executing node model.github_source.stg_github__team_tmp
[0m12:49:03.638415 [debug] [Thread-4  ]: Writing runtime SQL for node "model.github_source.stg_github__team_tmp"
[0m12:49:03.638415 [debug] [Thread-5  ]: On model.github_source.stg_github__requested_reviewer_history_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__requested_reviewer_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__requested_reviewer_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`requested_reviewer_history`;


[0m12:49:03.639725 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:49:03.644270 [debug] [Thread-4  ]: On model.github_source.stg_github__team_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__team_tmp`
  OPTIONS()
  as select * 
from `airflow-docker-352518`.`github`.`team`;


[0m12:49:03.779892 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__pull_request_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__pull_request_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`pull_request`;


[0m12:49:03.779892 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: ec66eba8-edee-473a-be4a-828c475b19ff

[0m12:49:03.779892 [debug] [Thread-3  ]: finished collecting timing info
[0m12:49:03.779892 [debug] [Thread-3  ]: Runtime Error in model stg_github__pull_request_tmp (models\tmp\stg_github__pull_request_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: ec66eba8-edee-473a-be4a-828c475b19ff
  
[0m12:49:03.780893 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E63A7D60>]}
[0m12:49:03.780893 [error] [Thread-3  ]: 13 of 40 ERROR creating view model dbt_x_airflow.stg_github__pull_request_tmp .. [[31mERROR[0m in 0.77s]
[0m12:49:03.781888 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__pull_request_tmp
[0m12:49:03.781888 [debug] [Thread-3  ]: Began running node model.github_source.stg_github__user_tmp
[0m12:49:03.782921 [info ] [Thread-3  ]: 18 of 40 START view model dbt_x_airflow.stg_github__user_tmp ................... [RUN]
[0m12:49:03.783887 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.github_source.stg_github__user_tmp"
[0m12:49:03.783887 [debug] [Thread-3  ]: Began compiling node model.github_source.stg_github__user_tmp
[0m12:49:03.783887 [debug] [Thread-3  ]: Compiling model.github_source.stg_github__user_tmp
[0m12:49:03.786892 [debug] [Thread-3  ]: Writing injected SQL for node "model.github_source.stg_github__user_tmp"
[0m12:49:03.788469 [debug] [Thread-3  ]: finished collecting timing info
[0m12:49:03.788469 [debug] [Thread-3  ]: Began executing node model.github_source.stg_github__user_tmp
[0m12:49:03.792473 [debug] [Thread-3  ]: Writing runtime SQL for node "model.github_source.stg_github__user_tmp"
[0m12:49:03.793480 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m12:49:03.798482 [debug] [Thread-3  ]: On model.github_source.stg_github__user_tmp: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__user_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__user_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`user`;


[0m12:49:04.048403 [debug] [Thread-2  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repo_team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repo_team_tmp`
  OPTIONS()
  as 

select * 
from `airflow-docker-352518`.`github`.`repo_team`;


[0m12:49:04.048403 [debug] [Thread-2  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 552c3f9f-8e23-4ed6-9cc4-1fc8620888af

[0m12:49:04.049431 [debug] [Thread-2  ]: finished collecting timing info
[0m12:49:04.049431 [debug] [Thread-2  ]: Runtime Error in model stg_github__repo_team_tmp (models\tmp\stg_github__repo_team_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 552c3f9f-8e23-4ed6-9cc4-1fc8620888af
  
[0m12:49:04.049431 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E6402BB0>]}
[0m12:49:04.049431 [error] [Thread-2  ]: 14 of 40 ERROR creating view model dbt_x_airflow.stg_github__repo_team_tmp ..... [[31mERROR[0m in 0.84s]
[0m12:49:04.051400 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__repo_team_tmp
[0m12:49:04.051400 [debug] [Thread-2  ]: Began running node model.github_source.stg_github__issue_assignee
[0m12:49:04.051400 [info ] [Thread-2  ]: 19 of 40 SKIP relation dbt_x_airflow.stg_github__issue_assignee ................ [[33mSKIP[0m]
[0m12:49:04.053399 [debug] [Thread-2  ]: Finished running node model.github_source.stg_github__issue_assignee
[0m12:49:04.053399 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m12:49:04.053399 [info ] [Thread-2  ]: 20 of 40 START table model dbt_x_airflow.pivoted_orders ........................ [RUN]
[0m12:49:04.054400 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m12:49:04.054400 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m12:49:04.054400 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m12:49:04.058400 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:49:04.058907 [debug] [Thread-2  ]: finished collecting timing info
[0m12:49:04.058907 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m12:49:04.061921 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:49:04.061921 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m12:49:04.066461 [debug] [Thread-2  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m12:49:04.680627 [debug] [Thread-1  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__repository_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__repository_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`repository`;


[0m12:49:04.681624 [debug] [Thread-1  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: e412e7c6-e29d-4ae5-94a9-da335713f4ba

[0m12:49:04.681624 [debug] [Thread-1  ]: finished collecting timing info
[0m12:49:04.681624 [debug] [Thread-1  ]: Runtime Error in model stg_github__repository_tmp (models\tmp\stg_github__repository_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: e412e7c6-e29d-4ae5-94a9-da335713f4ba
  
[0m12:49:04.682627 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E6363040>]}
[0m12:49:04.683629 [error] [Thread-1  ]: 15 of 40 ERROR creating view model dbt_x_airflow.stg_github__repository_tmp .... [[31mERROR[0m in 1.29s]
[0m12:49:04.684623 [debug] [Thread-1  ]: Finished running node model.github_source.stg_github__repository_tmp
[0m12:49:04.684623 [debug] [Thread-1  ]: Began running node model.github_source.stg_github__issue_closed_history
[0m12:49:04.685630 [info ] [Thread-1  ]: 21 of 40 SKIP relation dbt_x_airflow.stg_github__issue_closed_history .......... [[33mSKIP[0m]
[0m12:49:04.686630 [debug] [Thread-1  ]: Finished running node model.github_source.stg_github__issue_closed_history
[0m12:49:04.686630 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.dim_customers
[0m12:49:04.687629 [info ] [Thread-1  ]: 22 of 40 START table model dbt_x_airflow.dim_customers ......................... [RUN]
[0m12:49:04.688625 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m12:49:04.688625 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m12:49:04.688625 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.dim_customers
[0m12:49:04.693873 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:49:04.694884 [debug] [Thread-1  ]: finished collecting timing info
[0m12:49:04.694884 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m12:49:04.696879 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m12:49:05.106926 [debug] [Thread-4  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__team_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__team_tmp`
  OPTIONS()
  as select * 
from `airflow-docker-352518`.`github`.`team`;


[0m12:49:05.108432 [debug] [Thread-4  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 559d8685-f747-44ee-b70d-cadc16ff9b2f

[0m12:49:05.108432 [debug] [Thread-4  ]: finished collecting timing info
[0m12:49:05.108432 [debug] [Thread-4  ]: Runtime Error in model stg_github__team_tmp (models\tmp\stg_github__team_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 559d8685-f747-44ee-b70d-cadc16ff9b2f
  
[0m12:49:05.108432 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E74290A0>]}
[0m12:49:05.109487 [error] [Thread-4  ]: 17 of 40 ERROR creating view model dbt_x_airflow.stg_github__team_tmp .......... [[31mERROR[0m in 1.48s]
[0m12:49:05.110486 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__team_tmp
[0m12:49:05.110486 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_merged
[0m12:49:05.110486 [info ] [Thread-4  ]: 23 of 40 SKIP relation dbt_x_airflow.stg_github__issue_merged .................. [[33mSKIP[0m]
[0m12:49:05.112493 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_merged
[0m12:49:05.112493 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_label
[0m12:49:05.112493 [info ] [Thread-4  ]: 24 of 40 SKIP relation dbt_x_airflow.stg_github__issue_label ................... [[33mSKIP[0m]
[0m12:49:05.113500 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_label
[0m12:49:05.113500 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue_comment
[0m12:49:05.113500 [info ] [Thread-4  ]: 25 of 40 SKIP relation dbt_x_airflow.stg_github__issue_comment ................. [[33mSKIP[0m]
[0m12:49:05.114499 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue_comment
[0m12:49:05.114499 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__issue
[0m12:49:05.114499 [info ] [Thread-4  ]: 26 of 40 SKIP relation dbt_x_airflow.stg_github__issue ......................... [[33mSKIP[0m]
[0m12:49:05.115499 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__issue
[0m12:49:05.116499 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__label
[0m12:49:05.116499 [info ] [Thread-4  ]: 27 of 40 SKIP relation dbt_x_airflow.stg_github__label ......................... [[33mSKIP[0m]
[0m12:49:05.117499 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__label
[0m12:49:05.117499 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__pull_request_review
[0m12:49:05.117499 [info ] [Thread-4  ]: 28 of 40 SKIP relation dbt_x_airflow.stg_github__pull_request_review ........... [[33mSKIP[0m]
[0m12:49:05.118498 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__pull_request_review
[0m12:49:05.118498 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__pull_request
[0m12:49:05.118498 [info ] [Thread-4  ]: 29 of 40 SKIP relation dbt_x_airflow.stg_github__pull_request .................. [[33mSKIP[0m]
[0m12:49:05.118498 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__pull_request
[0m12:49:05.119957 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__repo_team
[0m12:49:05.119957 [info ] [Thread-4  ]: 30 of 40 SKIP relation dbt_x_airflow.stg_github__repo_team ..................... [[33mSKIP[0m]
[0m12:49:05.119957 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__repo_team
[0m12:49:05.119957 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__repository
[0m12:49:05.120963 [info ] [Thread-4  ]: 31 of 40 SKIP relation dbt_x_airflow.stg_github__repository .................... [[33mSKIP[0m]
[0m12:49:05.120963 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__repository
[0m12:49:05.120963 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__team
[0m12:49:05.121964 [info ] [Thread-4  ]: 32 of 40 SKIP relation dbt_x_airflow.stg_github__team .......................... [[33mSKIP[0m]
[0m12:49:05.121964 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__team
[0m12:49:05.122968 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_comments
[0m12:49:05.122968 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_comments
[0m12:49:05.123963 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_open_length
[0m12:49:05.123963 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_open_length
[0m12:49:05.123963 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_label_joined
[0m12:49:05.123963 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_label_joined
[0m12:49:05.123963 [debug] [Thread-4  ]: Began running node model.github.int_github__repository_teams
[0m12:49:05.124964 [debug] [Thread-4  ]: Finished running node model.github.int_github__repository_teams
[0m12:49:05.124964 [debug] [Thread-4  ]: Began running node model.github.int_github__issue_labels
[0m12:49:05.124964 [debug] [Thread-4  ]: Finished running node model.github.int_github__issue_labels
[0m12:49:05.154918 [debug] [Thread-5  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__requested_reviewer_history_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__requested_reviewer_history_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`requested_reviewer_history`;


[0m12:49:05.155920 [debug] [Thread-5  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: 97bd01af-f405-4f44-b805-7df7d5c74da8

[0m12:49:05.155920 [debug] [Thread-5  ]: finished collecting timing info
[0m12:49:05.155920 [debug] [Thread-5  ]: Runtime Error in model stg_github__requested_reviewer_history_tmp (models\tmp\stg_github__requested_reviewer_history_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: 97bd01af-f405-4f44-b805-7df7d5c74da8
  
[0m12:49:05.156926 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E6436B50>]}
[0m12:49:05.156926 [error] [Thread-5  ]: 16 of 40 ERROR creating view model dbt_x_airflow.stg_github__requested_reviewer_history_tmp  [[31mERROR[0m in 1.54s]
[0m12:49:05.158440 [debug] [Thread-5  ]: Finished running node model.github_source.stg_github__requested_reviewer_history_tmp
[0m12:49:05.158440 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__requested_reviewer_history
[0m12:49:05.159448 [info ] [Thread-4  ]: 33 of 40 SKIP relation dbt_x_airflow.stg_github__requested_reviewer_history .... [[33mSKIP[0m]
[0m12:49:05.160447 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__requested_reviewer_history
[0m12:49:05.160447 [debug] [Thread-5  ]: Began running node model.github.int_github__pull_request_times
[0m12:49:05.160447 [debug] [Thread-5  ]: Finished running node model.github.int_github__pull_request_times
[0m12:49:05.178679 [debug] [Thread-3  ]: BigQuery adapter: Unhandled error while running:
/* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.github_source.stg_github__user_tmp"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_github__user_tmp`
  OPTIONS()
  as select *
from `airflow-docker-352518`.`github`.`user`;


[0m12:49:05.179861 [debug] [Thread-3  ]: BigQuery adapter: 404 Not found: Dataset airflow-docker-352518:github was not found in location US

Location: US
Job ID: a1e37d1f-bb76-4f5f-ad84-1ab54100fb14

[0m12:49:05.179861 [debug] [Thread-3  ]: finished collecting timing info
[0m12:49:05.179861 [debug] [Thread-3  ]: Runtime Error in model stg_github__user_tmp (models\tmp\stg_github__user_tmp.sql)
  404 Not found: Dataset airflow-docker-352518:github was not found in location US
  
  Location: US
  Job ID: a1e37d1f-bb76-4f5f-ad84-1ab54100fb14
  
[0m12:49:05.179861 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E7488640>]}
[0m12:49:05.180874 [error] [Thread-3  ]: 18 of 40 ERROR creating view model dbt_x_airflow.stg_github__user_tmp .......... [[31mERROR[0m in 1.40s]
[0m12:49:05.180874 [debug] [Thread-3  ]: Finished running node model.github_source.stg_github__user_tmp
[0m12:49:05.181868 [debug] [Thread-4  ]: Began running node model.github_source.stg_github__user
[0m12:49:05.181868 [info ] [Thread-4  ]: 34 of 40 SKIP relation dbt_x_airflow.stg_github__user .......................... [[33mSKIP[0m]
[0m12:49:05.182868 [debug] [Thread-4  ]: Finished running node model.github_source.stg_github__user
[0m12:49:05.183865 [debug] [Thread-3  ]: Began running node model.github.int_github__issue_assignees
[0m12:49:05.183865 [debug] [Thread-3  ]: Finished running node model.github.int_github__issue_assignees
[0m12:49:05.183865 [debug] [Thread-5  ]: Began running node model.github.int_github__pull_request_reviewers
[0m12:49:05.183865 [debug] [Thread-5  ]: Finished running node model.github.int_github__pull_request_reviewers
[0m12:49:05.184876 [debug] [Thread-3  ]: Began running node model.github.int_github__issue_joined
[0m12:49:05.184876 [debug] [Thread-3  ]: Finished running node model.github.int_github__issue_joined
[0m12:49:05.184876 [debug] [Thread-5  ]: Began running node model.github.github__issues
[0m12:49:05.184876 [debug] [Thread-4  ]: Began running node model.github.github__pull_requests
[0m12:49:05.185866 [info ] [Thread-5  ]: 35 of 40 SKIP relation dbt_x_airflow.github__issues ............................ [[33mSKIP[0m]
[0m12:49:05.185866 [info ] [Thread-4  ]: 36 of 40 SKIP relation dbt_x_airflow.github__pull_requests ..................... [[33mSKIP[0m]
[0m12:49:05.186869 [debug] [Thread-5  ]: Finished running node model.github.github__issues
[0m12:49:05.186869 [debug] [Thread-4  ]: Finished running node model.github.github__pull_requests
[0m12:49:05.188298 [debug] [Thread-3  ]: Began running node model.github.github__daily_metrics
[0m12:49:05.188298 [info ] [Thread-3  ]: 37 of 40 SKIP relation dbt_x_airflow.github__daily_metrics ..................... [[33mSKIP[0m]
[0m12:49:05.189301 [debug] [Thread-3  ]: Finished running node model.github.github__daily_metrics
[0m12:49:05.189301 [debug] [Thread-4  ]: Began running node model.github.github__monthly_metrics
[0m12:49:05.189301 [info ] [Thread-4  ]: 38 of 40 SKIP relation dbt_x_airflow.github__monthly_metrics ................... [[33mSKIP[0m]
[0m12:49:05.190297 [debug] [Thread-5  ]: Began running node model.github.github__quarterly_metrics
[0m12:49:05.190297 [debug] [Thread-3  ]: Began running node model.github.github__weekly_metrics
[0m12:49:05.190297 [info ] [Thread-5  ]: 39 of 40 SKIP relation dbt_x_airflow.github__quarterly_metrics ................. [[33mSKIP[0m]
[0m12:49:05.191297 [info ] [Thread-3  ]: 40 of 40 SKIP relation dbt_x_airflow.github__weekly_metrics .................... [[33mSKIP[0m]
[0m12:49:05.192302 [debug] [Thread-4  ]: Finished running node model.github.github__monthly_metrics
[0m12:49:05.193299 [debug] [Thread-5  ]: Finished running node model.github.github__quarterly_metrics
[0m12:49:05.194300 [debug] [Thread-3  ]: Finished running node model.github.github__weekly_metrics
[0m12:49:05.366759 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:49:05.369786 [debug] [Thread-1  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m12:49:05.990083 [debug] [Thread-2  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]')
[0m12:49:07.786028 [debug] [Thread-2  ]: finished collecting timing info
[0m12:49:07.786028 [debug] [Thread-2  ]: Database Error in model pivoted_orders (models\prod\pivoted_orders.sql)
  No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
  compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:49:07.786028 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E7532400>]}
[0m12:49:07.787022 [error] [Thread-2  ]: 20 of 40 ERROR creating table model dbt_x_airflow.pivoted_orders ............... [[31mERROR[0m in 3.73s]
[0m12:49:07.787022 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m12:49:08.361778 [debug] [Thread-1  ]: finished collecting timing info
[0m12:49:08.362777 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c2f5054-4a6e-4050-88d8-c02142e37772', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E4FD31F0>]}
[0m12:49:08.362777 [info ] [Thread-1  ]: 22 of 40 OK created table model dbt_x_airflow.dim_customers .................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.68s]
[0m12:49:08.363746 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m12:49:08.365744 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:49:08.365744 [info ] [MainThread]: 
[0m12:49:08.366760 [info ] [MainThread]: Finished running 17 view models, 23 table models in 0 hours 0 minutes and 10.19 seconds (10.19s).
[0m12:49:08.366760 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:49:08.367750 [debug] [MainThread]: Connection 'list_airflow-docker-352518' was properly closed.
[0m12:49:08.367750 [debug] [MainThread]: Connection 'list_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m12:49:08.367750 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m12:49:08.367750 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m12:49:08.367750 [debug] [MainThread]: Connection 'model.github_source.stg_github__user_tmp' was properly closed.
[0m12:49:08.367750 [debug] [MainThread]: Connection 'model.github_source.stg_github__team_tmp' was properly closed.
[0m12:49:08.367750 [debug] [MainThread]: Connection 'model.github_source.stg_github__requested_reviewer_history_tmp' was properly closed.
[0m12:49:08.384296 [info ] [MainThread]: 
[0m12:49:08.384296 [info ] [MainThread]: [31mCompleted with 15 errors and 0 warnings:[0m
[0m12:49:08.385297 [info ] [MainThread]: 
[0m12:49:08.386300 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_assignee_tmp (models\tmp\stg_github__issue_assignee_tmp.sql)[0m
[0m12:49:08.386300 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.387299 [error] [MainThread]:   
[0m12:49:08.387299 [error] [MainThread]:   Location: US
[0m12:49:08.387299 [error] [MainThread]:   Job ID: a3681aac-cb0b-410b-9e70-0fd965acc7c2
[0m12:49:08.388809 [error] [MainThread]:   
[0m12:49:08.388809 [info ] [MainThread]: 
[0m12:49:08.388809 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_closed_history_tmp (models\tmp\stg_github__issue_closed_history_tmp.sql)[0m
[0m12:49:08.390293 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.390293 [error] [MainThread]:   
[0m12:49:08.391292 [error] [MainThread]:   Location: US
[0m12:49:08.391292 [error] [MainThread]:   Job ID: 44ced79e-9529-44a4-ac30-c337fbae097f
[0m12:49:08.392297 [error] [MainThread]:   
[0m12:49:08.392297 [info ] [MainThread]: 
[0m12:49:08.393291 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_merged_tmp (models\tmp\stg_github__issue_merged_tmp.sql)[0m
[0m12:49:08.393291 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.394293 [error] [MainThread]:   
[0m12:49:08.394293 [error] [MainThread]:   Location: US
[0m12:49:08.395294 [error] [MainThread]:   Job ID: b4388b0d-2568-466d-a323-caf586ed7cf4
[0m12:49:08.395294 [error] [MainThread]:   
[0m12:49:08.395294 [info ] [MainThread]: 
[0m12:49:08.396291 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_label_tmp (models\tmp\stg_github__issue_label_tmp.sql)[0m
[0m12:49:08.396291 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.397291 [error] [MainThread]:   
[0m12:49:08.397291 [error] [MainThread]:   Location: US
[0m12:49:08.397291 [error] [MainThread]:   Job ID: 8df124bc-2a19-4133-9a6c-ab5569b2f09b
[0m12:49:08.398296 [error] [MainThread]:   
[0m12:49:08.398296 [info ] [MainThread]: 
[0m12:49:08.398296 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_comment_tmp (models\tmp\stg_github__issue_comment_tmp.sql)[0m
[0m12:49:08.399297 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.399297 [error] [MainThread]:   
[0m12:49:08.399297 [error] [MainThread]:   Location: US
[0m12:49:08.400296 [error] [MainThread]:   Job ID: bf7dbffb-7cdc-4207-8542-04c0c4a8446b
[0m12:49:08.400296 [error] [MainThread]:   
[0m12:49:08.400296 [info ] [MainThread]: 
[0m12:49:08.401295 [error] [MainThread]: [33mRuntime Error in model stg_github__issue_tmp (models\tmp\stg_github__issue_tmp.sql)[0m
[0m12:49:08.401295 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.401295 [error] [MainThread]:   
[0m12:49:08.402295 [error] [MainThread]:   Location: US
[0m12:49:08.402295 [error] [MainThread]:   Job ID: bf1192cf-3ea0-4a11-b214-0c682721d0b8
[0m12:49:08.402295 [error] [MainThread]:   
[0m12:49:08.403296 [info ] [MainThread]: 
[0m12:49:08.403296 [error] [MainThread]: [33mRuntime Error in model stg_github__label_tmp (models\tmp\stg_github__label_tmp.sql)[0m
[0m12:49:08.404297 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.404297 [error] [MainThread]:   
[0m12:49:08.404297 [error] [MainThread]:   Location: US
[0m12:49:08.405297 [error] [MainThread]:   Job ID: 21c59455-a0be-4674-9e79-619b8a2965b3
[0m12:49:08.405297 [error] [MainThread]:   
[0m12:49:08.406297 [info ] [MainThread]: 
[0m12:49:08.406297 [error] [MainThread]: [33mRuntime Error in model stg_github__pull_request_review_tmp (models\tmp\stg_github__pull_request_review_tmp.sql)[0m
[0m12:49:08.407297 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.407297 [error] [MainThread]:   
[0m12:49:08.407297 [error] [MainThread]:   Location: US
[0m12:49:08.407297 [error] [MainThread]:   Job ID: 18fddf6c-f98f-4216-b38d-ff7f1707620b
[0m12:49:08.407297 [error] [MainThread]:   
[0m12:49:08.408801 [info ] [MainThread]: 
[0m12:49:08.408801 [error] [MainThread]: [33mRuntime Error in model stg_github__pull_request_tmp (models\tmp\stg_github__pull_request_tmp.sql)[0m
[0m12:49:08.409839 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.410837 [error] [MainThread]:   
[0m12:49:08.410837 [error] [MainThread]:   Location: US
[0m12:49:08.410837 [error] [MainThread]:   Job ID: ec66eba8-edee-473a-be4a-828c475b19ff
[0m12:49:08.411835 [error] [MainThread]:   
[0m12:49:08.411835 [info ] [MainThread]: 
[0m12:49:08.411835 [error] [MainThread]: [33mRuntime Error in model stg_github__repo_team_tmp (models\tmp\stg_github__repo_team_tmp.sql)[0m
[0m12:49:08.412844 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.412844 [error] [MainThread]:   
[0m12:49:08.413851 [error] [MainThread]:   Location: US
[0m12:49:08.413851 [error] [MainThread]:   Job ID: 552c3f9f-8e23-4ed6-9cc4-1fc8620888af
[0m12:49:08.413851 [error] [MainThread]:   
[0m12:49:08.413851 [info ] [MainThread]: 
[0m12:49:08.414849 [error] [MainThread]: [33mRuntime Error in model stg_github__repository_tmp (models\tmp\stg_github__repository_tmp.sql)[0m
[0m12:49:08.414849 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.415851 [error] [MainThread]:   
[0m12:49:08.415851 [error] [MainThread]:   Location: US
[0m12:49:08.415851 [error] [MainThread]:   Job ID: e412e7c6-e29d-4ae5-94a9-da335713f4ba
[0m12:49:08.416851 [error] [MainThread]:   
[0m12:49:08.416851 [info ] [MainThread]: 
[0m12:49:08.416851 [error] [MainThread]: [33mRuntime Error in model stg_github__team_tmp (models\tmp\stg_github__team_tmp.sql)[0m
[0m12:49:08.416851 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.416851 [error] [MainThread]:   
[0m12:49:08.418356 [error] [MainThread]:   Location: US
[0m12:49:08.418356 [error] [MainThread]:   Job ID: 559d8685-f747-44ee-b70d-cadc16ff9b2f
[0m12:49:08.418356 [error] [MainThread]:   
[0m12:49:08.418356 [info ] [MainThread]: 
[0m12:49:08.419693 [error] [MainThread]: [33mRuntime Error in model stg_github__requested_reviewer_history_tmp (models\tmp\stg_github__requested_reviewer_history_tmp.sql)[0m
[0m12:49:08.420703 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.420703 [error] [MainThread]:   
[0m12:49:08.420703 [error] [MainThread]:   Location: US
[0m12:49:08.421700 [error] [MainThread]:   Job ID: 97bd01af-f405-4f44-b805-7df7d5c74da8
[0m12:49:08.421700 [error] [MainThread]:   
[0m12:49:08.421700 [info ] [MainThread]: 
[0m12:49:08.422702 [error] [MainThread]: [33mRuntime Error in model stg_github__user_tmp (models\tmp\stg_github__user_tmp.sql)[0m
[0m12:49:08.423704 [error] [MainThread]:   404 Not found: Dataset airflow-docker-352518:github was not found in location US
[0m12:49:08.423704 [error] [MainThread]:   
[0m12:49:08.423704 [error] [MainThread]:   Location: US
[0m12:49:08.424701 [error] [MainThread]:   Job ID: a1e37d1f-bb76-4f5f-ad84-1ab54100fb14
[0m12:49:08.424701 [error] [MainThread]:   
[0m12:49:08.424701 [info ] [MainThread]: 
[0m12:49:08.425700 [error] [MainThread]: [33mDatabase Error in model pivoted_orders (models\prod\pivoted_orders.sql)[0m
[0m12:49:08.425700 [error] [MainThread]:   No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
[0m12:49:08.426700 [error] [MainThread]:   compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:49:08.426700 [info ] [MainThread]: 
[0m12:49:08.427824 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=15 SKIP=20 TOTAL=40
[0m12:49:08.427824 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E74A9790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E74A97F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001F6E74A9FD0>]}


============================== 2022-08-07 12:49:14.790975 | ad8334a7-c458-413d-aceb-04916816a7f0 ==============================
[0m12:49:14.790975 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:49:14.791971 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m12:49:14.791971 [debug] [MainThread]: Tracking: tracking
[0m12:49:14.812983 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA8102A250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA8102A610>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA8102A310>]}
[0m12:49:14.813990 [debug] [MainThread]: Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-bj6kc2wx'
[0m12:49:14.813990 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m12:49:15.401870 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m12:49:15.403374 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m12:49:15.498645 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m12:49:15.536259 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m12:49:16.258920 [info ] [MainThread]:   Installed from version 0.8.6
[0m12:49:16.259919 [info ] [MainThread]:   Up to date!
[0m12:49:16.259919 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': 'ad8334a7-c458-413d-aceb-04916816a7f0', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA8102AFD0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FA8102A460>]}
[0m12:49:16.260922 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FAFFFD5B80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FAFFFF72E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001FAFFFF75E0>]}


============================== 2022-08-07 12:49:45.478862 | 67f833cb-cfe5-405b-b10d-17e48737e3ed ==============================
[0m12:49:45.478862 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:49:45.478862 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'clean', 'indirect_selection': 'eager'}
[0m12:49:45.479864 [debug] [MainThread]: Tracking: tracking
[0m12:49:45.503316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C14B5B9310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C14B5B96D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C14B5B93D0>]}
[0m12:49:45.503316 [warn ] [MainThread]: [[33mWARNING[0m]: Deprecated functionality
The default package install path has changed from `dbt_modules` to
`dbt_packages`. Please update `clean-targets` in `dbt_project.yml` and check
`.gitignore` as well. Or, set `packages-install-path: dbt_modules` if you'd like
to keep the current value.
[0m12:49:45.504316 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'deprecation', 'label': '67f833cb-cfe5-405b-b10d-17e48737e3ed', 'property_': 'warn', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C14B5B9310>]}
[0m12:49:45.519844 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C14B5B9700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C14B5B9580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001C14B5B97F0>]}


============================== 2022-08-07 12:49:50.566555 | 8c2fc1da-013a-4563-b79f-5b7776dc62f3 ==============================
[0m12:49:50.566555 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:49:50.567555 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m12:49:50.567555 [debug] [MainThread]: Tracking: tracking
[0m12:49:50.590358 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB8DA220>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB8DA5E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB8DA2E0>]}
[0m12:49:50.592351 [debug] [MainThread]: Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-bkpeubzq'
[0m12:49:50.592351 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m12:49:50.711685 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m12:49:50.712696 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m12:49:50.797121 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m12:49:50.831988 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m12:49:51.451250 [info ] [MainThread]:   Installed from version 0.8.6
[0m12:49:51.452250 [info ] [MainThread]:   Up to date!
[0m12:49:51.452250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '8c2fc1da-013a-4563-b79f-5b7776dc62f3', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB8DAFA0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB8DA430>]}
[0m12:49:51.453250 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB8559D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB8772B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x00000210DB877BE0>]}


============================== 2022-08-07 12:50:47.017238 | 9f0ee401-6d64-4920-a9e4-9ef1ae5e98f7 ==============================
[0m12:50:47.017238 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:50:47.017238 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'deps', 'rpc_method': 'deps', 'indirect_selection': 'eager'}
[0m12:50:47.017238 [debug] [MainThread]: Tracking: tracking
[0m12:50:47.040451 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A1925A310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A1925A6D0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A1925A3D0>]}
[0m12:50:47.042451 [debug] [MainThread]: Set downloads directory='C:\Users\Vanmai40\AppData\Local\Temp\dbt-downloads-hojk62u3'
[0m12:50:47.043452 [debug] [MainThread]: Making package index registry request: GET https://hub.getdbt.com/api/v1/index.json
[0m12:50:47.182877 [debug] [MainThread]: Response from registry index: GET https://hub.getdbt.com/api/v1/index.json 200
[0m12:50:47.182877 [debug] [MainThread]: Making package registry request: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json
[0m12:50:47.283546 [debug] [MainThread]: Response from registry: GET https://hub.getdbt.com/api/v1/dbt-labs/dbt_utils.json 200
[0m12:50:47.321722 [info ] [MainThread]: Installing dbt-labs/dbt_utils
[0m12:50:47.855757 [info ] [MainThread]:   Installed from version 0.8.6
[0m12:50:47.856756 [info ] [MainThread]:   Up to date!
[0m12:50:47.856756 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'package', 'label': '9f0ee401-6d64-4920-a9e4-9ef1ae5e98f7', 'property_': 'install', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A192791C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A192792E0>]}
[0m12:50:47.857759 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A191C65B0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A191F6460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x0000018A191F62E0>]}


============================== 2022-08-07 12:51:04.159005 | f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4 ==============================
[0m12:51:04.159005 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:51:04.160013 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m12:51:04.160013 [debug] [MainThread]: Tracking: tracking
[0m12:51:04.170450 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067AE17250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067AE178E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067AE17A90>]}
[0m12:51:04.193686 [info ] [MainThread]: Partial parse save file not found. Starting full parse.
[0m12:51:04.194685 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067ADD6F10>]}
[0m12:51:04.320495 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m12:51:04.321543 [debug] [MainThread]: Parsing macros\adapters.sql
[0m12:51:04.341204 [debug] [MainThread]: Parsing macros\catalog.sql
[0m12:51:04.345204 [debug] [MainThread]: Parsing macros\etc.sql
[0m12:51:04.347217 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:51:04.349801 [debug] [MainThread]: Parsing macros\materializations\copy.sql
[0m12:51:04.351801 [debug] [MainThread]: Parsing macros\materializations\incremental.sql
[0m12:51:04.364388 [debug] [MainThread]: Parsing macros\materializations\seed.sql
[0m12:51:04.366387 [debug] [MainThread]: Parsing macros\materializations\snapshot.sql
[0m12:51:04.368356 [debug] [MainThread]: Parsing macros\materializations\table.sql
[0m12:51:04.371387 [debug] [MainThread]: Parsing macros\materializations\view.sql
[0m12:51:04.373390 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:51:04.374393 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:51:04.374393 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:51:04.375385 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:51:04.376396 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:51:04.376396 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:51:04.377359 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:51:04.377359 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:51:04.377359 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:51:04.378903 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:51:04.378903 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:51:04.378903 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:51:04.379943 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:51:04.380943 [debug] [MainThread]: Parsing macros\adapters\apply_grants.sql
[0m12:51:04.391614 [debug] [MainThread]: Parsing macros\adapters\columns.sql
[0m12:51:04.399636 [debug] [MainThread]: Parsing macros\adapters\freshness.sql
[0m12:51:04.401643 [debug] [MainThread]: Parsing macros\adapters\indexes.sql
[0m12:51:04.403673 [debug] [MainThread]: Parsing macros\adapters\metadata.sql
[0m12:51:04.409980 [debug] [MainThread]: Parsing macros\adapters\persist_docs.sql
[0m12:51:04.412974 [debug] [MainThread]: Parsing macros\adapters\relation.sql
[0m12:51:04.425034 [debug] [MainThread]: Parsing macros\adapters\schema.sql
[0m12:51:04.427056 [debug] [MainThread]: Parsing macros\etc\datetime.sql
[0m12:51:04.432974 [debug] [MainThread]: Parsing macros\etc\statement.sql
[0m12:51:04.436975 [debug] [MainThread]: Parsing macros\generic_test_sql\accepted_values.sql
[0m12:51:04.438394 [debug] [MainThread]: Parsing macros\generic_test_sql\not_null.sql
[0m12:51:04.438394 [debug] [MainThread]: Parsing macros\generic_test_sql\relationships.sql
[0m12:51:04.439395 [debug] [MainThread]: Parsing macros\generic_test_sql\unique.sql
[0m12:51:04.440366 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_alias.sql
[0m12:51:04.441365 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_database.sql
[0m12:51:04.442366 [debug] [MainThread]: Parsing macros\get_custom_name\get_custom_schema.sql
[0m12:51:04.444365 [debug] [MainThread]: Parsing macros\materializations\configs.sql
[0m12:51:04.446365 [debug] [MainThread]: Parsing macros\materializations\hooks.sql
[0m12:51:04.449887 [debug] [MainThread]: Parsing macros\materializations\models\incremental\column_helpers.sql
[0m12:51:04.452891 [debug] [MainThread]: Parsing macros\materializations\models\incremental\incremental.sql
[0m12:51:04.460499 [debug] [MainThread]: Parsing macros\materializations\models\incremental\is_incremental.sql
[0m12:51:04.461500 [debug] [MainThread]: Parsing macros\materializations\models\incremental\merge.sql
[0m12:51:04.474510 [debug] [MainThread]: Parsing macros\materializations\models\incremental\on_schema_change.sql
[0m12:51:04.485548 [debug] [MainThread]: Parsing macros\materializations\models\table\create_table_as.sql
[0m12:51:04.488549 [debug] [MainThread]: Parsing macros\materializations\models\table\table.sql
[0m12:51:04.493083 [debug] [MainThread]: Parsing macros\materializations\models\view\create_or_replace_view.sql
[0m12:51:04.495083 [debug] [MainThread]: Parsing macros\materializations\models\view\create_view_as.sql
[0m12:51:04.497084 [debug] [MainThread]: Parsing macros\materializations\models\view\helpers.sql
[0m12:51:04.498601 [debug] [MainThread]: Parsing macros\materializations\models\view\view.sql
[0m12:51:04.503613 [debug] [MainThread]: Parsing macros\materializations\seeds\helpers.sql
[0m12:51:04.517653 [debug] [MainThread]: Parsing macros\materializations\seeds\seed.sql
[0m12:51:04.523187 [debug] [MainThread]: Parsing macros\materializations\snapshots\helpers.sql
[0m12:51:04.532764 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot.sql
[0m12:51:04.542285 [debug] [MainThread]: Parsing macros\materializations\snapshots\snapshot_merge.sql
[0m12:51:04.543798 [debug] [MainThread]: Parsing macros\materializations\snapshots\strategies.sql
[0m12:51:04.557342 [debug] [MainThread]: Parsing macros\materializations\tests\helpers.sql
[0m12:51:04.558856 [debug] [MainThread]: Parsing macros\materializations\tests\test.sql
[0m12:51:04.561865 [debug] [MainThread]: Parsing macros\materializations\tests\where_subquery.sql
[0m12:51:04.563866 [debug] [MainThread]: Parsing macros\utils\any_value.sql
[0m12:51:04.564867 [debug] [MainThread]: Parsing macros\utils\bool_or.sql
[0m12:51:04.565867 [debug] [MainThread]: Parsing macros\utils\cast_bool_to_text.sql
[0m12:51:04.565867 [debug] [MainThread]: Parsing macros\utils\concat.sql
[0m12:51:04.566867 [debug] [MainThread]: Parsing macros\utils\data_types.sql
[0m12:51:04.571447 [debug] [MainThread]: Parsing macros\utils\dateadd.sql
[0m12:51:04.572451 [debug] [MainThread]: Parsing macros\utils\datediff.sql
[0m12:51:04.573462 [debug] [MainThread]: Parsing macros\utils\date_trunc.sql
[0m12:51:04.574462 [debug] [MainThread]: Parsing macros\utils\escape_single_quotes.sql
[0m12:51:04.575462 [debug] [MainThread]: Parsing macros\utils\except.sql
[0m12:51:04.576463 [debug] [MainThread]: Parsing macros\utils\hash.sql
[0m12:51:04.577462 [debug] [MainThread]: Parsing macros\utils\intersect.sql
[0m12:51:04.577462 [debug] [MainThread]: Parsing macros\utils\last_day.sql
[0m12:51:04.578977 [debug] [MainThread]: Parsing macros\utils\length.sql
[0m12:51:04.579993 [debug] [MainThread]: Parsing macros\utils\listagg.sql
[0m12:51:04.582003 [debug] [MainThread]: Parsing macros\utils\literal.sql
[0m12:51:04.583003 [debug] [MainThread]: Parsing macros\utils\position.sql
[0m12:51:04.584003 [debug] [MainThread]: Parsing macros\utils\replace.sql
[0m12:51:04.585004 [debug] [MainThread]: Parsing macros\utils\right.sql
[0m12:51:04.586004 [debug] [MainThread]: Parsing macros\utils\safe_cast.sql
[0m12:51:04.587004 [debug] [MainThread]: Parsing macros\utils\split_part.sql
[0m12:51:04.588517 [debug] [MainThread]: Parsing tests\generic\builtin.sql
[0m12:51:04.590528 [debug] [MainThread]: Parsing macros\cross_db_utils\any_value.sql
[0m12:51:04.592528 [debug] [MainThread]: Parsing macros\cross_db_utils\array_append.sql
[0m12:51:04.593528 [debug] [MainThread]: Parsing macros\cross_db_utils\array_concat.sql
[0m12:51:04.595528 [debug] [MainThread]: Parsing macros\cross_db_utils\array_construct.sql
[0m12:51:04.597528 [debug] [MainThread]: Parsing macros\cross_db_utils\bool_or.sql
[0m12:51:04.599041 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_array_to_string.sql
[0m12:51:04.601052 [debug] [MainThread]: Parsing macros\cross_db_utils\cast_bool_to_text.sql
[0m12:51:04.602052 [debug] [MainThread]: Parsing macros\cross_db_utils\concat.sql
[0m12:51:04.603565 [debug] [MainThread]: Parsing macros\cross_db_utils\current_timestamp.sql
[0m12:51:04.605576 [debug] [MainThread]: Parsing macros\cross_db_utils\datatypes.sql
[0m12:51:04.610101 [debug] [MainThread]: Parsing macros\cross_db_utils\dateadd.sql
[0m12:51:04.613111 [debug] [MainThread]: Parsing macros\cross_db_utils\datediff.sql
[0m12:51:04.620635 [debug] [MainThread]: Parsing macros\cross_db_utils\date_trunc.sql
[0m12:51:04.621636 [debug] [MainThread]: Parsing macros\cross_db_utils\escape_single_quotes.sql
[0m12:51:04.623636 [debug] [MainThread]: Parsing macros\cross_db_utils\except.sql
[0m12:51:04.624638 [debug] [MainThread]: Parsing macros\cross_db_utils\hash.sql
[0m12:51:04.625637 [debug] [MainThread]: Parsing macros\cross_db_utils\identifier.sql
[0m12:51:04.627635 [debug] [MainThread]: Parsing macros\cross_db_utils\intersect.sql
[0m12:51:04.627635 [debug] [MainThread]: Parsing macros\cross_db_utils\last_day.sql
[0m12:51:04.631158 [debug] [MainThread]: Parsing macros\cross_db_utils\length.sql
[0m12:51:04.632159 [debug] [MainThread]: Parsing macros\cross_db_utils\listagg.sql
[0m12:51:04.639193 [debug] [MainThread]: Parsing macros\cross_db_utils\literal.sql
[0m12:51:04.639193 [debug] [MainThread]: Parsing macros\cross_db_utils\position.sql
[0m12:51:04.641218 [debug] [MainThread]: Parsing macros\cross_db_utils\replace.sql
[0m12:51:04.642218 [debug] [MainThread]: Parsing macros\cross_db_utils\right.sql
[0m12:51:04.643218 [debug] [MainThread]: Parsing macros\cross_db_utils\safe_cast.sql
[0m12:51:04.645218 [debug] [MainThread]: Parsing macros\cross_db_utils\split_part.sql
[0m12:51:04.649738 [debug] [MainThread]: Parsing macros\cross_db_utils\width_bucket.sql
[0m12:51:04.653737 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_ephemeral.sql
[0m12:51:04.655738 [debug] [MainThread]: Parsing macros\cross_db_utils\_is_relation.sql
[0m12:51:04.655738 [debug] [MainThread]: Parsing macros\generic_tests\accepted_range.sql
[0m12:51:04.657738 [debug] [MainThread]: Parsing macros\generic_tests\at_least_one.sql
[0m12:51:04.659253 [debug] [MainThread]: Parsing macros\generic_tests\cardinality_equality.sql
[0m12:51:04.661264 [debug] [MainThread]: Parsing macros\generic_tests\equality.sql
[0m12:51:04.663777 [debug] [MainThread]: Parsing macros\generic_tests\equal_rowcount.sql
[0m12:51:04.665788 [debug] [MainThread]: Parsing macros\generic_tests\expression_is_true.sql
[0m12:51:04.666788 [debug] [MainThread]: Parsing macros\generic_tests\fewer_rows_than.sql
[0m12:51:04.668788 [debug] [MainThread]: Parsing macros\generic_tests\mutually_exclusive_ranges.sql
[0m12:51:04.675309 [debug] [MainThread]: Parsing macros\generic_tests\not_accepted_values.sql
[0m12:51:04.677310 [debug] [MainThread]: Parsing macros\generic_tests\not_constant.sql
[0m12:51:04.677310 [debug] [MainThread]: Parsing macros\generic_tests\not_null_proportion.sql
[0m12:51:04.679833 [debug] [MainThread]: Parsing macros\generic_tests\recency.sql
[0m12:51:04.680833 [debug] [MainThread]: Parsing macros\generic_tests\relationships_where.sql
[0m12:51:04.682833 [debug] [MainThread]: Parsing macros\generic_tests\sequential_values.sql
[0m12:51:04.685833 [debug] [MainThread]: Parsing macros\generic_tests\test_not_null_where.sql
[0m12:51:04.686833 [debug] [MainThread]: Parsing macros\generic_tests\test_unique_where.sql
[0m12:51:04.687834 [debug] [MainThread]: Parsing macros\generic_tests\unique_combination_of_columns.sql
[0m12:51:04.690356 [debug] [MainThread]: Parsing macros\jinja_helpers\log_info.sql
[0m12:51:04.691357 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_log_format.sql
[0m12:51:04.692361 [debug] [MainThread]: Parsing macros\jinja_helpers\pretty_time.sql
[0m12:51:04.692361 [debug] [MainThread]: Parsing macros\jinja_helpers\slugify.sql
[0m12:51:04.693372 [debug] [MainThread]: Parsing macros\materializations\insert_by_period_materialization.sql
[0m12:51:04.713481 [debug] [MainThread]: Parsing macros\sql\date_spine.sql
[0m12:51:04.716482 [debug] [MainThread]: Parsing macros\sql\deduplicate.sql
[0m12:51:04.722001 [debug] [MainThread]: Parsing macros\sql\generate_series.sql
[0m12:51:04.725524 [debug] [MainThread]: Parsing macros\sql\get_column_values.sql
[0m12:51:04.730048 [debug] [MainThread]: Parsing macros\sql\get_filtered_columns_in_relation.sql
[0m12:51:04.732059 [debug] [MainThread]: Parsing macros\sql\get_query_results_as_dict.sql
[0m12:51:04.734058 [debug] [MainThread]: Parsing macros\sql\get_relations_by_pattern.sql
[0m12:51:04.737058 [debug] [MainThread]: Parsing macros\sql\get_relations_by_prefix.sql
[0m12:51:04.739584 [debug] [MainThread]: Parsing macros\sql\get_tables_by_pattern_sql.sql
[0m12:51:04.745583 [debug] [MainThread]: Parsing macros\sql\get_tables_by_prefix_sql.sql
[0m12:51:04.746584 [debug] [MainThread]: Parsing macros\sql\get_table_types_sql.sql
[0m12:51:04.747584 [debug] [MainThread]: Parsing macros\sql\groupby.sql
[0m12:51:04.749095 [debug] [MainThread]: Parsing macros\sql\haversine_distance.sql
[0m12:51:04.753620 [debug] [MainThread]: Parsing macros\sql\nullcheck.sql
[0m12:51:04.755631 [debug] [MainThread]: Parsing macros\sql\nullcheck_table.sql
[0m12:51:04.756631 [debug] [MainThread]: Parsing macros\sql\pivot.sql
[0m12:51:04.760156 [debug] [MainThread]: Parsing macros\sql\safe_add.sql
[0m12:51:04.762163 [debug] [MainThread]: Parsing macros\sql\star.sql
[0m12:51:04.765161 [debug] [MainThread]: Parsing macros\sql\surrogate_key.sql
[0m12:51:04.767162 [debug] [MainThread]: Parsing macros\sql\union.sql
[0m12:51:04.776671 [debug] [MainThread]: Parsing macros\sql\unpivot.sql
[0m12:51:04.782178 [debug] [MainThread]: Parsing macros\web\get_url_host.sql
[0m12:51:04.784703 [debug] [MainThread]: Parsing macros\web\get_url_parameter.sql
[0m12:51:04.785702 [debug] [MainThread]: Parsing macros\web\get_url_path.sql
[0m12:51:05.123194 [debug] [MainThread]: 1699: static parser successfully parsed prod\agg_transactions.sql
[0m12:51:05.132718 [debug] [MainThread]: 1699: static parser successfully parsed prod\dim_customers.sql
[0m12:51:05.134714 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m12:51:05.138715 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m12:51:05.140232 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_customers.sql
[0m12:51:05.142232 [debug] [MainThread]: 1699: static parser successfully parsed stage\stg_orders.sql
[0m12:51:05.144463 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m12:51:05.148462 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m12:51:05.251716 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067C143AC0>]}
[0m12:51:05.261298 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067ADE30D0>]}
[0m12:51:05.261298 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m12:51:05.262270 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067B00DA30>]}
[0m12:51:05.263811 [info ] [MainThread]: 
[0m12:51:05.264818 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:51:05.265850 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m12:51:05.265850 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:51:05.750042 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m12:51:05.751047 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:51:06.278938 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067C143D30>]}
[0m12:51:06.279951 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m12:51:06.280952 [info ] [MainThread]: 
[0m12:51:06.286972 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m12:51:06.286972 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m12:51:06.288478 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m12:51:06.288478 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m12:51:06.288478 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m12:51:06.288478 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m12:51:06.288478 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m12:51:06.289532 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m12:51:06.289532 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m12:51:06.289532 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m12:51:06.290542 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m12:51:06.290542 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m12:51:06.291540 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m12:51:06.291540 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m12:51:06.339856 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:51:06.339856 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m12:51:06.339856 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m12:51:06.340857 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m12:51:06.340857 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m12:51:06.343384 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:51:06.344386 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m12:51:06.344386 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m12:51:06.347386 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:51:06.351031 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:51:06.352026 [debug] [Thread-1  ]: finished collecting timing info
[0m12:51:06.352026 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m12:51:06.366573 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m12:51:06.367543 [debug] [Thread-3  ]: finished collecting timing info
[0m12:51:06.367543 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m12:51:06.372714 [debug] [Thread-2  ]: finished collecting timing info
[0m12:51:06.372714 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m12:51:06.414009 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m12:51:06.416009 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m12:51:06.416009 [debug] [Thread-4  ]: finished collecting timing info
[0m12:51:06.416009 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m12:51:06.419517 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m12:51:06.421527 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m12:51:06.422518 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m12:51:06.422518 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m12:51:06.426517 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    CONCAT('$', ROUND(amount/100, 1)) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m12:51:06.427517 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m12:51:06.429033 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m12:51:06.875919 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m12:51:06.876919 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m12:51:07.437565 [debug] [Thread-4  ]: finished collecting timing info
[0m12:51:07.438575 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067AE71280>]}
[0m12:51:07.438575 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.10s]
[0m12:51:07.440573 [debug] [Thread-3  ]: finished collecting timing info
[0m12:51:07.440573 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067C1EA760>]}
[0m12:51:07.440573 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.15s]
[0m12:51:07.441574 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m12:51:07.442584 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m12:51:07.443571 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m12:51:07.443571 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m12:51:07.444571 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m12:51:07.444571 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m12:51:07.444571 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m12:51:07.448576 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:51:07.449924 [debug] [Thread-5  ]: finished collecting timing info
[0m12:51:07.449924 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m12:51:07.453241 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:51:07.454256 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m12:51:07.458805 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m12:51:07.535665 [debug] [Thread-2  ]: finished collecting timing info
[0m12:51:07.535665 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067AE56760>]}
[0m12:51:07.536667 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.25s]
[0m12:51:07.537659 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m12:51:07.537659 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.dim_customers
[0m12:51:07.537659 [info ] [Thread-4  ]: 6 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m12:51:07.538669 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m12:51:07.538669 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m12:51:07.538669 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.dim_customers
[0m12:51:07.542747 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:51:07.544756 [debug] [Thread-4  ]: finished collecting timing info
[0m12:51:07.544756 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m12:51:07.547759 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m12:51:07.983537 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m12:51:07.987527 [debug] [Thread-4  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m12:51:08.657378 [debug] [Thread-5  ]: BigQuery adapter: Retry attempt 1 of 1 after error: BadRequest('No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]')
[0m12:51:09.559141 [debug] [Thread-1  ]: finished collecting timing info
[0m12:51:09.559141 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067B075EE0>]}
[0m12:51:09.560192 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.27s]
[0m12:51:09.561200 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m12:51:10.600855 [debug] [Thread-5  ]: finished collecting timing info
[0m12:51:10.601849 [debug] [Thread-5  ]: Database Error in model pivoted_orders (models\prod\pivoted_orders.sql)
  No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
  compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:51:10.601849 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067B06DA60>]}
[0m12:51:10.601849 [error] [Thread-5  ]: 5 of 6 ERROR creating table model dbt_x_airflow.pivoted_orders ................. [[31mERROR[0m in 3.16s]
[0m12:51:10.603855 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m12:51:10.714389 [debug] [Thread-4  ]: finished collecting timing info
[0m12:51:10.714389 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'f4fcc5f9-1e2a-4b95-b718-9e4de7c6eee4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067AE64BB0>]}
[0m12:51:10.714389 [info ] [Thread-4  ]: 6 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.18s]
[0m12:51:10.716390 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m12:51:10.717381 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:51:10.717381 [info ] [MainThread]: 
[0m12:51:10.717381 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 5.45 seconds (5.45s).
[0m12:51:10.718885 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:51:10.718885 [debug] [MainThread]: Connection 'list_airflow-docker-352518' was properly closed.
[0m12:51:10.718885 [debug] [MainThread]: Connection 'list_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m12:51:10.718885 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m12:51:10.718885 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m12:51:10.718885 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m12:51:10.718885 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m12:51:10.718885 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m12:51:10.734478 [info ] [MainThread]: 
[0m12:51:10.735486 [info ] [MainThread]: [31mCompleted with 1 error and 0 warnings:[0m
[0m12:51:10.735486 [info ] [MainThread]: 
[0m12:51:10.736486 [error] [MainThread]: [33mDatabase Error in model pivoted_orders (models\prod\pivoted_orders.sql)[0m
[0m12:51:10.738497 [error] [MainThread]:   No matching signature for function IF for argument types: BOOL, STRING, INT64. Supported signature: IF(BOOL, ANY, ANY) at [11:10]
[0m12:51:10.739498 [error] [MainThread]:   compiled SQL at target\run\dbt_x_airflow\models\prod\pivoted_orders.sql
[0m12:51:10.739498 [info ] [MainThread]: 
[0m12:51:10.740496 [info ] [MainThread]: Done. PASS=5 WARN=0 ERROR=1 SKIP=0 TOTAL=6
[0m12:51:10.741497 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067AE5A040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067C1A7580>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002067C143D90>]}


============================== 2022-08-07 12:54:38.520760 | ef1ad6af-d93e-45c1-bd60-2d4abf0416a0 ==============================
[0m12:54:38.520760 [info ] [MainThread]: Running with dbt=1.2.0
[0m12:54:38.520760 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'parse_only': False, 'select': ['pivoted_orders'], 'which': 'compile', 'rpc_method': 'compile', 'indirect_selection': 'eager'}
[0m12:54:38.521761 [debug] [MainThread]: Tracking: tracking
[0m12:54:38.531089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EC544F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EC54AC0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EC54580>]}
[0m12:54:38.640577 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m12:54:38.640577 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m12:54:38.646578 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ef1ad6af-d93e-45c1-bd60-2d4abf0416a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EEF4310>]}
[0m12:54:38.655594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ef1ad6af-d93e-45c1-bd60-2d4abf0416a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EDF62B0>]}
[0m12:54:38.656554 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m12:54:38.656554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef1ad6af-d93e-45c1-bd60-2d4abf0416a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EC720D0>]}
[0m12:54:38.657558 [info ] [MainThread]: 
[0m12:54:38.658565 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m12:54:38.660068 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m12:54:38.660068 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m12:54:39.246030 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ef1ad6af-d93e-45c1-bd60-2d4abf0416a0', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EED8FD0>]}
[0m12:54:39.246030 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m12:54:39.247028 [info ] [MainThread]: 
[0m12:54:39.253588 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m12:54:39.253588 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m12:54:39.253588 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m12:54:39.254589 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m12:54:39.257589 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m12:54:39.258621 [debug] [Thread-1  ]: finished collecting timing info
[0m12:54:39.259135 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m12:54:39.259135 [debug] [Thread-1  ]: finished collecting timing info
[0m12:54:39.259135 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m12:54:39.260527 [debug] [MainThread]: Connection 'master' was properly closed.
[0m12:54:39.260527 [debug] [MainThread]: Connection 'list_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m12:54:39.260527 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m12:54:39.270547 [info ] [MainThread]: Done.
[0m12:54:39.271555 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EDF64F0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EDF6460>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002CC2EF57A30>]}


============================== 2022-08-07 13:07:15.583866 | 96d27185-9467-4bda-a5c9-8737c11246f4 ==============================
[0m13:07:15.583866 [info ] [MainThread]: Running with dbt=1.2.0
[0m13:07:15.584876 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m13:07:15.584876 [debug] [MainThread]: Tracking: tracking
[0m13:07:15.599086 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A3971C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A3978E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A397A90>]}
[0m13:07:15.697753 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 1 files changed.
[0m13:07:15.697753 [debug] [MainThread]: Partial parsing: deleted exposure exposure.dbt_x_airflow.customer_dashboard
[0m13:07:15.697753 [debug] [MainThread]: Partial parsing: updated file: dbt_x_airflow://macros\custom_macros.sql
[0m13:07:15.697753 [debug] [MainThread]: Parsing macros\custom_macros.sql
[0m13:07:15.709452 [debug] [MainThread]: 1603: static parser failed on prod\pivoted_orders.sql
[0m13:07:15.721044 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod\pivoted_orders.sql
[0m13:07:15.722561 [debug] [MainThread]: 1603: static parser failed on stage\stg_payments.sql
[0m13:07:15.726617 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage\stg_payments.sql
[0m13:07:15.746335 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A6813A0>]}
[0m13:07:15.755425 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A4F6F40>]}
[0m13:07:15.756415 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m13:07:15.756415 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A3B20A0>]}
[0m13:07:15.757382 [info ] [MainThread]: 
[0m13:07:15.758888 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m13:07:15.759919 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m13:07:15.759919 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:07:16.283268 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m13:07:16.283268 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:07:16.742149 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A3975E0>]}
[0m13:07:16.742149 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m13:07:16.743149 [info ] [MainThread]: 
[0m13:07:16.749665 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m13:07:16.749665 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m13:07:16.749665 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m13:07:16.749665 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m13:07:16.750671 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m13:07:16.750671 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m13:07:16.750671 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m13:07:16.750671 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m13:07:16.751672 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m13:07:16.752671 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m13:07:16.752671 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m13:07:16.752671 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m13:07:16.752671 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m13:07:16.755914 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m13:07:16.756914 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m13:07:16.756914 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m13:07:16.756914 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m13:07:16.761511 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m13:07:16.761511 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m13:07:16.761511 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m13:07:16.762507 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m13:07:16.766507 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m13:07:16.766507 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:16.766507 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m13:07:16.767508 [debug] [Thread-2  ]: finished collecting timing info
[0m13:07:16.767508 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m13:07:16.769907 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m13:07:16.769907 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m13:07:16.776900 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:16.784919 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:07:16.795855 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m13:07:16.808845 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m13:07:16.808845 [debug] [Thread-4  ]: finished collecting timing info
[0m13:07:16.811347 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m13:07:16.812352 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m13:07:16.813897 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m13:07:16.814937 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m13:07:16.814937 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m13:07:16.815903 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m13:07:16.820420 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m13:07:16.820420 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m13:07:16.820420 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m13:07:17.241612 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m13:07:17.242617 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m13:07:18.027850 [debug] [Thread-2  ]: finished collecting timing info
[0m13:07:18.027850 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A6D5AF0>]}
[0m13:07:18.027850 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.28s]
[0m13:07:18.029853 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m13:07:18.132186 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:18.133200 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A7A2130>]}
[0m13:07:18.133200 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.38s]
[0m13:07:18.134205 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m13:07:18.135205 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.dim_customers
[0m13:07:18.135205 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m13:07:18.136205 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m13:07:18.136205 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m13:07:18.136205 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.dim_customers
[0m13:07:18.139213 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m13:07:18.140804 [debug] [Thread-5  ]: finished collecting timing info
[0m13:07:18.140804 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m13:07:18.142819 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m13:07:18.163870 [debug] [Thread-4  ]: finished collecting timing info
[0m13:07:18.163870 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A6D5BB0>]}
[0m13:07:18.163870 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.40s]
[0m13:07:18.164883 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m13:07:18.165869 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m13:07:18.165869 [info ] [Thread-3  ]: 6 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m13:07:18.166880 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m13:07:18.166880 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m13:07:18.166880 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m13:07:18.169396 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m13:07:18.170417 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:18.170417 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m13:07:18.172431 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m13:07:18.173434 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m13:07:18.177458 [debug] [Thread-3  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m13:07:18.512028 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m13:07:18.514046 [debug] [Thread-5  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m13:07:19.935850 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:19.935850 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A6D5A30>]}
[0m13:07:19.935850 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.18s]
[0m13:07:19.937850 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m13:07:20.792197 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:20.792197 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A788580>]}
[0m13:07:20.792197 [info ] [Thread-3  ]: 6 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 2.63s]
[0m13:07:20.794189 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m13:07:21.674796 [debug] [Thread-5  ]: finished collecting timing info
[0m13:07:21.674796 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '96d27185-9467-4bda-a5c9-8737c11246f4', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A725C70>]}
[0m13:07:21.675794 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.54s]
[0m13:07:21.677747 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m13:07:21.679314 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m13:07:21.680365 [info ] [MainThread]: 
[0m13:07:21.680365 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 5.92 seconds (5.92s).
[0m13:07:21.681372 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:07:21.681372 [debug] [MainThread]: Connection 'list_airflow-docker-352518' was properly closed.
[0m13:07:21.681372 [debug] [MainThread]: Connection 'list_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m13:07:21.681372 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m13:07:21.681372 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m13:07:21.681372 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m13:07:21.681372 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_payments' was properly closed.
[0m13:07:21.682373 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m13:07:21.694062 [info ] [MainThread]: 
[0m13:07:21.694062 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:07:21.695050 [info ] [MainThread]: 
[0m13:07:21.696051 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m13:07:21.696051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A6FA4C0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A725C70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000001B41A3B24C0>]}


============================== 2022-08-07 13:07:29.305982 | 5ebba5fd-a8fa-4772-b8f7-645d172389c7 ==============================
[0m13:07:29.305982 [info ] [MainThread]: Running with dbt=1.2.0
[0m13:07:29.306982 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': 'C:\\Users\\Vanmai40\\.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m13:07:29.306982 [debug] [MainThread]: Tracking: tracking
[0m13:07:29.320496 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E631016190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E6310168E0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E6310163A0>]}
[0m13:07:29.434027 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m13:07:29.434027 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m13:07:29.439532 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5ebba5fd-a8fa-4772-b8f7-645d172389c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E6312741F0>]}
[0m13:07:29.449554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5ebba5fd-a8fa-4772-b8f7-645d172389c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E63119A2E0>]}
[0m13:07:29.449554 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m13:07:29.450872 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ebba5fd-a8fa-4772-b8f7-645d172389c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E63101DFA0>]}
[0m13:07:29.452874 [info ] [MainThread]: 
[0m13:07:29.453873 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m13:07:29.454873 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m13:07:29.454873 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m13:07:29.892333 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5ebba5fd-a8fa-4772-b8f7-645d172389c7', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E630FD6F70>]}
[0m13:07:29.892333 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m13:07:29.893338 [info ] [MainThread]: 
[0m13:07:29.898849 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m13:07:29.899884 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m13:07:29.899884 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m13:07:29.899884 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m13:07:29.899884 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m13:07:29.899884 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m13:07:29.900888 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m13:07:29.900888 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m13:07:29.900888 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m13:07:29.901891 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m13:07:29.901891 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m13:07:29.902893 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m13:07:29.902893 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m13:07:29.903892 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m13:07:29.903892 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m13:07:29.908913 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m13:07:29.917900 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m13:07:29.917900 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m13:07:29.918900 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m13:07:29.918900 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m13:07:29.928494 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m13:07:29.928494 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m13:07:29.929114 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m13:07:29.929114 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m13:07:29.933141 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m13:07:29.933141 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m13:07:29.933141 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:29.934131 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m13:07:29.934131 [debug] [Thread-2  ]: finished collecting timing info
[0m13:07:29.938131 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m13:07:29.938131 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m13:07:29.939137 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m13:07:29.939137 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:29.939137 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m13:07:29.953244 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m13:07:29.959754 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m13:07:29.959754 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m13:07:29.963781 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m13:07:29.963781 [debug] [Thread-4  ]: finished collecting timing info
[0m13:07:29.966775 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m13:07:29.966775 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m13:07:29.967771 [debug] [Thread-5  ]: finished collecting timing info
[0m13:07:29.969867 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m13:07:29.969867 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m13:07:29.970872 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m13:07:29.973867 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m13:07:29.973867 [debug] [Thread-1  ]: Opening a new connection, currently in state init
[0m13:07:29.974868 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m13:07:29.974868 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m13:07:29.975868 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m13:07:29.978901 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m13:07:29.981059 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m13:07:29.982061 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m13:07:29.983421 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m13:07:29.985432 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m13:07:31.577145 [debug] [Thread-4  ]: finished collecting timing info
[0m13:07:31.578147 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.66s]
[0m13:07:31.579151 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m13:07:31.579151 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m13:07:31.579151 [info ] [Thread-4  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m13:07:31.580596 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m13:07:31.580596 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m13:07:31.580596 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m13:07:31.586614 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m13:07:31.589864 [debug] [Thread-4  ]: finished collecting timing info
[0m13:07:31.589864 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m13:07:31.591857 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m13:07:31.592854 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m13:07:31.596855 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:31.596855 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.69s]
[0m13:07:31.598859 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:31.600004 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m13:07:31.601007 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m13:07:31.601007 [info ] [Thread-3  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m13:07:31.602003 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m13:07:31.602003 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m13:07:31.603007 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m13:07:31.606018 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m13:07:31.607015 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:31.607015 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m13:07:31.609532 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m13:07:31.609532 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m13:07:31.614802 [debug] [Thread-3  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:31.636100 [debug] [Thread-2  ]: finished collecting timing info
[0m13:07:31.636100 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.73s]
[0m13:07:31.638095 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m13:07:31.638095 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m13:07:31.638095 [info ] [Thread-2  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m13:07:31.639603 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m13:07:31.639603 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m13:07:31.639603 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m13:07:31.645621 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m13:07:31.646622 [debug] [Thread-2  ]: finished collecting timing info
[0m13:07:31.646622 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m13:07:31.650154 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m13:07:31.652150 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m13:07:31.657147 [debug] [Thread-2  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:31.803293 [debug] [Thread-5  ]: finished collecting timing info
[0m13:07:31.803293 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 1.87s]
[0m13:07:31.805304 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m13:07:31.805304 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m13:07:31.805304 [info ] [Thread-5  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m13:07:31.806293 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m13:07:31.806293 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m13:07:31.806293 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m13:07:31.809847 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m13:07:31.811837 [debug] [Thread-5  ]: finished collecting timing info
[0m13:07:31.811837 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m13:07:31.814853 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m13:07:31.815844 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m13:07:31.821410 [debug] [Thread-5  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m13:07:31.831100 [debug] [Thread-1  ]: finished collecting timing info
[0m13:07:31.832095 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 1.93s]
[0m13:07:31.833088 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m13:07:33.321532 [debug] [Thread-3  ]: finished collecting timing info
[0m13:07:33.322497 [info ] [Thread-3  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 1.72s]
[0m13:07:33.329127 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m13:07:33.450650 [debug] [Thread-2  ]: finished collecting timing info
[0m13:07:33.450650 [info ] [Thread-2  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.81s]
[0m13:07:33.452647 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m13:07:33.510782 [debug] [Thread-4  ]: finished collecting timing info
[0m13:07:33.510782 [info ] [Thread-4  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.93s]
[0m13:07:33.511773 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m13:07:33.571361 [debug] [Thread-5  ]: finished collecting timing info
[0m13:07:33.572366 [info ] [Thread-5  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.77s]
[0m13:07:33.573353 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m13:07:33.574354 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m13:07:33.574354 [info ] [MainThread]: 
[0m13:07:33.575354 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.12 seconds (4.12s).
[0m13:07:33.576353 [debug] [MainThread]: Connection 'master' was properly closed.
[0m13:07:33.576353 [debug] [MainThread]: Connection 'list_airflow-docker-352518_dbt_x_airflow' was properly closed.
[0m13:07:33.576353 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m13:07:33.576353 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m13:07:33.576353 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m13:07:33.576353 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m13:07:33.576353 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m13:07:33.586947 [info ] [MainThread]: 
[0m13:07:33.587942 [info ] [MainThread]: [32mCompleted successfully[0m
[0m13:07:33.588941 [info ] [MainThread]: 
[0m13:07:33.588941 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m13:07:33.588941 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E63119A370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E63119A400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x000002E63119A3D0>]}


============================== 2022-08-21 11:26:34.050830 | b5a93159-bc31-4db6-9204-a1a74a48ec5f ==============================
[0m11:26:34.050849 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:26:34.052378 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m11:26:34.053342 [debug] [MainThread]: Tracking: tracking
[0m11:26:34.058243 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca6f9e3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca6f9eee0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca6f9ef70>]}
[0m11:26:34.134424 [info ] [MainThread]: Unable to do partial parsing because profile has changed
[0m11:26:34.135589 [info ] [MainThread]: Unable to do partial parsing because env vars used in profiles.yml have changed
[0m11:26:34.136982 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca6f9bb20>]}
[0m11:26:36.219540 [debug] [MainThread]: Parsing macros/custom_macros.sql
[0m11:26:36.221112 [debug] [MainThread]: Parsing macros/etc.sql
[0m11:26:36.223620 [debug] [MainThread]: Parsing macros/adapters.sql
[0m11:26:36.247755 [debug] [MainThread]: Parsing macros/catalog.sql
[0m11:26:36.254696 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m11:26:36.258803 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m11:26:36.263021 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m11:26:36.265660 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m11:26:36.283158 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m11:26:36.289068 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m11:26:36.293472 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m11:26:36.297208 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m11:26:36.299343 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m11:26:36.301625 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m11:26:36.304175 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m11:26:36.306011 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m11:26:36.307752 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m11:26:36.309581 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m11:26:36.311241 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m11:26:36.312997 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m11:26:36.314585 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m11:26:36.316092 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m11:26:36.317401 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m11:26:36.318706 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m11:26:36.320471 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m11:26:36.331685 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m11:26:36.347029 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m11:26:36.352628 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m11:26:36.356726 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m11:26:36.372185 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m11:26:36.376797 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m11:26:36.380196 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m11:26:36.388204 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m11:26:36.392187 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m11:26:36.395010 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m11:26:36.397463 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m11:26:36.406859 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m11:26:36.412762 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m11:26:36.414590 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m11:26:36.416389 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m11:26:36.418140 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m11:26:36.420464 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m11:26:36.424099 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m11:26:36.429036 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m11:26:36.431931 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m11:26:36.434799 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m11:26:36.440636 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m11:26:36.460599 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m11:26:36.468495 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m11:26:36.471428 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m11:26:36.484490 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m11:26:36.502586 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m11:26:36.516012 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m11:26:36.532339 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m11:26:36.548907 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m11:26:36.559439 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m11:26:36.562139 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m11:26:36.567339 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m11:26:36.571467 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m11:26:36.573828 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m11:26:36.580283 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m11:26:36.583661 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m11:26:36.587456 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m11:26:36.594324 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m11:26:36.597657 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m11:26:36.600454 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m11:26:36.602146 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m11:26:36.604600 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m11:26:36.606572 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m11:26:36.608477 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m11:26:36.610672 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m11:26:36.612772 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m11:26:36.614729 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m11:26:36.617531 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m11:26:36.619298 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m11:26:36.621137 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m11:26:36.623056 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m11:26:36.624955 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m11:26:36.627079 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m11:26:36.628868 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m11:26:36.630515 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m11:26:36.636968 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m11:26:36.638735 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m11:26:36.640592 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m11:26:36.642951 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m11:26:36.647172 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m11:26:36.649581 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m11:26:36.652621 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m11:26:36.655813 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m11:26:36.660266 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m11:26:36.662959 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m11:26:36.666735 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m11:26:36.669339 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m11:26:36.671722 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m11:26:36.676793 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m11:26:36.684140 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m11:26:36.689187 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m11:26:36.700369 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m11:26:36.703073 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m11:26:36.706311 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m11:26:36.708509 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m11:26:36.710966 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m11:26:36.713246 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m11:26:36.715093 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m11:26:36.719292 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m11:26:36.722000 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m11:26:36.731997 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m11:26:36.733988 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m11:26:36.736633 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m11:26:36.739363 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m11:26:36.742595 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m11:26:36.745760 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m11:26:36.752530 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m11:26:36.758706 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m11:26:36.762065 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m11:26:36.764246 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m11:26:36.767739 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m11:26:36.769988 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m11:26:36.773541 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m11:26:36.778560 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m11:26:36.781587 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m11:26:36.784827 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m11:26:36.787537 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m11:26:36.797455 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m11:26:36.800740 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m11:26:36.802878 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m11:26:36.806215 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m11:26:36.809344 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m11:26:36.812836 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m11:26:36.816701 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m11:26:36.819063 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m11:26:36.821783 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m11:26:36.825854 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m11:26:36.828204 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m11:26:36.830174 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m11:26:36.832329 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m11:26:36.834411 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m11:26:36.859150 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m11:26:36.864603 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m11:26:36.872443 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m11:26:36.878031 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m11:26:36.884768 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m11:26:36.888506 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m11:26:36.892331 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m11:26:36.896972 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m11:26:36.901222 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m11:26:36.908476 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m11:26:36.911503 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m11:26:36.914000 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m11:26:36.916288 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m11:26:36.923145 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m11:26:36.926347 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m11:26:36.929097 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m11:26:36.934079 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m11:26:36.936759 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m11:26:36.942114 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m11:26:36.946968 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m11:26:36.959126 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m11:26:36.968291 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m11:26:36.971535 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m11:26:36.974283 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m11:26:37.456623 [debug] [MainThread]: 1699: static parser successfully parsed prod/agg_transactions.sql
[0m11:26:37.471013 [debug] [MainThread]: 1699: static parser successfully parsed prod/dim_customers.sql
[0m11:26:37.475061 [debug] [MainThread]: 1603: static parser failed on prod/pivoted_orders.sql
[0m11:26:37.481822 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod/pivoted_orders.sql
[0m11:26:37.484312 [debug] [MainThread]: 1699: static parser successfully parsed stage/stg_customers.sql
[0m11:26:37.488502 [debug] [MainThread]: 1699: static parser successfully parsed stage/stg_orders.sql
[0m11:26:37.492064 [debug] [MainThread]: 1603: static parser failed on stage/stg_payments.sql
[0m11:26:37.499733 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_payments.sql
[0m11:26:37.661101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca5513940>]}
[0m11:26:37.684293 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca54f9ee0>]}
[0m11:26:37.685289 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:26:37.686564 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca6f73400>]}
[0m11:26:37.689028 [info ] [MainThread]: 
[0m11:26:37.690620 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:26:37.693012 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m11:26:37.694259 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:26:38.326995 [debug] [ThreadPool]: Acquiring new bigquery connection "create_airflow-docker-352518_dbt_x_airflow"
[0m11:26:38.328329 [debug] [ThreadPool]: Acquiring new bigquery connection "create_airflow-docker-352518_dbt_x_airflow"
[0m11:26:38.329105 [debug] [ThreadPool]: BigQuery adapter: Creating schema "airflow-docker-352518.dbt_x_airflow".
[0m11:26:38.329863 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:26:39.179276 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:26:39.180357 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:26:39.767656 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca55131c0>]}
[0m11:26:39.769191 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:26:39.770328 [info ] [MainThread]: 
[0m11:26:39.783036 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:26:39.783498 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:26:39.783800 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:26:39.783980 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:26:39.784661 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m11:26:39.785714 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m11:26:39.786502 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m11:26:39.787394 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m11:26:39.788872 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:26:39.789955 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:26:39.791103 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:26:39.792239 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:26:39.792821 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:26:39.793530 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:26:39.794192 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:26:39.794907 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:26:39.795593 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:26:39.796292 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:26:39.796877 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:26:39.797572 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:26:39.801538 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:26:39.809997 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:26:39.813327 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:26:39.819224 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:26:39.826067 [debug] [Thread-2  ]: finished collecting timing info
[0m11:26:39.826534 [debug] [Thread-3  ]: finished collecting timing info
[0m11:26:39.827634 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:26:39.827943 [debug] [Thread-4  ]: finished collecting timing info
[0m11:26:39.828468 [debug] [Thread-1  ]: finished collecting timing info
[0m11:26:39.829289 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:26:39.852337 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:26:39.866022 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:26:39.867114 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:26:39.871150 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:26:39.875458 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:26:39.903710 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:26:39.904753 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:26:39.912250 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:26:39.913571 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:26:39.914483 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:26:39.950252 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m11:26:39.959506 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m11:26:39.963250 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m11:26:39.963466 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m11:26:41.325860 [debug] [Thread-3  ]: finished collecting timing info
[0m11:26:41.327985 [debug] [Thread-2  ]: finished collecting timing info
[0m11:26:41.329366 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca54e9fa0>]}
[0m11:26:41.330806 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca53b97c0>]}
[0m11:26:41.332102 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.54s]
[0m11:26:41.333232 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.54s]
[0m11:26:41.334595 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:26:41.335518 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:26:41.337754 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:26:41.338686 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m11:26:41.339994 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:26:41.340763 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:26:41.341554 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:26:41.346331 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:26:41.354488 [debug] [Thread-5  ]: finished collecting timing info
[0m11:26:41.355361 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:26:41.359358 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:26:41.368458 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:26:41.411276 [debug] [Thread-4  ]: finished collecting timing info
[0m11:26:41.412743 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca54e9610>]}
[0m11:26:41.413541 [debug] [Thread-5  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m11:26:41.413948 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.62s]
[0m11:26:41.417522 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:26:41.418996 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:26:41.420077 [info ] [Thread-2  ]: 6 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m11:26:41.421491 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:26:41.422166 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:26:41.422831 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:26:41.428185 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:26:41.435660 [debug] [Thread-2  ]: finished collecting timing info
[0m11:26:41.436459 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:26:41.440572 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:26:41.449136 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m11:26:41.492423 [debug] [Thread-2  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m11:26:43.470772 [debug] [Thread-1  ]: finished collecting timing info
[0m11:26:43.472805 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca54c1280>]}
[0m11:26:43.474097 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.68s]
[0m11:26:43.475478 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:26:44.767954 [debug] [Thread-2  ]: finished collecting timing info
[0m11:26:44.769808 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca427ca30>]}
[0m11:26:44.771031 [info ] [Thread-2  ]: 6 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 3.35s]
[0m11:26:44.772339 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:26:44.867613 [debug] [Thread-5  ]: finished collecting timing info
[0m11:26:44.869215 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'b5a93159-bc31-4db6-9204-a1a74a48ec5f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca54c10d0>]}
[0m11:26:44.870651 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.53s]
[0m11:26:44.871847 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:26:44.874859 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:26:44.876233 [info ] [MainThread]: 
[0m11:26:44.877255 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 7.19 seconds (7.19s).
[0m11:26:44.878226 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:26:44.879004 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m11:26:44.879677 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:26:44.880390 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m11:26:44.881036 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_payments' was properly closed.
[0m11:26:44.881646 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:26:44.915527 [info ] [MainThread]: 
[0m11:26:44.916816 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:26:44.918048 [info ] [MainThread]: 
[0m11:26:44.918994 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m11:26:44.920141 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca6f9b7f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca541c370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f5ca541c1c0>]}


============================== 2022-08-21 11:26:50.435708 | 41dacd60-072e-48fb-8c19-166a0cb758df ==============================
[0m11:26:50.435729 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:26:50.437328 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m11:26:50.438419 [debug] [MainThread]: Tracking: tracking
[0m11:26:50.441978 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5cff1c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5cff040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5cfff10>]}
[0m11:26:52.123127 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:26:52.123993 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:26:52.132245 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5a010d0>]}
[0m11:26:52.156181 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5b77850>]}
[0m11:26:52.157288 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:26:52.158434 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5b77700>]}
[0m11:26:52.161179 [info ] [MainThread]: 
[0m11:26:52.162923 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:26:52.165036 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m11:26:52.165894 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:26:52.995631 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:26:52.996808 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:26:53.677286 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5a4eb20>]}
[0m11:26:53.678932 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:26:53.680089 [info ] [MainThread]: 
[0m11:26:53.685915 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:26:53.686225 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:26:53.686457 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:26:53.686785 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:26:53.687878 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m11:26:53.688915 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m11:26:53.689796 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m11:26:53.690687 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m11:26:53.692379 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:26:53.693856 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:26:53.695330 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:26:53.696782 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:26:53.697438 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:26:53.698340 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:26:53.699122 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:26:53.699753 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:26:53.700480 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:26:53.701205 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:26:53.701891 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:26:53.702535 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:26:53.712079 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:26:53.716693 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:26:53.723292 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:26:53.728721 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:26:53.736145 [debug] [Thread-2  ]: finished collecting timing info
[0m11:26:53.737043 [debug] [Thread-4  ]: finished collecting timing info
[0m11:26:53.737551 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:26:53.737969 [debug] [Thread-1  ]: finished collecting timing info
[0m11:26:53.738704 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:26:53.738899 [debug] [Thread-3  ]: finished collecting timing info
[0m11:26:53.761259 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:26:53.774971 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:26:53.779417 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:26:53.781043 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:26:53.795180 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:26:53.801431 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:26:53.807291 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:26:53.807828 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:26:53.809971 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:26:53.851899 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m11:26:53.857712 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m11:26:53.860970 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m11:26:54.828559 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:26:54.837470 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m11:26:55.065683 [debug] [Thread-4  ]: finished collecting timing info
[0m11:26:55.067149 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b592bc70>]}
[0m11:26:55.068322 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.37s]
[0m11:26:55.071674 [debug] [Thread-3  ]: finished collecting timing info
[0m11:26:55.072275 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:26:55.073509 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b592bc10>]}
[0m11:26:55.075283 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:26:55.076009 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.38s]
[0m11:26:55.077315 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m11:26:55.078511 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:26:55.079806 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:26:55.081407 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:26:55.082050 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:26:55.087050 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:26:55.096026 [debug] [Thread-5  ]: finished collecting timing info
[0m11:26:55.097022 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:26:55.099531 [debug] [Thread-2  ]: finished collecting timing info
[0m11:26:55.102534 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:26:55.104173 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b594dfd0>]}
[0m11:26:55.106021 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.41s]
[0m11:26:55.107401 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:26:55.108895 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:26:55.109887 [info ] [Thread-4  ]: 6 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m11:26:55.111362 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:26:55.112215 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:26:55.112847 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:26:55.117325 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:26:55.127282 [debug] [Thread-4  ]: finished collecting timing info
[0m11:26:55.128260 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:26:55.134530 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:26:55.760348 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:26:55.769290 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:26:55.770894 [debug] [Thread-4  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m11:26:55.775263 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m11:26:57.867656 [debug] [Thread-1  ]: finished collecting timing info
[0m11:26:57.869263 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5974670>]}
[0m11:26:57.870489 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 4.18s]
[0m11:26:57.872004 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:26:58.667624 [debug] [Thread-4  ]: finished collecting timing info
[0m11:26:58.669259 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b4061490>]}
[0m11:26:58.670638 [info ] [Thread-4  ]: 6 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.56s]
[0m11:26:58.672144 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:26:58.767400 [debug] [Thread-5  ]: finished collecting timing info
[0m11:26:58.768871 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '41dacd60-072e-48fb-8c19-166a0cb758df', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b409be20>]}
[0m11:26:58.769995 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 3.69s]
[0m11:26:58.771373 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:26:58.774138 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:26:58.775806 [info ] [MainThread]: 
[0m11:26:58.777293 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 6.61 seconds (6.61s).
[0m11:26:58.778558 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:26:58.779558 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m11:26:58.780613 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m11:26:58.781598 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m11:26:58.782340 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:26:58.783115 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:26:58.804582 [info ] [MainThread]: 
[0m11:26:58.805657 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:26:58.806735 [info ] [MainThread]: 
[0m11:26:58.808000 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m11:26:58.809125 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5956dc0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b5a4ea00>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f66b405ddc0>]}


============================== 2022-08-21 11:27:03.808627 | 848e1cde-1122-4d37-8421-fcd58fb8ce62 ==============================
[0m11:27:03.808651 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:27:03.809873 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m11:27:03.810815 [debug] [MainThread]: Tracking: tracking
[0m11:27:03.814285 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e835160>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e835400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e835a30>]}
[0m11:27:05.984074 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:27:05.984921 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:27:05.994291 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '848e1cde-1122-4d37-8421-fcd58fb8ce62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e53e0a0>]}
[0m11:27:06.018961 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '848e1cde-1122-4d37-8421-fcd58fb8ce62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e6b47f0>]}
[0m11:27:06.019879 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:27:06.021150 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '848e1cde-1122-4d37-8421-fcd58fb8ce62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e6b46a0>]}
[0m11:27:06.023987 [info ] [MainThread]: 
[0m11:27:06.025932 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:27:06.028594 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:27:06.029611 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:27:06.577431 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '848e1cde-1122-4d37-8421-fcd58fb8ce62', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e641a30>]}
[0m11:27:06.579019 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:27:06.580220 [info ] [MainThread]: 
[0m11:27:06.585846 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:06.586098 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:06.586274 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:06.586630 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:06.587072 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:06.587350 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m11:27:06.588507 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m11:27:06.589501 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m11:27:06.590705 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m11:27:06.591568 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m11:27:06.592977 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:27:06.594344 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:27:06.595374 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:27:06.596620 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:27:06.597761 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:27:06.598682 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:06.599547 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:06.600180 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:06.601049 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:06.601737 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:06.602765 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:06.603604 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:06.604429 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:06.605080 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:06.605747 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:06.623469 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:27:06.628932 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:27:06.644883 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:27:06.650528 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:27:06.656186 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:27:06.663697 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:06.664948 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:06.665640 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:06.665880 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:06.666050 [debug] [Thread-1  ]: finished collecting timing info
[0m11:27:06.667197 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:06.667573 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:06.668321 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:06.689688 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:27:06.689997 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:06.693727 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:27:06.695012 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:06.699625 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:27:06.704841 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:27:06.709519 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:27:06.711296 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:27:06.713392 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:27:06.715168 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:27:06.715728 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:27:06.719469 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:27:06.761838 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m11:27:06.768076 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m11:27:06.768657 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m11:27:06.774153 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m11:27:06.779949 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m11:27:08.485772 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:08.487595 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.89s]
[0m11:27:08.488910 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:08.489987 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:08.491002 [info ] [Thread-4  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m11:27:08.492456 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:27:08.493436 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:08.494490 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:08.502773 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:27:08.514277 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:08.515227 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:08.521346 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:27:08.525887 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:08.527106 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.93s]
[0m11:27:08.528109 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:08.529049 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:08.529459 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:27:08.530051 [info ] [Thread-3  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m11:27:08.531987 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:27:08.532676 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:08.533513 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:08.539802 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:27:08.545828 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:08.547120 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.95s]
[0m11:27:08.548364 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:08.548661 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:08.549318 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:08.550052 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:08.550742 [info ] [Thread-2  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m11:27:08.554029 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:27:08.555333 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:27:08.556642 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:08.557336 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:08.562520 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:27:08.567902 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m11:27:08.571185 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:08.572005 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:08.575265 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:27:08.576226 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:08.585553 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m11:27:08.595911 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:08.597208 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 2.00s]
[0m11:27:08.598372 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:08.599294 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:08.600066 [info ] [Thread-5  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m11:27:08.601396 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:27:08.602301 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:08.603031 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:08.608439 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:27:08.611617 [debug] [Thread-3  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:08.617007 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:08.617934 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:08.622694 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:27:08.631794 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m11:27:08.633072 [debug] [Thread-2  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:08.678366 [debug] [Thread-5  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:08.735008 [debug] [Thread-1  ]: finished collecting timing info
[0m11:27:08.736582 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 2.14s]
[0m11:27:08.738304 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:10.366322 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:10.367897 [info ] [Thread-4  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.88s]
[0m11:27:10.369042 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:10.416783 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:10.418448 [info ] [Thread-3  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 1.89s]
[0m11:27:10.419877 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:10.495875 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:10.498870 [info ] [Thread-2  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.94s]
[0m11:27:10.501020 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:10.516913 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:10.518678 [info ] [Thread-5  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.92s]
[0m11:27:10.520281 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:10.526885 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:27:10.529876 [info ] [MainThread]: 
[0m11:27:10.532290 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.50 seconds (4.50s).
[0m11:27:10.534459 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:27:10.535525 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m11:27:10.536413 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m11:27:10.537413 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m11:27:10.538637 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m11:27:10.539740 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:27:10.570070 [info ] [MainThread]: 
[0m11:27:10.571795 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:27:10.573349 [info ] [MainThread]: 
[0m11:27:10.574696 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m11:27:10.576669 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e6b42e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e6b4130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fa75e835f40>]}


============================== 2022-08-21 11:27:16.146185 | a99d847c-d136-40de-bbc0-9faeeaa567d1 ==============================
[0m11:27:16.146212 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:27:16.148116 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m11:27:16.149116 [debug] [MainThread]: Tracking: tracking
[0m11:27:16.152873 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae933fae80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae933faac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae933fa220>]}
[0m11:27:17.914978 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:27:17.915908 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:27:17.925372 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a99d847c-d136-40de-bbc0-9faeeaa567d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae930fc0d0>]}
[0m11:27:17.948805 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a99d847c-d136-40de-bbc0-9faeeaa567d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae93271820>]}
[0m11:27:17.949870 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:27:17.950860 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a99d847c-d136-40de-bbc0-9faeeaa567d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae932716d0>]}
[0m11:27:17.953672 [info ] [MainThread]: 
[0m11:27:17.955470 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:27:17.958038 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:27:17.959106 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:27:18.587257 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a99d847c-d136-40de-bbc0-9faeeaa567d1', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae931fe9d0>]}
[0m11:27:18.589124 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:27:18.590247 [info ] [MainThread]: 
[0m11:27:18.596204 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:18.596538 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:18.596735 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:18.596958 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:18.597294 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:18.597529 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m11:27:18.598585 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m11:27:18.599469 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m11:27:18.600340 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m11:27:18.601201 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m11:27:18.602431 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:27:18.603871 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:27:18.605365 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:27:18.606729 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:27:18.607848 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:27:18.608522 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:18.609301 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:18.610099 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:18.611008 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:18.611940 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:18.612702 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:18.613576 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:18.614301 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:18.615012 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:18.615874 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:18.634098 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:27:18.644902 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:27:18.653034 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:27:18.659508 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:27:18.665266 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:27:18.673791 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:18.674308 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:18.675947 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:18.676448 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:18.676646 [debug] [Thread-1  ]: finished collecting timing info
[0m11:27:18.677544 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:18.678480 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:18.698900 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:27:18.699159 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:18.700128 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:18.704207 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:27:18.705582 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:18.710633 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:27:18.714425 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:27:18.716727 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:27:18.719428 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:27:18.721137 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:27:18.724049 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:27:18.725546 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:27:18.729108 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:27:18.766035 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m11:27:18.770740 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m11:27:18.776747 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m11:27:18.777411 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m11:27:18.781094 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m11:27:20.461767 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:20.463569 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.86s]
[0m11:27:20.464967 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:27:20.466126 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:20.467191 [info ] [Thread-2  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m11:27:20.468682 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:27:20.470028 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:20.471091 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:20.472190 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:20.482049 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:27:20.483357 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.88s]
[0m11:27:20.485590 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:27:20.486511 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:20.487375 [info ] [Thread-4  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m11:27:20.489289 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:27:20.490314 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:20.491146 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:20.492040 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:20.493636 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.89s]
[0m11:27:20.493911 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:20.499607 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:27:20.507630 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:27:20.508410 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:20.509662 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:20.515101 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:27:20.516444 [info ] [Thread-3  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m11:27:20.518936 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:27:20.519836 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:20.520849 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:20.521713 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:20.527254 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:27:20.528789 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:20.530013 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m11:27:20.533608 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:27:20.541744 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:20.542885 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:20.548208 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:27:20.548565 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:27:20.556358 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:20.557759 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 1.95s]
[0m11:27:20.559598 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:27:20.560156 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m11:27:20.561101 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:20.563431 [info ] [Thread-5  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m11:27:20.565347 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:27:20.566319 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:20.567213 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:20.573971 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:27:20.583963 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:20.584789 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:20.589172 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:27:20.590558 [debug] [Thread-2  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:20.600209 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m11:27:20.610780 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:20.614155 [debug] [Thread-3  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:20.619329 [debug] [Thread-1  ]: finished collecting timing info
[0m11:27:20.621481 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 2.02s]
[0m11:27:20.624697 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:27:20.649983 [debug] [Thread-5  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:27:22.256225 [debug] [Thread-2  ]: finished collecting timing info
[0m11:27:22.257956 [info ] [Thread-2  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.79s]
[0m11:27:22.259474 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:27:22.298697 [debug] [Thread-3  ]: finished collecting timing info
[0m11:27:22.300416 [info ] [Thread-3  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.78s]
[0m11:27:22.301467 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:27:22.467915 [debug] [Thread-5  ]: finished collecting timing info
[0m11:27:22.469534 [info ] [Thread-5  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.90s]
[0m11:27:22.470822 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:27:22.548093 [debug] [Thread-4  ]: finished collecting timing info
[0m11:27:22.549805 [info ] [Thread-4  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 2.06s]
[0m11:27:22.551398 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:27:22.555278 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:27:22.557192 [info ] [MainThread]: 
[0m11:27:22.559205 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.60 seconds (4.60s).
[0m11:27:22.560570 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:27:22.561984 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m11:27:22.563221 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m11:27:22.564413 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m11:27:22.565558 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m11:27:22.566715 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:27:22.588476 [info ] [MainThread]: 
[0m11:27:22.589742 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:27:22.590875 [info ] [MainThread]: 
[0m11:27:22.591816 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m11:27:22.592838 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae9316e8e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae9316e520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fae933fb700>]}


============================== 2022-08-21 11:40:27.953846 | c3a20f77-44c7-4d2e-ae97-17ea34b338a3 ==============================
[0m11:40:27.953870 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:40:27.955648 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m11:40:27.956850 [debug] [MainThread]: Tracking: tracking
[0m11:40:27.960523 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b9f3fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b9f3eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b9f3f40>]}
[0m11:40:29.701605 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:40:29.703432 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:40:29.713901 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b7970d0>]}
[0m11:40:29.739173 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b8de820>]}
[0m11:40:29.740711 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:40:29.742327 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b8de6d0>]}
[0m11:40:29.745688 [info ] [MainThread]: 
[0m11:40:29.747109 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:40:29.749973 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m11:40:29.751175 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:30.354600 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:40:30.355705 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:30.883574 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b8fec40>]}
[0m11:40:30.885197 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:40:30.886331 [info ] [MainThread]: 
[0m11:40:30.898471 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:40:30.898723 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:40:30.899166 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:40:30.899474 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:40:30.900245 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m11:40:30.901353 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m11:40:30.902504 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m11:40:30.903663 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m11:40:30.905456 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:40:30.906790 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:40:30.908280 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:40:30.909586 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:40:30.910628 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:40:30.911587 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:40:30.912630 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:40:30.913446 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:40:30.914413 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:40:30.915464 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:40:30.916589 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:40:30.917453 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:40:30.926379 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:40:30.930430 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:40:30.936059 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:40:30.941474 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:40:30.948858 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:30.950016 [debug] [Thread-3  ]: finished collecting timing info
[0m11:40:30.950307 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:40:30.950484 [debug] [Thread-1  ]: finished collecting timing info
[0m11:40:30.951445 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:40:30.952234 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:30.974331 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:40:30.989458 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:40:30.993853 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:40:30.994931 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:40:31.008946 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:31.015215 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:40:31.021522 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:40:31.022197 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:40:31.024265 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:40:31.065369 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m11:40:31.072840 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m11:40:31.076451 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m11:40:31.510438 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:40:31.521097 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m11:40:32.272210 [debug] [Thread-3  ]: finished collecting timing info
[0m11:40:32.274522 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b803ca0>]}
[0m11:40:32.275821 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.37s]
[0m11:40:32.277168 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:40:32.287014 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:32.288771 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b803d60>]}
[0m11:40:32.290325 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.38s]
[0m11:40:32.291612 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:40:32.293279 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:40:32.294491 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m11:40:32.295961 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:40:32.296852 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:40:32.297839 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:40:32.303650 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:40:32.313258 [debug] [Thread-5  ]: finished collecting timing info
[0m11:40:32.314188 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:40:32.317474 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:40:32.335251 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:32.336894 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b803f10>]}
[0m11:40:32.337786 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.43s]
[0m11:40:32.338829 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:40:32.340223 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:40:32.341138 [info ] [Thread-2  ]: 6 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m11:40:32.342471 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:40:32.343180 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:40:32.343905 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:40:32.349344 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:40:32.357100 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:32.357979 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:40:32.363674 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m11:40:32.823556 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:40:32.833236 [debug] [Thread-5  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m11:40:32.933560 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:40:32.944525 [debug] [Thread-2  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m11:40:34.455946 [debug] [Thread-1  ]: finished collecting timing info
[0m11:40:34.457354 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b7e8f40>]}
[0m11:40:34.458302 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.55s]
[0m11:40:34.459294 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:40:35.454708 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:35.457115 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2485979a0>]}
[0m11:40:35.458190 [info ] [Thread-2  ]: 6 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 3.11s]
[0m11:40:35.459297 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:40:35.743541 [debug] [Thread-5  ]: finished collecting timing info
[0m11:40:35.745085 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'c3a20f77-44c7-4d2e-ae97-17ea34b338a3', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b7e89a0>]}
[0m11:40:35.746280 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.45s]
[0m11:40:35.747404 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:40:35.750319 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:40:35.751853 [info ] [MainThread]: 
[0m11:40:35.752672 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 6.01 seconds (6.01s).
[0m11:40:35.753749 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:40:35.754560 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m11:40:35.755346 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:40:35.756132 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m11:40:35.756962 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_payments' was properly closed.
[0m11:40:35.757615 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:40:35.777045 [info ] [MainThread]: 
[0m11:40:35.777984 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:40:35.779140 [info ] [MainThread]: 
[0m11:40:35.780118 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m11:40:35.781077 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b9f0d90>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe24b8febb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fe2485978b0>]}


============================== 2022-08-21 11:40:41.067044 | 6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b ==============================
[0m11:40:41.067067 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:40:41.068528 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m11:40:41.069655 [debug] [MainThread]: Tracking: tracking
[0m11:40:41.073653 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe594316d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe59431cd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe59431100>]}
[0m11:40:42.769811 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:40:42.770729 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:40:42.778792 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe591320d0>]}
[0m11:40:42.800772 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe592a77c0>]}
[0m11:40:42.801978 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:40:42.803175 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe592a7670>]}
[0m11:40:42.805933 [info ] [MainThread]: 
[0m11:40:42.807610 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:40:42.809578 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m11:40:42.810486 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:43.324434 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:40:43.325554 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:40:43.824119 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe59234af0>]}
[0m11:40:43.825581 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:40:43.826465 [info ] [MainThread]: 
[0m11:40:43.831730 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:40:43.831992 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:40:43.832455 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:40:43.832865 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:40:43.833666 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m11:40:43.834585 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m11:40:43.835413 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m11:40:43.836279 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m11:40:43.837770 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:40:43.839391 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:40:43.840933 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:40:43.842308 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:40:43.843009 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:40:43.843896 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:40:43.844552 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:40:43.845258 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:40:43.845874 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:40:43.846491 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:40:43.847089 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:40:43.847679 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:40:43.857504 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:40:43.861992 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:40:43.867123 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:40:43.872738 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:40:43.880544 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:43.880964 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:43.881354 [debug] [Thread-1  ]: finished collecting timing info
[0m11:40:43.882228 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:40:43.883300 [debug] [Thread-3  ]: finished collecting timing info
[0m11:40:43.883563 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:40:43.884412 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:40:43.906869 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:40:43.928224 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:40:43.932104 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:40:43.946405 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:43.950949 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:40:43.960071 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:40:43.960444 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:40:43.962637 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:40:44.003645 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m11:40:44.004849 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m11:40:44.008008 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m11:40:44.417823 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:40:44.427154 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m11:40:45.234962 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:45.236779 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe5905bb50>]}
[0m11:40:45.238267 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.39s]
[0m11:40:45.239876 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:40:45.241543 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:40:45.242574 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m11:40:45.244264 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:40:45.245238 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:40:45.246028 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:40:45.251372 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:40:45.262580 [debug] [Thread-5  ]: finished collecting timing info
[0m11:40:45.263459 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:40:45.267098 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:40:45.404237 [debug] [Thread-3  ]: finished collecting timing info
[0m11:40:45.405849 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe5905baf0>]}
[0m11:40:45.407167 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.57s]
[0m11:40:45.408325 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:40:45.516623 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:45.519574 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe5907edf0>]}
[0m11:40:45.521602 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.68s]
[0m11:40:45.523549 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:40:45.525661 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:40:45.527430 [info ] [Thread-4  ]: 6 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m11:40:45.529901 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:40:45.531592 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:40:45.532896 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:40:45.540805 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:40:45.554132 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:45.555277 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:40:45.562724 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:40:45.748628 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:40:45.757792 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m11:40:46.083672 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:40:46.092372 [debug] [Thread-4  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m11:40:47.277425 [debug] [Thread-1  ]: finished collecting timing info
[0m11:40:47.279444 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe50fbd550>]}
[0m11:40:47.280687 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.44s]
[0m11:40:47.281960 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:40:48.523585 [debug] [Thread-5  ]: finished collecting timing info
[0m11:40:48.524884 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe59053df0>]}
[0m11:40:48.525779 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 3.28s]
[0m11:40:48.526709 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:40:48.995270 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:48.996910 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '6c4b1c43-b0a0-4ea7-9f71-a534ea66d75b', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe50f41670>]}
[0m11:40:48.998067 [info ] [Thread-4  ]: 6 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.47s]
[0m11:40:48.999174 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:40:49.002386 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:40:49.004152 [info ] [MainThread]: 
[0m11:40:49.005443 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 6.20 seconds (6.20s).
[0m11:40:49.006485 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:40:49.007426 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m11:40:49.008278 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m11:40:49.009143 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m11:40:49.009901 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:40:49.010649 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:40:49.031328 [info ] [MainThread]: 
[0m11:40:49.032491 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:40:49.033958 [info ] [MainThread]: 
[0m11:40:49.035395 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m11:40:49.036573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe592a7370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe592a76a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7efe50f63ac0>]}


============================== 2022-08-21 11:40:54.066231 | ac23221a-5e1b-457b-aa2e-87ab0fa9742e ==============================
[0m11:40:54.066259 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:40:54.067844 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m11:40:54.069003 [debug] [MainThread]: Tracking: tracking
[0m11:40:54.073469 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336fc65af0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336fc65250>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336fc65b20>]}
[0m11:40:55.779044 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:40:55.779900 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:40:55.788256 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'ac23221a-5e1b-457b-aa2e-87ab0fa9742e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336f9650d0>]}
[0m11:40:55.809499 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'ac23221a-5e1b-457b-aa2e-87ab0fa9742e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336fadb760>]}
[0m11:40:55.810483 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:40:55.811297 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac23221a-5e1b-457b-aa2e-87ab0fa9742e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336fadb610>]}
[0m11:40:55.813711 [info ] [MainThread]: 
[0m11:40:55.815099 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:40:55.817330 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:40:55.818332 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:40:56.305101 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'ac23221a-5e1b-457b-aa2e-87ab0fa9742e', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336fa68940>]}
[0m11:40:56.306853 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:40:56.308221 [info ] [MainThread]: 
[0m11:40:56.313895 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:40:56.314147 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:40:56.314334 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:40:56.314557 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:40:56.314846 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:40:56.315515 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m11:40:56.316714 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m11:40:56.317882 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m11:40:56.319056 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m11:40:56.320226 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m11:40:56.321973 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:40:56.324046 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:40:56.325231 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:40:56.326758 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:40:56.327824 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:40:56.328788 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:40:56.329612 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:40:56.330528 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:40:56.331376 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:40:56.332080 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:40:56.333003 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:40:56.333943 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:40:56.334837 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:40:56.335571 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:40:56.336267 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:40:56.355872 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:40:56.366105 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:40:56.374271 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:40:56.379814 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:40:56.385265 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:40:56.393731 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:56.394126 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:56.395181 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:40:56.395413 [debug] [Thread-1  ]: finished collecting timing info
[0m11:40:56.396207 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:40:56.396417 [debug] [Thread-3  ]: finished collecting timing info
[0m11:40:56.407623 [debug] [Thread-5  ]: finished collecting timing info
[0m11:40:56.418412 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:40:56.419070 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:40:56.424173 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:40:56.425441 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:40:56.426694 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:40:56.431321 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:40:56.436005 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:40:56.440206 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:40:56.440634 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:40:56.442361 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:40:56.445219 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:56.446460 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:40:56.449146 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:40:56.490349 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m11:40:56.490649 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m11:40:56.492770 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m11:40:56.493055 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m11:40:56.495099 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m11:40:58.273196 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:58.274839 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.95s]
[0m11:40:58.276180 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:40:58.277319 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:40:58.278300 [info ] [Thread-4  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m11:40:58.279729 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:40:58.280627 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:40:58.281852 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:40:58.291143 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:40:58.291851 [debug] [Thread-1  ]: finished collecting timing info
[0m11:40:58.294129 [debug] [Thread-3  ]: finished collecting timing info
[0m11:40:58.294788 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 1.97s]
[0m11:40:58.296082 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.97s]
[0m11:40:58.297480 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:40:58.299046 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:40:58.300079 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:40:58.301027 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:40:58.301913 [info ] [Thread-1  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m11:40:58.302687 [debug] [Thread-4  ]: finished collecting timing info
[0m11:40:58.302924 [info ] [Thread-3  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m11:40:58.304428 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:40:58.305091 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:40:58.306793 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:40:58.307438 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:40:58.311829 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:40:58.318253 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:58.317516 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:40:58.319270 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:40:58.320708 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:40:58.323102 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 2.00s]
[0m11:40:58.329420 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:40:58.334799 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:40:58.336492 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:40:58.337562 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:40:58.340010 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:40:58.342405 [info ] [Thread-2  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m11:40:58.343431 [debug] [Thread-5  ]: finished collecting timing info
[0m11:40:58.344941 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:40:58.345257 [debug] [Thread-1  ]: finished collecting timing info
[0m11:40:58.346507 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 2.02s]
[0m11:40:58.346813 [debug] [Thread-3  ]: finished collecting timing info
[0m11:40:58.347474 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:40:58.348215 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:40:58.349399 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:40:58.350053 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:40:58.350837 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:40:58.354671 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:40:58.359123 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:40:58.364427 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:40:58.372904 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:40:58.374527 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m11:40:58.377660 [debug] [Thread-2  ]: finished collecting timing info
[0m11:40:58.378712 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:40:58.383366 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:40:58.386677 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:40:58.394244 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m11:40:58.417101 [debug] [Thread-1  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:40:58.418241 [debug] [Thread-3  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:40:58.443747 [debug] [Thread-2  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:41:00.010017 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:00.011899 [info ] [Thread-4  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.73s]
[0m11:41:00.013217 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:00.264332 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:00.266099 [info ] [Thread-3  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.96s]
[0m11:41:00.267777 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:00.342725 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:00.344596 [info ] [Thread-1  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 2.04s]
[0m11:41:00.345863 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:00.352099 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:00.353846 [info ] [Thread-2  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 2.01s]
[0m11:41:00.355242 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:00.358357 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:41:00.359775 [info ] [MainThread]: 
[0m11:41:00.360890 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.55 seconds (4.55s).
[0m11:41:00.361936 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:41:00.362789 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m11:41:00.363686 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:41:00.364475 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m11:41:00.365216 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m11:41:00.366059 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13' was properly closed.
[0m11:41:00.386686 [info ] [MainThread]: 
[0m11:41:00.387837 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:41:00.388853 [info ] [MainThread]: 
[0m11:41:00.390001 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m11:41:00.390952 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336fadb0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f336f9d9940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f334f7e25b0>]}


============================== 2022-08-21 11:41:10.971870 | f9144147-31db-4f14-bfbd-76ae54701d17 ==============================
[0m11:41:10.971906 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:41:10.973459 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m11:41:10.974543 [debug] [MainThread]: Tracking: tracking
[0m11:41:10.979373 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5dce0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5dceeb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5dce2e0>]}
[0m11:41:12.693966 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:41:12.694978 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:41:12.703567 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'f9144147-31db-4f14-bfbd-76ae54701d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5acf0d0>]}
[0m11:41:12.727618 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'f9144147-31db-4f14-bfbd-76ae54701d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5c45730>]}
[0m11:41:12.728586 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:41:12.729502 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9144147-31db-4f14-bfbd-76ae54701d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5c455e0>]}
[0m11:41:12.732013 [info ] [MainThread]: 
[0m11:41:12.733825 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:41:12.736385 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:41:12.737654 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:13.302672 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'f9144147-31db-4f14-bfbd-76ae54701d17', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5bd6820>]}
[0m11:41:13.304381 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:41:13.305557 [info ] [MainThread]: 
[0m11:41:13.311269 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:13.311578 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:13.311813 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:13.312109 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:13.312288 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:13.312845 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m11:41:13.313884 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m11:41:13.314679 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m11:41:13.315587 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m11:41:13.316856 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m11:41:13.318567 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:41:13.319836 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:41:13.321206 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:41:13.322553 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:41:13.324037 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:41:13.324672 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:13.325570 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:13.326356 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:13.327056 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:13.327804 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:13.328499 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:13.329275 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:13.330009 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:13.330669 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:13.331386 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:13.350935 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:41:13.367265 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:41:13.369464 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:41:13.375961 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:41:13.381558 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:41:13.389791 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:13.390499 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:13.391355 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:13.391722 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:13.392154 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:13.392460 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:13.398587 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:13.414803 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:41:13.415082 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:13.416237 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:13.421229 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:41:13.422811 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:13.427972 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:41:13.431458 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:41:13.436306 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:41:13.436692 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:13.438215 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:41:13.441058 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:41:13.442718 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:41:13.444972 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:41:13.485930 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m11:41:13.486957 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m11:41:13.491620 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m11:41:13.502035 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m11:41:13.503246 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m11:41:15.154135 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:15.155882 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.83s]
[0m11:41:15.157436 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:15.158537 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:15.159615 [info ] [Thread-4  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m11:41:15.161047 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:41:15.162143 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:15.162880 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:15.171703 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:41:15.181931 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:15.182836 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:15.191117 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:41:15.199448 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:41:15.212486 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:15.213936 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 1.89s]
[0m11:41:15.215095 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:15.215982 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:15.217103 [info ] [Thread-5  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m11:41:15.218557 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:41:15.219208 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:15.219832 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:15.225449 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:41:15.234523 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:15.235323 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:15.238925 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:41:15.242172 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:41:15.249249 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m11:41:15.272682 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:15.274534 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.95s]
[0m11:41:15.275839 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:15.276845 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:15.277691 [info ] [Thread-3  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m11:41:15.279057 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:41:15.279834 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:15.280720 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:15.287912 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:41:15.294497 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:41:15.297589 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:15.298450 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:15.302315 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:41:15.303213 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:15.306108 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.99s]
[0m11:41:15.308028 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:15.309385 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:15.310740 [info ] [Thread-2  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m11:41:15.312693 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:41:15.312899 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m11:41:15.313409 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:15.314753 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:15.320604 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:41:15.327204 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:15.327916 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:15.332155 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:41:15.339283 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m11:41:15.363073 [debug] [Thread-3  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:41:15.376756 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:15.378046 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 2.06s]
[0m11:41:15.379140 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:15.383549 [debug] [Thread-2  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:41:16.932752 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:16.934989 [info ] [Thread-4  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.77s]
[0m11:41:16.936450 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:17.142175 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:17.143695 [info ] [Thread-5  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 1.93s]
[0m11:41:17.144886 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:17.211694 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:17.213094 [info ] [Thread-3  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.93s]
[0m11:41:17.214038 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:17.287801 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:17.289612 [info ] [Thread-2  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.98s]
[0m11:41:17.291134 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:17.293745 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:41:17.294889 [info ] [MainThread]: 
[0m11:41:17.295662 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.56 seconds (4.56s).
[0m11:41:17.296497 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:41:17.297222 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m11:41:17.297895 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:41:17.298557 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m11:41:17.299239 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m11:41:17.300214 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m11:41:17.322987 [info ] [MainThread]: 
[0m11:41:17.324227 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:41:17.325230 [info ] [MainThread]: 
[0m11:41:17.326021 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m11:41:17.327020 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5c452e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b5c45070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f71b41aa8e0>]}


============================== 2022-08-21 11:41:22.404237 | 9656c24b-8334-4717-862c-b8fc7074b10d ==============================
[0m11:41:22.404266 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:41:22.405650 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m11:41:22.407008 [debug] [MainThread]: Tracking: tracking
[0m11:41:22.410605 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fe20f0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fe20f1f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fe20f430>]}
[0m11:41:24.131445 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:41:24.132268 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:41:24.141627 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '9656c24b-8334-4717-862c-b8fc7074b10d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fdf100d0>]}
[0m11:41:24.163738 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '9656c24b-8334-4717-862c-b8fc7074b10d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fe086760>]}
[0m11:41:24.164907 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:41:24.166053 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9656c24b-8334-4717-862c-b8fc7074b10d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fe086610>]}
[0m11:41:24.169402 [info ] [MainThread]: 
[0m11:41:24.171210 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:41:24.173721 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:41:24.174948 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:24.672997 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '9656c24b-8334-4717-862c-b8fc7074b10d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fe0865b0>]}
[0m11:41:24.674768 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:41:24.675777 [info ] [MainThread]: 
[0m11:41:24.681341 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:41:24.681595 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:41:24.681878 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:41:24.682163 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:41:24.682337 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:24.683436 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:41:24.685107 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:41:24.686418 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:41:24.687940 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:41:24.689250 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:41:24.690371 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:41:24.691317 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:41:24.692380 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:41:24.693301 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:41:24.694165 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:24.695090 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:41:24.695807 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:41:24.696504 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:41:24.697235 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:41:24.697979 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:24.706753 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:41:24.711501 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:41:24.715886 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:41:24.722713 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:41:24.741024 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:41:24.749516 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:24.749948 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:24.751488 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:41:24.752201 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:24.753084 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:24.753366 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:41:24.754935 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:24.755422 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:24.756398 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:41:24.757850 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:41:24.759055 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:24.760932 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:41:24.762077 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:24.763160 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:24.764161 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:24.765771 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:41:24.767142 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:24.768613 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:24.770381 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:41:24.772501 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:41:24.773835 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:24.775510 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:41:24.777089 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:24.777971 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:24.778930 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:24.780350 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:41:24.781392 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:24.782364 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:24.783821 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:41:24.786176 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:41:24.788124 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:24.789271 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:24.791032 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:41:24.792049 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:24.792968 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:24.793957 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:24.800550 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:41:24.801666 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:24.802899 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:24.804290 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:24.814875 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:41:24.817023 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:24.825372 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:41:24.831103 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:41:24.832948 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:24.838657 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:41:24.840175 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:24.841877 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:24.843739 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:24.845017 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:24.846357 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:24.846737 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:24.846997 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:24.848184 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:24.848421 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:24.848976 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:24.850288 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:24.851377 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:24.852330 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:41:24.853180 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:24.853942 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:24.854934 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:24.855762 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:24.856938 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:41:24.857930 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:24.859418 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:24.860532 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:41:24.861759 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:24.862491 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:41:24.863631 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:24.864377 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:24.865133 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:24.865880 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:24.866820 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:41:24.867644 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:41:24.868955 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:41:24.869748 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:24.870890 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:41:24.875959 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:41:24.877237 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:41:24.877978 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:24.891650 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:41:24.892476 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:24.894796 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:41:24.896181 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:24.898251 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:24.899172 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:41:24.905918 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:24.906941 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:41:24.907969 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:24.914567 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:41:24.920059 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:41:24.921741 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:41:24.924530 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:24.928006 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:24.929105 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:24.931142 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:41:24.931392 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:24.931566 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:24.932638 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:24.932965 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:24.935052 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:24.935955 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:24.937993 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:41:24.939083 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:24.940014 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:24.941116 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:24.942509 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:24.943812 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:24.945125 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:41:24.948670 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:41:24.949384 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64' was properly closed.
[0m11:41:24.950250 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:41:24.951346 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m11:41:24.952250 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:41:24.952926 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:41:24.973090 [info ] [MainThread]: Done.
[0m11:41:25.040626 [debug] [MainThread]: Acquiring new bigquery connection "generate_catalog"
[0m11:41:25.041808 [info ] [MainThread]: Building catalog
[0m11:41:25.043834 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:41:25.902373 [debug] [ThreadPool]: Acquiring new bigquery connection "airflow-docker-352518.information_schema"
[0m11:41:25.903224 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m11:41:25.903987 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m11:41:25.921811 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:25.925313 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:25.928629 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:25.972425 [debug] [ThreadPool]: On airflow-docker-352518.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "airflow-docker-352518.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `airflow-docker-352518`.`dbt_x_airflow`.__TABLES__
        where (upper(dataset_id) = upper('dbt_x_airflow'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:41:25.973089 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`stripe`.__TABLES__
        where (upper(dataset_id) = upper('stripe'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:41:25.977384 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`jaffle_shop`.__TABLES__
        where (upper(dataset_id) = upper('jaffle_shop'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:41:29.255126 [info ] [MainThread]: Catalog written to /airflow-dbt/dbt_jaffleshop/target/catalog.json
[0m11:41:29.256711 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fe20f0a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fde76130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f02fde76d60>]}
[0m11:41:29.709094 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m11:41:29.710534 [debug] [MainThread]: Connection 'airflow-docker-352518.information_schema' was properly closed.
[0m11:41:29.711656 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.
[0m11:41:29.712680 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.


============================== 2022-08-21 11:41:34.514916 | 677d0f13-1def-48ca-a5fc-4e5062a850f9 ==============================
[0m11:41:34.514939 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:41:34.516075 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m11:41:34.516930 [debug] [MainThread]: Tracking: tracking
[0m11:41:34.520976 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79aa1e5190>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79aa1e53d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79aa1e5cd0>]}
[0m11:41:36.256057 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:41:36.256983 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:41:36.264750 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a9ee70d0>]}
[0m11:41:36.285340 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79aa05c7c0>]}
[0m11:41:36.286768 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:41:36.287970 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79aa05c670>]}
[0m11:41:36.290854 [info ] [MainThread]: 
[0m11:41:36.292301 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:41:36.294341 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m11:41:36.295284 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:36.853930 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:41:36.855071 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m11:41:37.403083 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a9fedaf0>]}
[0m11:41:37.404922 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:41:37.406294 [info ] [MainThread]: 
[0m11:41:37.411765 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:41:37.412005 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:41:37.412230 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:41:37.412511 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:41:37.413270 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m11:41:37.414386 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m11:41:37.415437 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m11:41:37.416353 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m11:41:37.418020 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:41:37.420118 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:41:37.421778 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:41:37.423418 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:41:37.424125 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:41:37.425004 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:41:37.425964 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:41:37.426947 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:41:37.427812 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:41:37.428594 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:41:37.429543 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:41:37.430448 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:41:37.439299 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:41:37.443844 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:41:37.448958 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:41:37.454172 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:41:37.461500 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:37.462300 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:37.462596 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:41:37.462778 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:37.463451 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:41:37.463753 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:37.485906 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:41:37.499540 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:41:37.504308 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:41:37.505644 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:41:37.519406 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:41:37.525989 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:41:37.531071 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:41:37.531345 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:41:37.532933 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:41:37.574975 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m11:41:37.575262 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m11:41:37.591451 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m11:41:38.026484 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:41:38.038372 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m11:41:38.692185 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:38.693752 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a9e11c40>]}
[0m11:41:38.694850 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.27s]
[0m11:41:38.696052 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:41:38.713665 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:38.715090 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a9e11ca0>]}
[0m11:41:38.716086 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.29s]
[0m11:41:38.717138 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:41:38.719155 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:41:38.720403 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m11:41:38.722456 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:41:38.723266 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:41:38.723892 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:41:38.728846 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:41:38.735829 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:38.736927 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:41:38.740586 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:41:38.814441 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:38.816056 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a9e33f70>]}
[0m11:41:38.817302 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.40s]
[0m11:41:38.819123 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:41:38.820793 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:41:38.822230 [info ] [Thread-4  ]: 6 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m11:41:38.823778 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:41:38.824568 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:41:38.825335 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:41:38.829645 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:41:38.839019 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:38.840014 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:41:38.845683 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:41:39.213254 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:41:39.222376 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m11:41:39.343149 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:41:39.352901 [debug] [Thread-4  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m11:41:41.054341 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:41.055893 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a9e596a0>]}
[0m11:41:41.057089 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.64s]
[0m11:41:41.058355 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:41:41.663512 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:41.665145 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a852b910>]}
[0m11:41:41.666480 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 2.94s]
[0m11:41:41.668018 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:41:42.097235 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:42.099141 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '677d0f13-1def-48ca-a5fc-4e5062a850f9', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a857c880>]}
[0m11:41:42.100209 [info ] [Thread-4  ]: 6 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.28s]
[0m11:41:42.101442 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:41:42.104218 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:41:42.105368 [info ] [MainThread]: 
[0m11:41:42.106164 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 5.81 seconds (5.81s).
[0m11:41:42.106901 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:41:42.107544 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m11:41:42.108192 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m11:41:42.108763 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m11:41:42.109328 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:41:42.109884 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:41:42.130205 [info ] [MainThread]: 
[0m11:41:42.131402 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:41:42.132641 [info ] [MainThread]: 
[0m11:41:42.133639 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m11:41:42.134903 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a9f33d30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79aa05c670>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f79a85de100>]}


============================== 2022-08-21 11:41:47.159247 | 44333bac-50d2-40d5-bcb4-43c103b1ff35 ==============================
[0m11:41:47.159270 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:41:47.160717 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m11:41:47.161731 [debug] [MainThread]: Tracking: tracking
[0m11:41:47.165554 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441072c760>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441072ccd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f441072c3d0>]}
[0m11:41:48.745804 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:41:48.746480 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:41:48.754379 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '44333bac-50d2-40d5-bcb4-43c103b1ff35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f44104ac0a0>]}
[0m11:41:48.772863 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '44333bac-50d2-40d5-bcb4-43c103b1ff35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f44105a37f0>]}
[0m11:41:48.773712 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:41:48.774631 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '44333bac-50d2-40d5-bcb4-43c103b1ff35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f44105a36a0>]}
[0m11:41:48.777155 [info ] [MainThread]: 
[0m11:41:48.778439 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:41:48.780356 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:41:48.781446 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:49.263001 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '44333bac-50d2-40d5-bcb4-43c103b1ff35', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4410530a30>]}
[0m11:41:49.264218 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:41:49.264999 [info ] [MainThread]: 
[0m11:41:49.269548 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:41:49.269795 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:41:49.270025 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:41:49.270256 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:41:49.270829 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:41:49.271039 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:49.272381 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:41:49.273655 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:41:49.274876 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:41:49.275795 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:41:49.277104 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:41:49.278119 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:41:49.279074 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:41:49.280086 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:41:49.281051 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:41:49.282062 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:49.282946 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:41:49.283683 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:41:49.284677 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:41:49.287958 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:49.292361 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:41:49.296196 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:41:49.300327 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:41:49.305749 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:41:49.321141 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:41:49.328790 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:49.329056 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:49.329840 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:49.330189 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:41:49.330412 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:49.331062 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:41:49.331612 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:49.332014 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:41:49.332749 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:49.333471 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:41:49.334204 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:49.335140 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:49.335791 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:49.337050 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:41:49.337808 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:49.338957 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:41:49.339722 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:49.340773 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:41:49.341566 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:49.342876 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:41:49.343669 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:49.344779 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:41:49.345478 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:49.346613 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:41:49.347560 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:49.348657 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:41:49.349606 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:49.350559 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:41:49.351471 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:49.352475 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:41:49.353377 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:49.354602 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:41:49.355272 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:49.355948 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:49.356810 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:49.357504 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:49.358298 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:49.358901 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:49.364147 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:41:49.365039 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:49.373947 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:41:49.374902 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:49.380095 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:41:49.386645 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:41:49.393017 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:41:49.394594 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:49.396462 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:49.397592 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:49.397796 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:49.398336 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:49.399223 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:49.400042 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:49.400301 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:49.400813 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:49.401858 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:49.402640 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:49.403823 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:41:49.404580 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:49.405254 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:49.405950 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:49.407144 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:41:49.407949 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:41:49.408841 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:49.409889 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:41:49.410959 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:41:49.411773 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:49.412800 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:41:49.413913 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:41:49.414589 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:49.415533 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:49.416608 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:41:49.417272 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:41:49.418303 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:41:49.419244 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:41:49.420436 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:41:49.421192 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:49.421886 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:41:49.423074 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:41:49.423687 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:49.424548 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:49.425269 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:49.429776 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:41:49.430943 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:41:49.431803 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:49.432682 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:49.445530 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:41:49.447249 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:41:49.455553 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:41:49.460516 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:41:49.461938 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:49.466581 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:41:49.469228 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:41:49.471727 [debug] [Thread-2  ]: finished collecting timing info
[0m11:41:49.472053 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:49.472562 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:49.473774 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:41:49.474011 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:49.474763 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:49.475549 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:49.475965 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:49.476917 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:49.477666 [debug] [Thread-1  ]: finished collecting timing info
[0m11:41:49.478365 [debug] [Thread-4  ]: finished collecting timing info
[0m11:41:49.479089 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:41:49.479716 [debug] [Thread-3  ]: finished collecting timing info
[0m11:41:49.480957 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:41:49.482108 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:41:49.482826 [debug] [Thread-5  ]: finished collecting timing info
[0m11:41:49.483888 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:41:49.486824 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:41:49.489367 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:41:49.490033 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m11:41:49.490637 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:41:49.491190 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64' was properly closed.
[0m11:41:49.491782 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:41:49.492347 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:41:49.509179 [info ] [MainThread]: Done.
[0m11:41:49.560132 [debug] [MainThread]: Acquiring new bigquery connection "generate_catalog"
[0m11:41:49.561096 [info ] [MainThread]: Building catalog
[0m11:41:49.563331 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:41:50.405519 [debug] [ThreadPool]: Acquiring new bigquery connection "airflow-docker-352518.information_schema"
[0m11:41:50.406511 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m11:41:50.407333 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m11:41:50.423369 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:50.426123 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:50.429404 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:41:50.473364 [debug] [ThreadPool]: On airflow-docker-352518.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "airflow-docker-352518.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `airflow-docker-352518`.`dbt_x_airflow`.__TABLES__
        where (upper(dataset_id) = upper('dbt_x_airflow'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:41:50.475352 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`jaffle_shop`.__TABLES__
        where (upper(dataset_id) = upper('jaffle_shop'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:41:50.478064 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`stripe`.__TABLES__
        where (upper(dataset_id) = upper('stripe'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:41:53.564004 [info ] [MainThread]: Catalog written to /airflow-dbt/dbt_jaffleshop/target/catalog.json
[0m11:41:53.565136 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f4410498910>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f44107e8e20>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f44107e8c70>]}
[0m11:41:54.090720 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m11:41:54.092251 [debug] [MainThread]: Connection 'airflow-docker-352518.information_schema' was properly closed.
[0m11:41:54.093005 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.
[0m11:41:54.093759 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.


============================== 2022-08-21 11:41:58.766221 | 43234a06-46f0-46af-87c4-ae46818f2e8f ==============================
[0m11:41:58.766244 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:41:58.767438 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m11:41:58.768584 [debug] [MainThread]: Tracking: tracking
[0m11:41:58.772058 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2dd7fa30>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2dd7f700>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2dd7fbe0>]}
[0m11:42:00.309108 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:42:00.310128 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:42:00.318051 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '43234a06-46f0-46af-87c4-ae46818f2e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2da800d0>]}
[0m11:42:00.340044 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '43234a06-46f0-46af-87c4-ae46818f2e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2dbf37f0>]}
[0m11:42:00.341305 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:42:00.342430 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '43234a06-46f0-46af-87c4-ae46818f2e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2dbf36a0>]}
[0m11:42:00.345049 [info ] [MainThread]: 
[0m11:42:00.346877 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:42:00.349995 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:42:00.351217 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:00.902967 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '43234a06-46f0-46af-87c4-ae46818f2e8f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2db84a30>]}
[0m11:42:00.904842 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:42:00.906083 [info ] [MainThread]: 
[0m11:42:00.911463 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:00.911699 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:00.911942 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:00.912187 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:00.912417 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:00.912852 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m11:42:00.913640 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m11:42:00.914426 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m11:42:00.915233 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m11:42:00.916059 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m11:42:00.917479 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:42:00.918622 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:42:00.919870 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:42:00.921150 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:42:00.922452 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:42:00.923166 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:00.923877 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:00.924569 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:00.925217 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:00.926027 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:00.926867 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:00.927612 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:00.928341 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:00.929006 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:00.929861 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:00.947154 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:42:00.952690 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:42:00.965900 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:42:00.970540 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:42:00.977574 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:42:00.984368 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:00.985799 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:00.986135 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:00.992368 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:00.999501 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:00.999759 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:01.005428 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:01.008512 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:42:01.009764 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:01.010881 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:01.012100 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:01.015615 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:42:01.021746 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:42:01.026070 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:42:01.029900 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:42:01.030266 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m11:42:01.037348 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m11:42:01.038477 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m11:42:01.039533 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m11:42:01.041243 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m11:42:01.078085 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m11:42:01.081658 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m11:42:01.084046 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m11:42:01.086193 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m11:42:01.087048 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m11:42:02.683812 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:02.686138 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.77s]
[0m11:42:02.687290 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:02.688210 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:02.688999 [info ] [Thread-3  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m11:42:02.690284 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:42:02.691047 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:02.694359 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:02.702356 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:42:02.711278 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:02.712139 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:02.717715 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:42:02.724933 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m11:42:02.731498 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:02.732792 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 1.81s]
[0m11:42:02.733753 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:02.734573 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:02.735323 [info ] [Thread-5  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m11:42:02.736470 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:42:02.737144 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:02.737772 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:02.743710 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:42:02.750889 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:02.751797 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:02.755071 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:42:02.755706 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:02.757426 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.84s]
[0m11:42:02.758523 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:02.759399 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:02.760304 [info ] [Thread-4  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m11:42:02.761592 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:42:02.762236 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:02.763041 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:02.763528 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m11:42:02.768377 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:42:02.769319 [debug] [Thread-3  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:42:02.776481 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:02.777388 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:02.781136 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:42:02.787654 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m11:42:02.812212 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:02.813539 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.90s]
[0m11:42:02.814675 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:02.815415 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:02.816152 [info ] [Thread-2  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m11:42:02.817405 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:42:02.817996 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:02.818577 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:42:02.819090 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:02.826084 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:42:02.830424 [debug] [Thread-4  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:42:02.835325 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:02.836256 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:02.840541 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:42:02.846409 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m11:42:02.889585 [debug] [Thread-2  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m11:42:02.931850 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:02.933422 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 2.02s]
[0m11:42:02.934696 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:04.393645 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:04.395215 [info ] [Thread-3  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.71s]
[0m11:42:04.396477 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:04.574280 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:04.575585 [info ] [Thread-5  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 1.84s]
[0m11:42:04.576581 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:04.772599 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:04.774246 [info ] [Thread-4  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 2.01s]
[0m11:42:04.775435 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:05.003234 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:05.004972 [info ] [Thread-2  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 2.19s]
[0m11:42:05.006468 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:05.009335 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:42:05.010859 [info ] [MainThread]: 
[0m11:42:05.012166 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.66 seconds (4.66s).
[0m11:42:05.013135 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:42:05.013991 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m11:42:05.014789 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:42:05.015471 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m11:42:05.016161 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m11:42:05.016845 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m11:42:05.038057 [info ] [MainThread]: 
[0m11:42:05.039170 [info ] [MainThread]: [32mCompleted successfully[0m
[0m11:42:05.040445 [info ] [MainThread]: 
[0m11:42:05.041318 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m11:42:05.042551 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2daec940>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2dbf3130>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7faf2c090ca0>]}


============================== 2022-08-21 11:42:10.241542 | abae51d9-35de-44d3-8847-74038d64b1e8 ==============================
[0m11:42:10.241624 [info ] [MainThread]: Running with dbt=1.2.0
[0m11:42:10.243315 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m11:42:10.244442 [debug] [MainThread]: Tracking: tracking
[0m11:42:10.248032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09dd0e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09dd01f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09dd0af0>]}
[0m11:42:11.913529 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m11:42:11.914398 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m11:42:11.922426 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'abae51d9-35de-44d3-8847-74038d64b1e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09b4f0d0>]}
[0m11:42:11.946698 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'abae51d9-35de-44d3-8847-74038d64b1e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09c46790>]}
[0m11:42:11.947750 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m11:42:11.948744 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abae51d9-35de-44d3-8847-74038d64b1e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09c46640>]}
[0m11:42:11.951334 [info ] [MainThread]: 
[0m11:42:11.953060 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m11:42:11.955144 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m11:42:11.955915 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:12.463032 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'abae51d9-35de-44d3-8847-74038d64b1e8', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09c465e0>]}
[0m11:42:12.464569 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m11:42:12.465768 [info ] [MainThread]: 
[0m11:42:12.471346 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m11:42:12.471635 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m11:42:12.471838 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m11:42:12.472155 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m11:42:12.472321 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:12.473410 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m11:42:12.474871 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m11:42:12.476286 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m11:42:12.477630 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m11:42:12.479002 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:42:12.480096 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m11:42:12.481160 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m11:42:12.482203 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m11:42:12.483246 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m11:42:12.484071 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:12.485136 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m11:42:12.486324 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m11:42:12.487479 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m11:42:12.488668 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m11:42:12.489821 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:12.497525 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m11:42:12.501871 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m11:42:12.505568 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m11:42:12.510688 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m11:42:12.525354 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m11:42:12.533818 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:12.534201 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:12.534577 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:12.535360 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m11:42:12.535628 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:12.536333 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m11:42:12.537187 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m11:42:12.537401 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:12.537981 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:12.538747 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m11:42:12.539486 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:12.540193 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:12.540890 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:12.542088 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m11:42:12.542821 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:12.544001 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m11:42:12.545133 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m11:42:12.545894 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:12.546697 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:12.548067 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m11:42:12.548881 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:12.549979 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:12.551101 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m11:42:12.552190 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:42:12.553089 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:12.554189 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:42:12.555222 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:42:12.556069 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:12.556994 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:12.558426 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:42:12.559194 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:12.559918 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:12.560903 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:42:12.561770 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:12.562651 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:12.563507 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:12.564208 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:12.564926 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:12.570055 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m11:42:12.570983 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:12.578725 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m11:42:12.584853 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m11:42:12.585887 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:12.592070 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m11:42:12.599625 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m11:42:12.601480 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:12.603389 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:12.603714 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:12.604061 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:12.604701 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:12.605053 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:12.605508 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:12.606481 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:12.606670 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:12.607216 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:12.607995 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:12.609477 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m11:42:12.610359 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:12.611264 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:12.612404 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m11:42:12.613147 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:12.613897 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.dim_customers
[0m11:42:12.615020 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m11:42:12.615909 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:12.616834 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:12.618030 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m11:42:12.619131 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m11:42:12.619958 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:12.621043 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m11:42:12.622176 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:42:12.623083 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:12.623842 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m11:42:12.624866 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:42:12.625829 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m11:42:12.626732 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:12.627756 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:42:12.628469 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.dim_customers
[0m11:42:12.629178 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:12.630389 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m11:42:12.631282 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:12.632037 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:12.636351 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m11:42:12.637318 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:12.638042 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m11:42:12.650360 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m11:42:12.650995 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:12.657682 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m11:42:12.659000 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m11:42:12.666811 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m11:42:12.668198 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:12.673428 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m11:42:12.674271 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:12.675217 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:12.676075 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m11:42:12.678028 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:12.679100 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:12.680063 [debug] [Thread-2  ]: finished collecting timing info
[0m11:42:12.680969 [debug] [Thread-1  ]: finished collecting timing info
[0m11:42:12.681405 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:12.681933 [debug] [Thread-3  ]: finished collecting timing info
[0m11:42:12.683175 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m11:42:12.683351 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:12.684565 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m11:42:12.685245 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:12.686379 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m11:42:12.687705 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m11:42:12.689170 [debug] [Thread-4  ]: finished collecting timing info
[0m11:42:12.690503 [debug] [Thread-5  ]: finished collecting timing info
[0m11:42:12.691695 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m11:42:12.692783 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m11:42:12.695414 [debug] [MainThread]: Connection 'master' was properly closed.
[0m11:42:12.696055 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m11:42:12.696621 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m11:42:12.697198 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64' was properly closed.
[0m11:42:12.697755 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m11:42:12.698326 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m11:42:12.715240 [info ] [MainThread]: Done.
[0m11:42:12.763215 [debug] [MainThread]: Acquiring new bigquery connection "generate_catalog"
[0m11:42:12.763991 [info ] [MainThread]: Building catalog
[0m11:42:12.765686 [debug] [MainThread]: Opening a new connection, currently in state init
[0m11:42:13.622478 [debug] [ThreadPool]: Acquiring new bigquery connection "airflow-docker-352518.information_schema"
[0m11:42:13.623201 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m11:42:13.623891 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m11:42:13.639987 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:13.643374 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:13.646658 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m11:42:13.690838 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`jaffle_shop`.__TABLES__
        where (upper(dataset_id) = upper('jaffle_shop'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:42:13.691119 [debug] [ThreadPool]: On airflow-docker-352518.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "airflow-docker-352518.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `airflow-docker-352518`.`dbt_x_airflow`.__TABLES__
        where (upper(dataset_id) = upper('dbt_x_airflow'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:42:13.692129 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`stripe`.__TABLES__
        where (upper(dataset_id) = upper('stripe'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m11:42:16.689748 [info ] [MainThread]: Catalog written to /airflow-dbt/dbt_jaffleshop/target/catalog.json
[0m11:42:16.691894 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09dd0e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba0919de80>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fba09a1e0d0>]}
[0m11:42:17.144166 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m11:42:17.145865 [debug] [MainThread]: Connection 'airflow-docker-352518.information_schema' was properly closed.
[0m11:42:17.147011 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.
[0m11:42:17.148307 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.


============================== 2022-08-21 19:48:50.831580 | 5cc766af-928e-46e7-9bed-dc7b9bc5cc09 ==============================
[0m19:48:50.831607 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:48:50.833408 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:48:50.834301 [debug] [MainThread]: Tracking: tracking
[0m19:48:50.838143 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c46668e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c4666790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c46662b0>]}
[0m19:48:50.907425 [info ] [MainThread]: Unable to do partial parsing because env vars used in profiles.yml have changed
[0m19:48:50.909045 [info ] [MainThread]: Unable to do partial parsing because a project config has changed
[0m19:48:50.910289 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'partial_parser', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c45df4f0>]}
[0m19:48:52.606788 [debug] [MainThread]: Parsing macros/custom_macros.sql
[0m19:48:52.608663 [debug] [MainThread]: Parsing macros/etc.sql
[0m19:48:52.611441 [debug] [MainThread]: Parsing macros/adapters.sql
[0m19:48:52.636829 [debug] [MainThread]: Parsing macros/catalog.sql
[0m19:48:52.643708 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m19:48:52.648185 [debug] [MainThread]: Parsing macros/materializations/copy.sql
[0m19:48:52.652304 [debug] [MainThread]: Parsing macros/materializations/snapshot.sql
[0m19:48:52.654925 [debug] [MainThread]: Parsing macros/materializations/incremental.sql
[0m19:48:52.672426 [debug] [MainThread]: Parsing macros/materializations/table.sql
[0m19:48:52.678285 [debug] [MainThread]: Parsing macros/materializations/view.sql
[0m19:48:52.682922 [debug] [MainThread]: Parsing macros/materializations/seed.sql
[0m19:48:52.686454 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m19:48:52.688426 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m19:48:52.690733 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m19:48:52.693080 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m19:48:52.694733 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m19:48:52.696204 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m19:48:52.698246 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m19:48:52.699807 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m19:48:52.701288 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m19:48:52.702627 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m19:48:52.703984 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m19:48:52.705139 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m19:48:52.706298 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m19:48:52.707538 [debug] [MainThread]: Parsing macros/adapters/columns.sql
[0m19:48:52.718977 [debug] [MainThread]: Parsing macros/adapters/relation.sql
[0m19:48:52.734471 [debug] [MainThread]: Parsing macros/adapters/persist_docs.sql
[0m19:48:52.739854 [debug] [MainThread]: Parsing macros/adapters/freshness.sql
[0m19:48:52.744125 [debug] [MainThread]: Parsing macros/adapters/apply_grants.sql
[0m19:48:52.759144 [debug] [MainThread]: Parsing macros/adapters/indexes.sql
[0m19:48:52.763584 [debug] [MainThread]: Parsing macros/adapters/schema.sql
[0m19:48:52.767080 [debug] [MainThread]: Parsing macros/adapters/metadata.sql
[0m19:48:52.775170 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_schema.sql
[0m19:48:52.778947 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_database.sql
[0m19:48:52.781704 [debug] [MainThread]: Parsing macros/get_custom_name/get_custom_alias.sql
[0m19:48:52.784036 [debug] [MainThread]: Parsing macros/etc/datetime.sql
[0m19:48:52.793230 [debug] [MainThread]: Parsing macros/etc/statement.sql
[0m19:48:52.799243 [debug] [MainThread]: Parsing macros/generic_test_sql/unique.sql
[0m19:48:52.800995 [debug] [MainThread]: Parsing macros/generic_test_sql/not_null.sql
[0m19:48:52.802551 [debug] [MainThread]: Parsing macros/generic_test_sql/relationships.sql
[0m19:48:52.803986 [debug] [MainThread]: Parsing macros/generic_test_sql/accepted_values.sql
[0m19:48:52.805941 [debug] [MainThread]: Parsing macros/materializations/configs.sql
[0m19:48:52.809265 [debug] [MainThread]: Parsing macros/materializations/hooks.sql
[0m19:48:52.814432 [debug] [MainThread]: Parsing macros/materializations/tests/where_subquery.sql
[0m19:48:52.817291 [debug] [MainThread]: Parsing macros/materializations/tests/helpers.sql
[0m19:48:52.819926 [debug] [MainThread]: Parsing macros/materializations/tests/test.sql
[0m19:48:52.825514 [debug] [MainThread]: Parsing macros/materializations/seeds/helpers.sql
[0m19:48:52.844379 [debug] [MainThread]: Parsing macros/materializations/seeds/seed.sql
[0m19:48:52.853051 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot_merge.sql
[0m19:48:52.856026 [debug] [MainThread]: Parsing macros/materializations/snapshots/snapshot.sql
[0m19:48:52.869656 [debug] [MainThread]: Parsing macros/materializations/snapshots/strategies.sql
[0m19:48:52.888546 [debug] [MainThread]: Parsing macros/materializations/snapshots/helpers.sql
[0m19:48:52.902361 [debug] [MainThread]: Parsing macros/materializations/models/incremental/on_schema_change.sql
[0m19:48:52.918705 [debug] [MainThread]: Parsing macros/materializations/models/incremental/merge.sql
[0m19:48:52.935440 [debug] [MainThread]: Parsing macros/materializations/models/incremental/incremental.sql
[0m19:48:52.946061 [debug] [MainThread]: Parsing macros/materializations/models/incremental/is_incremental.sql
[0m19:48:52.948816 [debug] [MainThread]: Parsing macros/materializations/models/incremental/column_helpers.sql
[0m19:48:52.954192 [debug] [MainThread]: Parsing macros/materializations/models/view/create_or_replace_view.sql
[0m19:48:52.958318 [debug] [MainThread]: Parsing macros/materializations/models/view/helpers.sql
[0m19:48:52.960822 [debug] [MainThread]: Parsing macros/materializations/models/view/view.sql
[0m19:48:52.967575 [debug] [MainThread]: Parsing macros/materializations/models/view/create_view_as.sql
[0m19:48:52.970937 [debug] [MainThread]: Parsing macros/materializations/models/table/create_table_as.sql
[0m19:48:52.974751 [debug] [MainThread]: Parsing macros/materializations/models/table/table.sql
[0m19:48:52.982034 [debug] [MainThread]: Parsing macros/utils/listagg.sql
[0m19:48:52.985283 [debug] [MainThread]: Parsing macros/utils/split_part.sql
[0m19:48:52.988052 [debug] [MainThread]: Parsing macros/utils/literal.sql
[0m19:48:52.989672 [debug] [MainThread]: Parsing macros/utils/datediff.sql
[0m19:48:52.991710 [debug] [MainThread]: Parsing macros/utils/date_trunc.sql
[0m19:48:52.993972 [debug] [MainThread]: Parsing macros/utils/any_value.sql
[0m19:48:52.996155 [debug] [MainThread]: Parsing macros/utils/bool_or.sql
[0m19:48:52.998190 [debug] [MainThread]: Parsing macros/utils/dateadd.sql
[0m19:48:53.000160 [debug] [MainThread]: Parsing macros/utils/safe_cast.sql
[0m19:48:53.001946 [debug] [MainThread]: Parsing macros/utils/last_day.sql
[0m19:48:53.004483 [debug] [MainThread]: Parsing macros/utils/length.sql
[0m19:48:53.006092 [debug] [MainThread]: Parsing macros/utils/concat.sql
[0m19:48:53.007719 [debug] [MainThread]: Parsing macros/utils/right.sql
[0m19:48:53.009806 [debug] [MainThread]: Parsing macros/utils/escape_single_quotes.sql
[0m19:48:53.011756 [debug] [MainThread]: Parsing macros/utils/position.sql
[0m19:48:53.013978 [debug] [MainThread]: Parsing macros/utils/replace.sql
[0m19:48:53.016276 [debug] [MainThread]: Parsing macros/utils/cast_bool_to_text.sql
[0m19:48:53.017994 [debug] [MainThread]: Parsing macros/utils/data_types.sql
[0m19:48:53.024468 [debug] [MainThread]: Parsing macros/utils/intersect.sql
[0m19:48:53.026536 [debug] [MainThread]: Parsing macros/utils/except.sql
[0m19:48:53.028201 [debug] [MainThread]: Parsing macros/utils/hash.sql
[0m19:48:53.030284 [debug] [MainThread]: Parsing tests/generic/builtin.sql
[0m19:48:53.034595 [debug] [MainThread]: Parsing macros/cross_db_utils/any_value.sql
[0m19:48:53.036815 [debug] [MainThread]: Parsing macros/cross_db_utils/array_append.sql
[0m19:48:53.039859 [debug] [MainThread]: Parsing macros/cross_db_utils/array_concat.sql
[0m19:48:53.042805 [debug] [MainThread]: Parsing macros/cross_db_utils/array_construct.sql
[0m19:48:53.046611 [debug] [MainThread]: Parsing macros/cross_db_utils/bool_or.sql
[0m19:48:53.049408 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_array_to_string.sql
[0m19:48:53.052950 [debug] [MainThread]: Parsing macros/cross_db_utils/cast_bool_to_text.sql
[0m19:48:53.055154 [debug] [MainThread]: Parsing macros/cross_db_utils/concat.sql
[0m19:48:53.056928 [debug] [MainThread]: Parsing macros/cross_db_utils/current_timestamp.sql
[0m19:48:53.061396 [debug] [MainThread]: Parsing macros/cross_db_utils/datatypes.sql
[0m19:48:53.068820 [debug] [MainThread]: Parsing macros/cross_db_utils/dateadd.sql
[0m19:48:53.072718 [debug] [MainThread]: Parsing macros/cross_db_utils/datediff.sql
[0m19:48:53.083703 [debug] [MainThread]: Parsing macros/cross_db_utils/date_trunc.sql
[0m19:48:53.086375 [debug] [MainThread]: Parsing macros/cross_db_utils/escape_single_quotes.sql
[0m19:48:53.089120 [debug] [MainThread]: Parsing macros/cross_db_utils/except.sql
[0m19:48:53.091040 [debug] [MainThread]: Parsing macros/cross_db_utils/hash.sql
[0m19:48:53.093668 [debug] [MainThread]: Parsing macros/cross_db_utils/identifier.sql
[0m19:48:53.096227 [debug] [MainThread]: Parsing macros/cross_db_utils/intersect.sql
[0m19:48:53.098631 [debug] [MainThread]: Parsing macros/cross_db_utils/last_day.sql
[0m19:48:53.103021 [debug] [MainThread]: Parsing macros/cross_db_utils/length.sql
[0m19:48:53.105114 [debug] [MainThread]: Parsing macros/cross_db_utils/listagg.sql
[0m19:48:53.114762 [debug] [MainThread]: Parsing macros/cross_db_utils/literal.sql
[0m19:48:53.116794 [debug] [MainThread]: Parsing macros/cross_db_utils/position.sql
[0m19:48:53.119163 [debug] [MainThread]: Parsing macros/cross_db_utils/replace.sql
[0m19:48:53.121313 [debug] [MainThread]: Parsing macros/cross_db_utils/right.sql
[0m19:48:53.124272 [debug] [MainThread]: Parsing macros/cross_db_utils/safe_cast.sql
[0m19:48:53.127077 [debug] [MainThread]: Parsing macros/cross_db_utils/split_part.sql
[0m19:48:53.134294 [debug] [MainThread]: Parsing macros/cross_db_utils/width_bucket.sql
[0m19:48:53.140240 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_ephemeral.sql
[0m19:48:53.143374 [debug] [MainThread]: Parsing macros/cross_db_utils/_is_relation.sql
[0m19:48:53.145582 [debug] [MainThread]: Parsing macros/generic_tests/accepted_range.sql
[0m19:48:53.149148 [debug] [MainThread]: Parsing macros/generic_tests/at_least_one.sql
[0m19:48:53.151451 [debug] [MainThread]: Parsing macros/generic_tests/cardinality_equality.sql
[0m19:48:53.154253 [debug] [MainThread]: Parsing macros/generic_tests/equality.sql
[0m19:48:53.158887 [debug] [MainThread]: Parsing macros/generic_tests/equal_rowcount.sql
[0m19:48:53.161612 [debug] [MainThread]: Parsing macros/generic_tests/expression_is_true.sql
[0m19:48:53.164973 [debug] [MainThread]: Parsing macros/generic_tests/fewer_rows_than.sql
[0m19:48:53.167579 [debug] [MainThread]: Parsing macros/generic_tests/mutually_exclusive_ranges.sql
[0m19:48:53.176780 [debug] [MainThread]: Parsing macros/generic_tests/not_accepted_values.sql
[0m19:48:53.180173 [debug] [MainThread]: Parsing macros/generic_tests/not_constant.sql
[0m19:48:53.182639 [debug] [MainThread]: Parsing macros/generic_tests/not_null_proportion.sql
[0m19:48:53.185652 [debug] [MainThread]: Parsing macros/generic_tests/recency.sql
[0m19:48:53.188237 [debug] [MainThread]: Parsing macros/generic_tests/relationships_where.sql
[0m19:48:53.191227 [debug] [MainThread]: Parsing macros/generic_tests/sequential_values.sql
[0m19:48:53.195325 [debug] [MainThread]: Parsing macros/generic_tests/test_not_null_where.sql
[0m19:48:53.198041 [debug] [MainThread]: Parsing macros/generic_tests/test_unique_where.sql
[0m19:48:53.200525 [debug] [MainThread]: Parsing macros/generic_tests/unique_combination_of_columns.sql
[0m19:48:53.204170 [debug] [MainThread]: Parsing macros/jinja_helpers/log_info.sql
[0m19:48:53.206206 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_log_format.sql
[0m19:48:53.208273 [debug] [MainThread]: Parsing macros/jinja_helpers/pretty_time.sql
[0m19:48:53.210525 [debug] [MainThread]: Parsing macros/jinja_helpers/slugify.sql
[0m19:48:53.212714 [debug] [MainThread]: Parsing macros/materializations/insert_by_period_materialization.sql
[0m19:48:53.238215 [debug] [MainThread]: Parsing macros/sql/date_spine.sql
[0m19:48:53.243737 [debug] [MainThread]: Parsing macros/sql/deduplicate.sql
[0m19:48:53.252391 [debug] [MainThread]: Parsing macros/sql/generate_series.sql
[0m19:48:53.257719 [debug] [MainThread]: Parsing macros/sql/get_column_values.sql
[0m19:48:53.264924 [debug] [MainThread]: Parsing macros/sql/get_filtered_columns_in_relation.sql
[0m19:48:53.268956 [debug] [MainThread]: Parsing macros/sql/get_query_results_as_dict.sql
[0m19:48:53.272314 [debug] [MainThread]: Parsing macros/sql/get_relations_by_pattern.sql
[0m19:48:53.276906 [debug] [MainThread]: Parsing macros/sql/get_relations_by_prefix.sql
[0m19:48:53.281848 [debug] [MainThread]: Parsing macros/sql/get_tables_by_pattern_sql.sql
[0m19:48:53.289337 [debug] [MainThread]: Parsing macros/sql/get_tables_by_prefix_sql.sql
[0m19:48:53.292170 [debug] [MainThread]: Parsing macros/sql/get_table_types_sql.sql
[0m19:48:53.294950 [debug] [MainThread]: Parsing macros/sql/groupby.sql
[0m19:48:53.297710 [debug] [MainThread]: Parsing macros/sql/haversine_distance.sql
[0m19:48:53.304589 [debug] [MainThread]: Parsing macros/sql/nullcheck.sql
[0m19:48:53.307293 [debug] [MainThread]: Parsing macros/sql/nullcheck_table.sql
[0m19:48:53.309978 [debug] [MainThread]: Parsing macros/sql/pivot.sql
[0m19:48:53.315242 [debug] [MainThread]: Parsing macros/sql/safe_add.sql
[0m19:48:53.318147 [debug] [MainThread]: Parsing macros/sql/star.sql
[0m19:48:53.323160 [debug] [MainThread]: Parsing macros/sql/surrogate_key.sql
[0m19:48:53.327966 [debug] [MainThread]: Parsing macros/sql/union.sql
[0m19:48:53.340319 [debug] [MainThread]: Parsing macros/sql/unpivot.sql
[0m19:48:53.349700 [debug] [MainThread]: Parsing macros/web/get_url_host.sql
[0m19:48:53.352773 [debug] [MainThread]: Parsing macros/web/get_url_parameter.sql
[0m19:48:53.355205 [debug] [MainThread]: Parsing macros/web/get_url_path.sql
[0m19:48:53.841687 [debug] [MainThread]: 1699: static parser successfully parsed prod/agg_transactions.sql
[0m19:48:53.855922 [debug] [MainThread]: 1699: static parser successfully parsed prod/dim_customers.sql
[0m19:48:53.859836 [debug] [MainThread]: 1603: static parser failed on prod/pivoted_orders.sql
[0m19:48:53.866681 [debug] [MainThread]: 1602: parser fallback to jinja rendering on prod/pivoted_orders.sql
[0m19:48:53.869632 [debug] [MainThread]: 1699: static parser successfully parsed stage/stg_customers.sql
[0m19:48:53.873399 [debug] [MainThread]: 1699: static parser successfully parsed stage/stg_orders.sql
[0m19:48:53.877198 [debug] [MainThread]: 1603: static parser failed on stage/stg_payments.sql
[0m19:48:53.885114 [debug] [MainThread]: 1602: parser fallback to jinja rendering on stage/stg_payments.sql
[0m19:48:54.046118 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c2b55940>]}
[0m19:48:54.070556 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c4618c70>]}
[0m19:48:54.071633 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:48:54.072619 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c447c400>]}
[0m19:48:54.075120 [info ] [MainThread]: 
[0m19:48:54.076660 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:48:54.078783 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m19:48:54.079504 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:48:54.847762 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:48:54.848868 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:48:55.618262 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c2b55430>]}
[0m19:48:55.619601 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:48:55.620414 [info ] [MainThread]: 
[0m19:48:55.634528 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m19:48:55.634759 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m19:48:55.635145 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m19:48:55.635409 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m19:48:55.635990 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m19:48:55.636908 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m19:48:55.637766 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m19:48:55.638668 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m19:48:55.639960 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m19:48:55.641618 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m19:48:55.643336 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m19:48:55.645114 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m19:48:55.646072 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m19:48:55.646960 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m19:48:55.647882 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m19:48:55.648707 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m19:48:55.649616 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m19:48:55.650505 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m19:48:55.651280 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m19:48:55.652029 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m19:48:55.656256 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:48:55.665503 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:48:55.669957 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:48:55.675605 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:48:55.682185 [debug] [Thread-4  ]: finished collecting timing info
[0m19:48:55.683018 [debug] [Thread-3  ]: finished collecting timing info
[0m19:48:55.683371 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m19:48:55.683601 [debug] [Thread-1  ]: finished collecting timing info
[0m19:48:55.684600 [debug] [Thread-2  ]: finished collecting timing info
[0m19:48:55.684834 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m19:48:55.707511 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m19:48:55.721495 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:48:55.722506 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m19:48:55.727121 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:48:55.742005 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:48:55.747659 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:48:55.755400 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m19:48:55.764593 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m19:48:55.766367 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m19:48:55.813318 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m19:48:55.817784 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m19:48:55.818076 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m19:48:56.306108 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:48:56.317891 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m19:48:57.116814 [debug] [Thread-4  ]: finished collecting timing info
[0m19:48:57.118640 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c29a6e80>]}
[0m19:48:57.120891 [debug] [Thread-2  ]: finished collecting timing info
[0m19:48:57.121991 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.47s]
[0m19:48:57.123391 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c29a6f40>]}
[0m19:48:57.125089 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m19:48:57.126279 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.48s]
[0m19:48:57.128115 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m19:48:57.128988 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m19:48:57.130289 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m19:48:57.133225 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m19:48:57.134062 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m19:48:57.137255 [debug] [Thread-3  ]: finished collecting timing info
[0m19:48:57.137893 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m19:48:57.139084 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c29a6ee0>]}
[0m19:48:57.144813 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:48:57.146344 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.50s]
[0m19:48:57.149169 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m19:48:57.150646 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.dim_customers
[0m19:48:57.151577 [info ] [Thread-4  ]: 6 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m19:48:57.153003 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m19:48:57.153677 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m19:48:57.154449 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.dim_customers
[0m19:48:57.159396 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:48:57.159692 [debug] [Thread-5  ]: finished collecting timing info
[0m19:48:57.161512 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m19:48:57.165489 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m19:48:57.171022 [debug] [Thread-4  ]: finished collecting timing info
[0m19:48:57.171924 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m19:48:57.175212 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m19:48:57.692804 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:48:57.694243 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:48:57.701347 [debug] [Thread-4  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m19:48:57.703865 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m19:48:58.961368 [debug] [Thread-1  ]: finished collecting timing info
[0m19:48:58.963340 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c00881c0>]}
[0m19:48:58.964855 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.32s]
[0m19:48:58.966434 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m19:49:00.684073 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:00.686244 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c01701c0>]}
[0m19:49:00.687768 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 3.55s]
[0m19:49:00.689109 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m19:49:00.903914 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:00.905473 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': '5cc766af-928e-46e7-9bed-dc7b9bc5cc09', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c0128a60>]}
[0m19:49:00.906508 [info ] [Thread-4  ]: 6 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.75s]
[0m19:49:00.907765 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m19:49:00.910640 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:00.912315 [info ] [MainThread]: 
[0m19:49:00.913402 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 6.84 seconds (6.84s).
[0m19:49:00.914650 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:49:00.915579 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m19:49:00.916429 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m19:49:00.917200 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m19:49:00.917866 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m19:49:00.918491 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m19:49:00.945055 [info ] [MainThread]: 
[0m19:49:00.946318 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:49:00.947600 [info ] [MainThread]: 
[0m19:49:00.948689 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m19:49:00.949852 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c2b557f0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c2b559a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f17c2b5f9d0>]}


============================== 2022-08-21 19:49:11.312121 | fefebaf2-fb26-4dd2-ae27-2100a7ad865d ==============================
[0m19:49:11.312145 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:49:11.313787 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:49:11.315098 [debug] [MainThread]: Tracking: tracking
[0m19:49:11.318691 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205f13ac0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22084bf100>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22084bf3d0>]}
[0m19:49:13.086720 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:49:13.087573 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:49:13.096155 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205c910d0>]}
[0m19:49:13.118549 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205d896d0>]}
[0m19:49:13.119540 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:49:13.120461 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22084bf3d0>]}
[0m19:49:13.122820 [info ] [MainThread]: 
[0m19:49:13.124206 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:13.126643 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m19:49:13.127744 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:49:13.674258 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:49:13.675781 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:49:14.214448 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205e75550>]}
[0m19:49:14.216162 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:49:14.217247 [info ] [MainThread]: 
[0m19:49:14.222935 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m19:49:14.223178 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m19:49:14.223358 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m19:49:14.223611 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m19:49:14.224510 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m19:49:14.225854 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m19:49:14.227048 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m19:49:14.228207 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m19:49:14.229907 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m19:49:14.231339 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m19:49:14.232673 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m19:49:14.234300 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m19:49:14.234967 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m19:49:14.236024 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m19:49:14.237087 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m19:49:14.237938 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m19:49:14.238729 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m19:49:14.239516 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m19:49:14.240416 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m19:49:14.241188 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m19:49:14.250554 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:49:14.255419 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:49:14.260503 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:49:14.266271 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:49:14.273630 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:14.274430 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:14.274684 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:14.274965 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m19:49:14.276174 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m19:49:14.276487 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:14.277120 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m19:49:14.323646 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:49:14.324718 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:49:14.324996 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m19:49:14.340066 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:14.346835 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:49:14.353252 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m19:49:14.354067 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m19:49:14.355420 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m19:49:14.397705 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m19:49:14.398170 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m19:49:14.398584 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m19:49:14.817251 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:49:14.828247 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m19:49:15.594179 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:15.595975 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205b39c70>]}
[0m19:49:15.597850 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.36s]
[0m19:49:15.599637 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m19:49:15.600928 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m19:49:15.601820 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m19:49:15.603217 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m19:49:15.604001 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m19:49:15.605321 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m19:49:15.611133 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:49:15.619903 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:15.620799 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m19:49:15.624019 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m19:49:15.626774 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:15.628921 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205b39c10>]}
[0m19:49:15.630843 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.40s]
[0m19:49:15.632308 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m19:49:15.664533 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:15.666523 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205b5bf10>]}
[0m19:49:15.667764 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.44s]
[0m19:49:15.669020 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m19:49:15.670451 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.dim_customers
[0m19:49:15.671553 [info ] [Thread-4  ]: 6 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m19:49:15.673021 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m19:49:15.674215 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m19:49:15.676645 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.dim_customers
[0m19:49:15.683043 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:49:15.691759 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:15.692870 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m19:49:15.700723 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m19:49:16.166150 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:49:16.176682 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m19:49:16.233693 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:49:16.243980 [debug] [Thread-4  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m19:49:17.373685 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:17.375397 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22042df850>]}
[0m19:49:17.376838 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.15s]
[0m19:49:17.378198 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m19:49:18.753661 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:18.755380 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2204256190>]}
[0m19:49:18.756659 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 3.15s]
[0m19:49:18.757899 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m19:49:19.113783 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:19.115407 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'fefebaf2-fb26-4dd2-ae27-2100a7ad865d', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2204218b50>]}
[0m19:49:19.116573 [info ] [Thread-4  ]: 6 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.44s]
[0m19:49:19.117818 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m19:49:19.121084 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:19.122673 [info ] [MainThread]: 
[0m19:49:19.123406 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 6.00 seconds (6.00s).
[0m19:49:19.124495 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:49:19.125381 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m19:49:19.126301 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m19:49:19.127134 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m19:49:19.127785 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m19:49:19.128383 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m19:49:19.153817 [info ] [MainThread]: 
[0m19:49:19.155518 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:49:19.157162 [info ] [MainThread]: 
[0m19:49:19.158337 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m19:49:19.159915 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205bbac70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f2205c91eb0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f22042bc160>]}


============================== 2022-08-21 19:49:24.144407 | 90b0a554-9c5e-4b2a-b1fc-8a3210fcdac5 ==============================
[0m19:49:24.144430 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:49:24.146391 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m19:49:24.147599 [debug] [MainThread]: Tracking: tracking
[0m19:49:24.151613 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8290a87070>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8290a7a400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8290a87400>]}
[0m19:49:25.825643 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:49:25.826497 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:49:25.834734 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '90b0a554-9c5e-4b2a-b1fc-8a3210fcdac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82908060d0>]}
[0m19:49:25.856593 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '90b0a554-9c5e-4b2a-b1fc-8a3210fcdac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82908fb700>]}
[0m19:49:25.857728 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:49:25.859029 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90b0a554-9c5e-4b2a-b1fc-8a3210fcdac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8290a87eb0>]}
[0m19:49:25.861679 [info ] [MainThread]: 
[0m19:49:25.863374 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:25.865933 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:49:25.867101 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:49:26.444987 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '90b0a554-9c5e-4b2a-b1fc-8a3210fcdac5', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f8290a7a400>]}
[0m19:49:26.447251 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:49:26.448607 [info ] [MainThread]: 
[0m19:49:26.453994 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:26.454191 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:26.454416 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:26.454592 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:26.454878 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:26.455458 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m19:49:26.456523 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m19:49:26.457544 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m19:49:26.458509 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m19:49:26.459426 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m19:49:26.460804 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:49:26.461909 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:49:26.463223 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:49:26.464370 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:49:26.465852 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:49:26.466456 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:26.467255 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:26.467923 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:26.468660 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:26.469347 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:26.470076 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:26.470955 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:26.471661 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:26.472360 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:26.473083 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:26.491238 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:49:26.505088 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:49:26.510057 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:49:26.516957 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:49:26.522863 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:49:26.534632 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:26.535764 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:26.536677 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:26.537244 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:26.537742 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:26.538004 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:26.538731 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:26.539867 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:26.562630 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:26.563009 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:49:26.564460 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:26.569222 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:49:26.572566 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:49:26.576028 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:49:26.581429 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:49:26.588174 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m19:49:26.589228 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m19:49:26.590559 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:26.591674 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m19:49:26.594018 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m19:49:26.634484 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m19:49:26.635553 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m19:49:26.640035 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m19:49:26.645373 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m19:49:26.645748 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m19:49:28.418710 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:28.420401 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.96s]
[0m19:49:28.422409 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:28.423088 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:28.424464 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 1.96s]
[0m19:49:28.426232 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:28.428062 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:28.429631 [info ] [Thread-4  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m19:49:28.431343 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:28.433610 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:49:28.434973 [info ] [Thread-5  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m19:49:28.435996 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:28.437555 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:49:28.438182 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:28.439060 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:28.449558 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:28.455954 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:49:28.463210 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:49:28.470948 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:28.472023 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:28.472223 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:28.475396 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:49:28.476778 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:28.481391 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:49:28.487199 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m19:49:28.488325 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m19:49:28.530977 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:28.531343 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:28.563113 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:28.565161 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 2.10s]
[0m19:49:28.566536 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:28.567606 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:28.568504 [info ] [Thread-3  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m19:49:28.569890 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:49:28.570505 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:28.571115 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:28.576778 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:49:28.584377 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:28.585232 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:28.588531 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:49:28.596894 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m19:49:28.602174 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:28.603574 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 2.14s]
[0m19:49:28.605122 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:28.606497 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:28.607280 [info ] [Thread-1  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m19:49:28.608657 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:49:28.609506 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:28.610304 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:28.617319 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:49:28.626519 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:28.627531 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:28.631358 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:49:28.642253 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:28.644489 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:28.645114 [debug] [Thread-3  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:28.650781 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 2.19s]
[0m19:49:28.655033 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:28.693055 [debug] [Thread-1  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:30.413323 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:30.415252 [info ] [Thread-4  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.98s]
[0m19:49:30.416837 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:30.483694 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:30.485266 [info ] [Thread-5  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 2.05s]
[0m19:49:30.486404 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:30.520826 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:30.523105 [info ] [Thread-3  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.95s]
[0m19:49:30.524554 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:30.534679 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:30.536603 [info ] [Thread-1  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.93s]
[0m19:49:30.538090 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:30.541523 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:30.543866 [info ] [MainThread]: 
[0m19:49:30.545248 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.68 seconds (4.68s).
[0m19:49:30.546507 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:49:30.547802 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m19:49:30.548912 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa' was properly closed.
[0m19:49:30.550297 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m19:49:30.551518 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m19:49:30.552475 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m19:49:30.573099 [info ] [MainThread]: 
[0m19:49:30.574217 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:49:30.575375 [info ] [MainThread]: 
[0m19:49:30.576654 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m19:49:30.578253 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82907f83d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82908fb3d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f82909eaaf0>]}


============================== 2022-08-21 19:49:36.030171 | a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f ==============================
[0m19:49:36.030196 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:49:36.031798 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'which': 'run', 'rpc_method': 'run', 'indirect_selection': 'eager'}
[0m19:49:36.033215 [debug] [MainThread]: Tracking: tracking
[0m19:49:36.037024 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d87c400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d87c2e0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d87c310>]}
[0m19:49:37.797450 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:49:37.798255 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:49:37.806480 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d57c0a0>]}
[0m19:49:37.827908 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d6f1790>]}
[0m19:49:37.828971 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:49:37.830095 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d6f1640>]}
[0m19:49:37.832752 [info ] [MainThread]: 
[0m19:49:37.834260 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:37.836389 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518"
[0m19:49:37.837265 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:49:38.464681 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:49:38.466026 [debug] [ThreadPool]: Opening a new connection, currently in state closed
[0m19:49:39.003720 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d6f1490>]}
[0m19:49:39.005310 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:49:39.006368 [info ] [MainThread]: 
[0m19:49:39.011587 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m19:49:39.011842 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m19:49:39.012185 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m19:49:39.012537 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m19:49:39.013424 [info ] [Thread-1  ]: 1 of 6 START table model dbt_x_airflow.agg_transactions ........................ [RUN]
[0m19:49:39.014497 [info ] [Thread-2  ]: 2 of 6 START view model dbt_x_airflow.stg_customers ............................ [RUN]
[0m19:49:39.015670 [info ] [Thread-3  ]: 3 of 6 START view model dbt_x_airflow.stg_orders ............................... [RUN]
[0m19:49:39.016801 [info ] [Thread-4  ]: 4 of 6 START view model dbt_x_airflow.stg_payments ............................. [RUN]
[0m19:49:39.018341 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m19:49:39.019528 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m19:49:39.020605 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m19:49:39.021820 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m19:49:39.022437 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m19:49:39.023072 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m19:49:39.023763 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m19:49:39.024457 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m19:49:39.025211 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m19:49:39.026038 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m19:49:39.027043 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m19:49:39.028061 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m19:49:39.037396 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:49:39.042073 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:49:39.048019 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:49:39.053141 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:49:39.060443 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:39.060968 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:39.061716 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:39.062400 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m19:49:39.062684 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:39.063629 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m19:49:39.064636 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m19:49:39.087544 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m19:49:39.103127 [debug] [Thread-3  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:49:39.107245 [debug] [Thread-2  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:49:39.111770 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:49:39.127211 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:39.135419 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m19:49:39.136247 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m19:49:39.138386 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m19:49:39.181602 [debug] [Thread-3  ]: On model.dbt_x_airflow.stg_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_orders"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  OPTIONS()
  as 

with
orders as (

    select
        id as order_id,
        user_id as customer_id,
        order_date,
        status

    from `dbt-tutorial`.`jaffle_shop`.`orders`

)
select * from orders;


[0m19:49:39.185056 [debug] [Thread-2  ]: On model.dbt_x_airflow.stg_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_customers"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  OPTIONS()
  as 

with 
customers as (

    select
        id as customer_id,
        first_name,
        last_name

    from `dbt-tutorial`.`jaffle_shop`.`customers`

)

select * from customers;


[0m19:49:39.189163 [debug] [Thread-4  ]: On model.dbt_x_airflow.stg_payments: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.stg_payments"} */


  create or replace view `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
  OPTIONS()
  as select
    id as payment_id,
    orderid as order_id,
    paymentmethod as payment_method,
    status,
    ROUND(amount/100, 1) as amount,
    created as created_at
from `dbt-tutorial`.`stripe`.`payment`;


[0m19:49:39.668613 [debug] [Thread-1  ]: Writing runtime SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:49:39.677449 [debug] [Thread-1  ]: On model.dbt_x_airflow.agg_transactions: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.agg_transactions"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`agg_transactions`
  
  
  OPTIONS()
  as (
    select 
  created,
  paymentmethod,
  count(paymentmethod) as transactions
from `dbt-tutorial`.`stripe`.`payment`
group by 1,2
  );
  
[0m19:49:40.338314 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:40.340168 [debug] [Thread-2  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1c457700>]}
[0m19:49:40.341902 [info ] [Thread-2  ]: 2 of 6 OK created view model dbt_x_airflow.stg_customers ....................... [[32mOK[0m in 1.32s]
[0m19:49:40.344049 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m19:49:40.364408 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:40.366011 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d4a4af0>]}
[0m19:49:40.367498 [info ] [Thread-4  ]: 4 of 6 OK created view model dbt_x_airflow.stg_payments ........................ [[32mOK[0m in 1.34s]
[0m19:49:40.368769 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m19:49:40.370045 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m19:49:40.370965 [info ] [Thread-5  ]: 5 of 6 START table model dbt_x_airflow.pivoted_orders .......................... [RUN]
[0m19:49:40.372452 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m19:49:40.373176 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m19:49:40.373898 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m19:49:40.379642 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:49:40.387064 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:40.387757 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m19:49:40.390740 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m19:49:40.424490 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:40.426186 [debug] [Thread-3  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d4a4a90>]}
[0m19:49:40.427526 [info ] [Thread-3  ]: 3 of 6 OK created view model dbt_x_airflow.stg_orders .......................... [[32mOK[0m in 1.41s]
[0m19:49:40.428673 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m19:49:40.430113 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.dim_customers
[0m19:49:40.431479 [info ] [Thread-4  ]: 6 of 6 START table model dbt_x_airflow.dim_customers ........................... [RUN]
[0m19:49:40.433177 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m19:49:40.434030 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m19:49:40.434793 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.dim_customers
[0m19:49:40.439408 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:49:40.450761 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:40.451865 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m19:49:40.458809 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m19:49:40.873342 [debug] [Thread-5  ]: Writing runtime SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:49:40.882971 [debug] [Thread-5  ]: On model.dbt_x_airflow.pivoted_orders: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.pivoted_orders"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`pivoted_orders`
  
  
  OPTIONS()
  as (
    select
    order_id,
    sum( if (payment_method = 'bank_transfer', amount,0)) bank_transfer,
    sum( if (payment_method = 'coupon', amount,0)) coupon,
    sum( if (payment_method = 'credit_card', amount,0)) credit_card,
    sum( if (payment_method = 'gift_card', amount,0)) gift_card,
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_payments`
where status = 'success'
group by 1
  );
  
[0m19:49:40.963182 [debug] [Thread-4  ]: Writing runtime SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:49:40.971437 [debug] [Thread-4  ]: On model.dbt_x_airflow.dim_customers: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "model.dbt_x_airflow.dim_customers"} */


  create or replace table `airflow-docker-352518`.`dbt_x_airflow`.`dim_customers`
  
  
  OPTIONS()
  as (
    


with
customer_orders as (

    select
        customer_id,

        min(order_date) as first_order_date,
        max(order_date) as most_recent_order_date,
        count(order_id) as number_of_orders

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    

    group by 1

)


select
    customers.customer_id,
    customers.first_name,
    customers.last_name,
    customer_orders.first_order_date,
    customer_orders.most_recent_order_date,
    coalesce(customer_orders.number_of_orders, 0) as number_of_orders


from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers` as customers

left join customer_orders using (customer_id)
  );
  
[0m19:49:42.294692 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:42.296780 [debug] [Thread-1  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1c413a90>]}
[0m19:49:42.298704 [info ] [Thread-1  ]: 1 of 6 OK created table model dbt_x_airflow.agg_transactions ................... [[32mCREATE TABLE (96.0 rows, 2.4 KB processed)[0m in 3.28s]
[0m19:49:42.300515 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m19:49:43.754443 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:43.755867 [debug] [Thread-5  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1c3dd700>]}
[0m19:49:43.756926 [info ] [Thread-5  ]: 5 of 6 OK created table model dbt_x_airflow.pivoted_orders ..................... [[32mCREATE TABLE (99.0 rows, 4.4 KB processed)[0m in 3.38s]
[0m19:49:43.758066 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m19:49:43.869183 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:43.870893 [debug] [Thread-4  ]: Sending event: {'category': 'dbt', 'action': 'run_model', 'label': 'a6cf79d6-8bf1-42ba-a75b-f9c1e164ef3f', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1c428670>]}
[0m19:49:43.872348 [info ] [Thread-4  ]: 6 of 6 OK created table model dbt_x_airflow.dim_customers ...................... [[32mCREATE TABLE (100.0 rows, 4.3 KB processed)[0m in 3.44s]
[0m19:49:43.873686 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m19:49:43.876518 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:43.878185 [info ] [MainThread]: 
[0m19:49:43.879069 [info ] [MainThread]: Finished running 3 view models, 3 table models in 0 hours 0 minutes and 6.04 seconds (6.04s).
[0m19:49:43.880344 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:49:43.881315 [debug] [MainThread]: Connection 'model.dbt_x_airflow.agg_transactions' was properly closed.
[0m19:49:43.882440 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_customers' was properly closed.
[0m19:49:43.883790 [debug] [MainThread]: Connection 'model.dbt_x_airflow.stg_orders' was properly closed.
[0m19:49:43.884668 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m19:49:43.885529 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m19:49:43.906804 [info ] [MainThread]: 
[0m19:49:43.907864 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:49:43.908924 [info ] [MainThread]: 
[0m19:49:43.910053 [info ] [MainThread]: Done. PASS=6 WARN=0 ERROR=0 SKIP=0 TOTAL=6
[0m19:49:43.911127 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d5c8c40>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1d6f1490>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fbd1c475400>]}


============================== 2022-08-21 19:49:49.035241 | af010b58-37de-453e-8cd8-d37b12ad2f87 ==============================
[0m19:49:49.035263 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:49:49.036543 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m19:49:49.037520 [debug] [MainThread]: Tracking: tracking
[0m19:49:49.041238 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd522714400>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5227143d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd522714d00>]}
[0m19:49:50.701365 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:49:50.702164 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:49:50.710197 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': 'af010b58-37de-453e-8cd8-d37b12ad2f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd5224930a0>]}
[0m19:49:50.733586 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': 'af010b58-37de-453e-8cd8-d37b12ad2f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd52258e7c0>]}
[0m19:49:50.734584 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:49:50.735610 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af010b58-37de-453e-8cd8-d37b12ad2f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd52258e670>]}
[0m19:49:50.738084 [info ] [MainThread]: 
[0m19:49:50.739484 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:50.741680 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:49:50.742660 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:49:51.293702 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': 'af010b58-37de-453e-8cd8-d37b12ad2f87', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd52251b910>]}
[0m19:49:51.295301 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:49:51.296281 [info ] [MainThread]: 
[0m19:49:51.302531 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:51.302810 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:51.303080 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:51.303434 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:51.303705 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:51.303942 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m19:49:51.305143 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m19:49:51.305963 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m19:49:51.306967 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m19:49:51.307691 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m19:49:51.309132 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:49:51.310839 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:49:51.312170 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:49:51.313569 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:49:51.314710 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:49:51.315668 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:51.316740 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:51.317544 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:51.318462 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:51.319070 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:51.319981 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:51.320769 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:51.321563 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:51.322546 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:51.323190 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:51.344495 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:49:51.349712 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:49:51.363287 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:49:51.369061 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:49:51.374633 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:49:51.382818 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:51.383330 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:51.384661 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:51.385075 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:51.386063 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:51.386883 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:51.387205 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:51.408462 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:49:51.408751 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:51.410039 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:51.414040 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:49:51.415341 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:51.422021 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:49:51.426462 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:49:51.428106 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m19:49:51.431599 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:49:51.438132 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m19:49:51.438982 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m19:49:51.439786 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:49:51.440975 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m19:49:51.481864 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m19:49:51.484511 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m19:49:51.486031 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m19:49:51.494443 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m19:49:51.494797 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m19:49:53.167751 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:53.169753 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.86s]
[0m19:49:53.171163 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:49:53.172376 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:53.173468 [info ] [Thread-4  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m19:49:53.175226 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:49:53.184020 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:53.185669 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:53.197783 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:49:53.211625 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:53.212884 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:53.220215 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:49:53.231271 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m19:49:53.246990 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:53.249646 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.94s]
[0m19:49:53.251851 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:49:53.253307 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:53.254478 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:53.256888 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.95s]
[0m19:49:53.258231 [info ] [Thread-3  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m19:49:53.259773 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:49:53.261728 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:49:53.262757 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:53.263694 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:53.264679 [info ] [Thread-2  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m19:49:53.265628 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:53.267197 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:49:53.281075 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:49:53.281786 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:53.284314 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:53.292793 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:49:53.293797 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:53.298653 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:53.299972 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:53.300630 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:53.305476 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:49:53.306854 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:53.311443 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:49:53.315574 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m19:49:53.316844 [debug] [Thread-2  ]: Opening a new connection, currently in state closed
[0m19:49:53.366420 [debug] [Thread-3  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:53.369240 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:53.374449 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 2.06s]
[0m19:49:53.375666 [debug] [Thread-1  ]: finished collecting timing info
[0m19:49:53.376034 [debug] [Thread-2  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:53.377561 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:49:53.378944 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 2.07s]
[0m19:49:53.381766 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:53.385188 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:49:53.386256 [info ] [Thread-5  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m19:49:53.390011 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:49:53.391262 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:53.392084 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:53.399616 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:49:53.408495 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:53.412958 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:53.419566 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:49:53.428634 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m19:49:53.475074 [debug] [Thread-5  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:49:55.015223 [debug] [Thread-4  ]: finished collecting timing info
[0m19:49:55.021785 [info ] [Thread-4  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.85s]
[0m19:49:55.023152 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:49:55.152732 [debug] [Thread-3  ]: finished collecting timing info
[0m19:49:55.154694 [info ] [Thread-3  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 1.89s]
[0m19:49:55.156003 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:49:55.422805 [debug] [Thread-2  ]: finished collecting timing info
[0m19:49:55.424349 [info ] [Thread-2  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 2.16s]
[0m19:49:55.425536 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:49:55.593013 [debug] [Thread-5  ]: finished collecting timing info
[0m19:49:55.595626 [info ] [Thread-5  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 2.21s]
[0m19:49:55.597791 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:49:55.602260 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:49:55.604135 [info ] [MainThread]: 
[0m19:49:55.605406 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.86 seconds (4.86s).
[0m19:49:55.606491 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:49:55.607328 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m19:49:55.608273 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m19:49:55.609128 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m19:49:55.610149 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m19:49:55.611241 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m19:49:55.636670 [info ] [MainThread]: 
[0m19:49:55.638318 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:49:55.639658 [info ] [MainThread]: 
[0m19:49:55.641071 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m19:49:55.642594 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd52247e7c0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd52247ec70>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd52258e850>]}


============================== 2022-08-21 19:50:01.437224 | 71f6aa12-a244-4147-bef3-15884dd87c67 ==============================
[0m19:50:01.437245 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:50:01.438526 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m19:50:01.439575 [debug] [MainThread]: Tracking: tracking
[0m19:50:01.443299 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144aebf0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144aebfe50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144aebfcd0>]}
[0m19:50:03.162760 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:50:03.163675 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:50:03.171846 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '71f6aa12-a244-4147-bef3-15884dd87c67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144ac400a0>]}
[0m19:50:03.192120 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '71f6aa12-a244-4147-bef3-15884dd87c67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144ad32820>]}
[0m19:50:03.193339 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:50:03.194612 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71f6aa12-a244-4147-bef3-15884dd87c67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144ad326d0>]}
[0m19:50:03.197604 [info ] [MainThread]: 
[0m19:50:03.199287 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:50:03.201699 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:50:03.202757 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:03.753780 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '71f6aa12-a244-4147-bef3-15884dd87c67', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144ad3e9d0>]}
[0m19:50:03.755391 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:50:03.756480 [info ] [MainThread]: 
[0m19:50:03.762011 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m19:50:03.762310 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m19:50:03.762543 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m19:50:03.762741 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m19:50:03.762951 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:03.764272 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m19:50:03.765517 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m19:50:03.767022 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m19:50:03.768378 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m19:50:03.769660 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:03.770567 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m19:50:03.771389 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m19:50:03.772106 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m19:50:03.772800 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m19:50:03.773500 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:03.774217 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m19:50:03.774913 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m19:50:03.775621 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m19:50:03.776608 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m19:50:03.777891 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:03.785791 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:50:03.790977 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:50:03.795101 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:50:03.803857 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:50:03.820585 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:03.828447 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:03.829057 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:03.829293 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:03.830610 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m19:50:03.830907 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:03.831831 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m19:50:03.832512 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:03.833117 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m19:50:03.834089 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:03.834792 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m19:50:03.835552 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:03.836288 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:03.837309 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:03.838530 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m19:50:03.839166 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:03.840496 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m19:50:03.841242 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:03.842562 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m19:50:03.843521 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:03.845129 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m19:50:03.846190 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:03.847789 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:03.848777 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:03.850155 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:03.850955 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:03.852226 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:03.853229 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:03.854465 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:03.855283 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:03.856539 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:03.857235 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:03.858254 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:03.859175 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:03.860093 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:03.861032 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:03.861839 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:03.862635 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:03.863357 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:03.869500 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:03.870222 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:03.879036 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:03.880086 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:03.885949 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:03.892505 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:03.899526 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:03.901779 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:03.903604 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:03.904931 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:03.905232 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:03.906070 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:03.906278 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:03.907334 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:03.908035 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:03.908227 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:03.908976 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:03.909778 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:03.911190 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:03.912041 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:03.912775 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:03.914011 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:03.914907 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:03.916017 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.dim_customers
[0m19:50:03.917012 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:03.918333 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:03.919147 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:03.920381 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:03.921621 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m19:50:03.922781 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:03.923569 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:03.924674 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:03.925591 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:03.926492 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m19:50:03.927585 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m19:50:03.928908 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:03.929673 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:03.930979 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:03.931768 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.dim_customers
[0m19:50:03.933240 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m19:50:03.934132 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:03.934933 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:03.935650 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:03.940206 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:50:03.941212 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m19:50:03.942141 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:03.955288 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:03.955869 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:03.957989 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m19:50:03.965322 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:03.972445 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:03.977817 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:50:03.978189 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:03.979640 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:03.982681 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m19:50:03.983079 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:03.984051 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:03.985265 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:03.985502 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:03.986258 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:03.987122 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:03.987414 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:03.988452 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m19:50:03.989255 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:03.989967 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:03.991294 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:03.992068 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m19:50:03.993801 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:03.995347 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:03.997185 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:03.998577 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:04.001014 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m19:50:04.003786 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:50:04.004622 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64' was properly closed.
[0m19:50:04.005300 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m19:50:04.006101 [debug] [MainThread]: Connection 'test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1' was properly closed.
[0m19:50:04.006918 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m19:50:04.007707 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m19:50:04.029876 [info ] [MainThread]: Done.
[0m19:50:04.100663 [debug] [MainThread]: Acquiring new bigquery connection "generate_catalog"
[0m19:50:04.102001 [info ] [MainThread]: Building catalog
[0m19:50:04.104200 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:50:04.957156 [debug] [ThreadPool]: Acquiring new bigquery connection "airflow-docker-352518.information_schema"
[0m19:50:04.958214 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m19:50:04.959300 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m19:50:04.992841 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:04.999426 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:05.003132 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:05.058015 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`stripe`.__TABLES__
        where (upper(dataset_id) = upper('stripe'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:05.061129 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`jaffle_shop`.__TABLES__
        where (upper(dataset_id) = upper('jaffle_shop'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:05.063482 [debug] [ThreadPool]: On airflow-docker-352518.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "airflow-docker-352518.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `airflow-docker-352518`.`dbt_x_airflow`.__TABLES__
        where (upper(dataset_id) = upper('dbt_x_airflow'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:08.600687 [info ] [MainThread]: Catalog written to /dbt-airflow/dbt_jaffleshop/target/catalog.json
[0m19:50:08.602089 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144aebf0d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144aebf040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f144af7dcd0>]}
[0m19:50:09.076217 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m19:50:09.077732 [debug] [MainThread]: Connection 'airflow-docker-352518.information_schema' was properly closed.
[0m19:50:09.078918 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.
[0m19:50:09.080028 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.


============================== 2022-08-21 19:50:14.465849 | 4aa73eaf-df6f-4b08-bb35-35e4fb899514 ==============================
[0m19:50:14.465875 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:50:14.467743 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'indirect_selection': 'eager', 'which': 'test', 'rpc_method': 'test'}
[0m19:50:14.468869 [debug] [MainThread]: Tracking: tracking
[0m19:50:14.472573 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075b9d790>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075b9d3a0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075b9d670>]}
[0m19:50:16.186562 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:50:16.187356 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:50:16.195212 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '4aa73eaf-df6f-4b08-bb35-35e4fb899514', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f10759770d0>]}
[0m19:50:16.218326 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '4aa73eaf-df6f-4b08-bb35-35e4fb899514', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075a6d730>]}
[0m19:50:16.219389 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:50:16.220360 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4aa73eaf-df6f-4b08-bb35-35e4fb899514', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f107878a1c0>]}
[0m19:50:16.222910 [info ] [MainThread]: 
[0m19:50:16.224237 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:50:16.226357 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:50:16.227395 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:16.732926 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '4aa73eaf-df6f-4b08-bb35-35e4fb899514', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075b896a0>]}
[0m19:50:16.734560 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:50:16.735525 [info ] [MainThread]: 
[0m19:50:16.740402 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:16.740629 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:16.740844 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:16.741039 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:16.741289 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:16.741556 [info ] [Thread-1  ]: 1 of 9 START test accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [RUN]
[0m19:50:16.742298 [info ] [Thread-2  ]: 2 of 9 START test not_null_stg_customers_customer_id ........................... [RUN]
[0m19:50:16.743123 [info ] [Thread-3  ]: 3 of 9 START test not_null_stg_orders_order_id ................................. [RUN]
[0m19:50:16.744106 [info ] [Thread-4  ]: 4 of 9 START test source_not_null_jaffle_shop_customers_id ..................... [RUN]
[0m19:50:16.745069 [info ] [Thread-5  ]: 5 of 9 START test source_not_null_jaffle_shop_orders_id ........................ [RUN]
[0m19:50:16.746756 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:16.748358 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:16.749748 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:16.751133 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:16.752709 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:16.753408 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:16.754206 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:16.754992 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:16.755800 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:16.756480 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:16.757163 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:16.757922 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:16.758700 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:16.759453 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:16.760459 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:16.781159 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:16.791354 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:16.799767 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:16.805575 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:16.811205 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:16.819558 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:16.820821 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:16.821185 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:16.821641 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:16.822198 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:16.828045 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:16.843787 [debug] [Thread-2  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:16.844599 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:16.845679 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:16.846943 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:16.847967 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:16.853999 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:16.858447 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:16.862220 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:16.862605 [debug] [Thread-2  ]: Opening a new connection, currently in state init
[0m19:50:16.865827 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:16.873234 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:50:16.874095 [debug] [Thread-3  ]: Opening a new connection, currently in state init
[0m19:50:16.875259 [debug] [Thread-5  ]: Opening a new connection, currently in state init
[0m19:50:16.876083 [debug] [Thread-4  ]: Opening a new connection, currently in state init
[0m19:50:16.917964 [debug] [Thread-3  ]: On test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select order_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
where order_id is null



      
    ) dbt_internal_test
[0m19:50:16.920087 [debug] [Thread-2  ]: On test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select customer_id
from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
where customer_id is null



      
    ) dbt_internal_test
[0m19:50:16.922814 [debug] [Thread-1  ]: On test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with all_values as (

    select
        status as value_field,
        count(*) as n_records

    from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
    group by status

)

select *
from all_values
where value_field not in (
    'completed','shipped','returned','return_pending','placed'
)



      
    ) dbt_internal_test
[0m19:50:16.928195 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`customers`
where id is null



      
    ) dbt_internal_test
[0m19:50:16.932620 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    



select id
from `dbt-tutorial`.`jaffle_shop`.`orders`
where id is null



      
    ) dbt_internal_test
[0m19:50:18.503065 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:18.504553 [info ] [Thread-4  ]: 4 of 9 PASS source_not_null_jaffle_shop_customers_id ........................... [[32mPASS[0m in 1.75s]
[0m19:50:18.505794 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:18.506752 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:18.507725 [info ] [Thread-4  ]: 6 of 9 START test source_unique_jaffle_shop_customers_id ....................... [RUN]
[0m19:50:18.509188 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:18.510257 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:18.511413 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:18.520185 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:18.529840 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:18.531070 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:18.538135 [debug] [Thread-4  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:18.546711 [debug] [Thread-4  ]: Opening a new connection, currently in state closed
[0m19:50:18.589653 [debug] [Thread-4  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`customers`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:50:18.610476 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:18.611987 [info ] [Thread-5  ]: 5 of 9 PASS source_not_null_jaffle_shop_orders_id .............................. [[32mPASS[0m in 1.86s]
[0m19:50:18.613928 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:18.614761 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:18.615933 [info ] [Thread-3  ]: 3 of 9 PASS not_null_stg_orders_order_id ....................................... [[32mPASS[0m in 1.87s]
[0m19:50:18.617164 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:18.618331 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:18.618969 [info ] [Thread-5  ]: 7 of 9 START test source_unique_jaffle_shop_orders_id .......................... [RUN]
[0m19:50:18.619864 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:18.621229 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:18.621784 [info ] [Thread-3  ]: 8 of 9 START test unique_stg_customers_customer_id ............................. [RUN]
[0m19:50:18.622497 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:18.623861 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:18.624481 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:18.625167 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:18.630618 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:18.631891 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:18.638340 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:18.643984 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:18.645210 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:18.645518 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:18.649236 [debug] [Thread-5  ]: Writing runtime SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:18.650460 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:18.655076 [debug] [Thread-3  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:18.660319 [debug] [Thread-5  ]: Opening a new connection, currently in state closed
[0m19:50:18.661414 [debug] [Thread-3  ]: Opening a new connection, currently in state closed
[0m19:50:18.692855 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:18.694781 [info ] [Thread-1  ]: 1 of 9 PASS accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed  [[32mPASS[0m in 1.95s]
[0m19:50:18.696247 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:18.698097 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:18.699334 [info ] [Thread-1  ]: 9 of 9 START test unique_stg_orders_order_id ................................... [RUN]
[0m19:50:18.701149 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:18.702085 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:18.702891 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:18.708227 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:18.709271 [debug] [Thread-3  ]: On test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select customer_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_customers`
  where customer_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:50:18.709649 [debug] [Thread-5  ]: On test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select id as unique_field
  from `dbt-tutorial`.`jaffle_shop`.`orders`
  where id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:50:18.712702 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:18.718841 [info ] [Thread-2  ]: 2 of 9 PASS not_null_stg_customers_customer_id ................................. [[32mPASS[0m in 1.97s]
[0m19:50:18.720288 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:18.720999 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:18.722215 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:18.726514 [debug] [Thread-1  ]: Writing runtime SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:18.735921 [debug] [Thread-1  ]: Opening a new connection, currently in state closed
[0m19:50:18.778608 [debug] [Thread-1  ]: On test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "node_id": "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"} */
select
      count(*) as failures,
      count(*) != 0 as should_warn,
      count(*) != 0 as should_error
    from (
      
    
    

with dbt_test__target as (

  select order_id as unique_field
  from `airflow-docker-352518`.`dbt_x_airflow`.`stg_orders`
  where order_id is not null

)

select
    unique_field,
    count(*) as n_records

from dbt_test__target
group by unique_field
having count(*) > 1



      
    ) dbt_internal_test
[0m19:50:20.242304 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:20.244004 [info ] [Thread-4  ]: 6 of 9 PASS source_unique_jaffle_shop_customers_id ............................. [[32mPASS[0m in 1.74s]
[0m19:50:20.245536 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:20.412693 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:20.415258 [info ] [Thread-3  ]: 8 of 9 PASS unique_stg_customers_customer_id ................................... [[32mPASS[0m in 1.79s]
[0m19:50:20.416714 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:20.592037 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:20.593412 [info ] [Thread-5  ]: 7 of 9 PASS source_unique_jaffle_shop_orders_id ................................ [[32mPASS[0m in 1.97s]
[0m19:50:20.594860 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:20.683209 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:20.685768 [info ] [Thread-1  ]: 9 of 9 PASS unique_stg_orders_order_id ......................................... [[32mPASS[0m in 1.98s]
[0m19:50:20.687464 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:20.690713 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:50:20.692455 [info ] [MainThread]: 
[0m19:50:20.693906 [info ] [MainThread]: Finished running 9 tests in 0 hours 0 minutes and 4.47 seconds (4.47s).
[0m19:50:20.695237 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:50:20.696328 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m19:50:20.697264 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa' was properly closed.
[0m19:50:20.698255 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m19:50:20.699194 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e' was properly closed.
[0m19:50:20.700222 [debug] [MainThread]: Connection 'test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba' was properly closed.
[0m19:50:20.725194 [info ] [MainThread]: 
[0m19:50:20.726474 [info ] [MainThread]: [32mCompleted successfully[0m
[0m19:50:20.728023 [info ] [MainThread]: 
[0m19:50:20.729426 [info ] [MainThread]: Done. PASS=9 WARN=0 ERROR=0 SKIP=0 TOTAL=9
[0m19:50:20.730988 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075bf4040>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075963fd0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1075963820>]}


============================== 2022-08-21 19:50:25.724834 | 19974a9e-f95f-43f2-893c-5a74400bb49a ==============================
[0m19:50:25.724856 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:50:25.726277 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m19:50:25.727529 [debug] [MainThread]: Tracking: tracking
[0m19:50:25.731201 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8fcc310>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8fcc520>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8fccd00>]}
[0m19:50:27.268727 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:50:27.269685 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:50:27.277897 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '19974a9e-f95f-43f2-893c-5a74400bb49a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8d4a0a0>]}
[0m19:50:27.300066 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '19974a9e-f95f-43f2-893c-5a74400bb49a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8e407c0>]}
[0m19:50:27.301331 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:50:27.302422 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '19974a9e-f95f-43f2-893c-5a74400bb49a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8e40670>]}
[0m19:50:27.305067 [info ] [MainThread]: 
[0m19:50:27.306583 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:50:27.308791 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:50:27.309617 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:27.803389 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '19974a9e-f95f-43f2-893c-5a74400bb49a', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8f5c280>]}
[0m19:50:27.804760 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:50:27.805800 [info ] [MainThread]: 
[0m19:50:27.810898 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m19:50:27.811239 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m19:50:27.811469 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m19:50:27.811718 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m19:50:27.811908 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:27.812927 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m19:50:27.814134 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m19:50:27.815535 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m19:50:27.816769 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m19:50:27.818269 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:27.819125 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m19:50:27.819902 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m19:50:27.820631 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m19:50:27.821409 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m19:50:27.822107 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:27.822839 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m19:50:27.823570 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m19:50:27.824270 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m19:50:27.824954 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m19:50:27.825662 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:27.833322 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:50:27.837971 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:50:27.841769 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:50:27.847530 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:50:27.864098 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:27.872053 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:27.872460 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:27.873580 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m19:50:27.873865 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:27.874226 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:27.875098 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m19:50:27.875856 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:27.876094 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:27.876847 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m19:50:27.877815 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m19:50:27.878719 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:27.879511 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:27.881129 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m19:50:27.881907 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:27.882710 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:27.884088 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m19:50:27.885377 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:27.886403 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:27.888021 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m19:50:27.889418 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m19:50:27.890255 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:27.891463 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:27.892628 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:27.893355 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:27.894639 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:27.895945 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:27.896778 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:27.897581 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:27.898641 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:27.900065 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:27.901144 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:27.902686 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:27.903511 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:27.904189 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:27.905004 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:27.905821 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:27.906695 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:27.912336 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:27.913255 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:27.914205 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:27.922609 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:27.923568 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:27.933908 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:27.940344 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:27.947876 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:27.955275 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:27.957219 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:27.960437 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:27.961843 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:27.963081 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:27.963406 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:27.964078 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:27.964441 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:27.965574 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:27.965921 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:27.966638 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:27.967458 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:27.968357 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:27.969473 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:27.970315 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:27.971126 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:27.971772 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:27.972915 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:27.973610 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:27.974337 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:27.975483 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:27.976678 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:27.977957 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.dim_customers
[0m19:50:27.978857 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:27.980247 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:27.981203 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:27.982024 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:27.983356 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m19:50:27.988972 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:27.990130 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m19:50:27.991518 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:27.992639 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:27.993528 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m19:50:27.996051 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m19:50:27.996879 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:27.997730 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:27.998674 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.dim_customers
[0m19:50:27.999599 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m19:50:28.000961 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:28.002190 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:28.007016 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:50:28.007310 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:28.007988 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m19:50:28.013241 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:28.020601 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:28.022466 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:28.027793 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:50:28.031201 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:28.034696 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:28.035755 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:28.036875 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:28.037459 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:28.037726 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m19:50:28.038173 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:28.038619 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:28.039391 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m19:50:28.040100 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:28.040757 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:28.041421 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:28.042167 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:28.043805 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m19:50:28.044742 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:28.046415 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:28.047645 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m19:50:28.049912 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:28.053974 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:50:28.054812 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m19:50:28.055561 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m19:50:28.056306 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m19:50:28.056997 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa' was properly closed.
[0m19:50:28.057666 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m19:50:28.076501 [info ] [MainThread]: Done.
[0m19:50:28.129981 [debug] [MainThread]: Acquiring new bigquery connection "generate_catalog"
[0m19:50:28.130970 [info ] [MainThread]: Building catalog
[0m19:50:28.133012 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:50:28.962521 [debug] [ThreadPool]: Acquiring new bigquery connection "airflow-docker-352518.information_schema"
[0m19:50:28.963340 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m19:50:28.964230 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m19:50:28.980211 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:28.984325 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:28.987992 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:29.031833 [debug] [ThreadPool]: On airflow-docker-352518.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "airflow-docker-352518.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `airflow-docker-352518`.`dbt_x_airflow`.__TABLES__
        where (upper(dataset_id) = upper('dbt_x_airflow'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:29.033068 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`jaffle_shop`.__TABLES__
        where (upper(dataset_id) = upper('jaffle_shop'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:29.034409 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`stripe`.__TABLES__
        where (upper(dataset_id) = upper('stripe'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:32.043376 [info ] [MainThread]: Catalog written to /dbt-airflow/dbt_jaffleshop/target/catalog.json
[0m19:50:32.045137 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8c74e50>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8c740d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7f1af8c57160>]}
[0m19:50:32.512386 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m19:50:32.513770 [debug] [MainThread]: Connection 'airflow-docker-352518.information_schema' was properly closed.
[0m19:50:32.514777 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.
[0m19:50:32.515613 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.


============================== 2022-08-21 19:50:37.689476 | 265074e8-dbd1-4e43-b7e1-1ccdc13deaa6 ==============================
[0m19:50:37.689499 [info ] [MainThread]: Running with dbt=1.2.0
[0m19:50:37.690992 [debug] [MainThread]: running dbt with arguments {'write_json': True, 'use_colors': True, 'printer_width': 80, 'version_check': True, 'partial_parse': True, 'static_parser': True, 'profiles_dir': '/root/.dbt', 'send_anonymous_usage_stats': True, 'event_buffer_size': 100000, 'quiet': False, 'no_print': False, 'compile': True, 'which': 'generate', 'rpc_method': 'docs.generate', 'indirect_selection': 'eager'}
[0m19:50:37.691943 [debug] [MainThread]: Tracking: tracking
[0m19:50:37.696194 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'start', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd8421800d0>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd842177370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd842180160>]}
[0m19:50:39.421153 [debug] [MainThread]: Partial parsing enabled: 0 files deleted, 0 files added, 0 files changed.
[0m19:50:39.421915 [debug] [MainThread]: Partial parsing enabled, no changes found, skipping parsing
[0m19:50:39.429993 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'load_project', 'label': '265074e8-dbd1-4e43-b7e1-1ccdc13deaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd841f030d0>]}
[0m19:50:39.452934 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'resource_counts', 'label': '265074e8-dbd1-4e43-b7e1-1ccdc13deaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd841ff7820>]}
[0m19:50:39.453959 [info ] [MainThread]: Found 6 models, 9 tests, 0 snapshots, 0 analyses, 523 macros, 0 operations, 0 seed files, 3 sources, 1 exposure, 0 metrics
[0m19:50:39.454876 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '265074e8-dbd1-4e43-b7e1-1ccdc13deaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd841ff76d0>]}
[0m19:50:39.457530 [info ] [MainThread]: 
[0m19:50:39.458906 [debug] [MainThread]: Acquiring new bigquery connection "master"
[0m19:50:39.460857 [debug] [ThreadPool]: Acquiring new bigquery connection "list_airflow-docker-352518_dbt_x_airflow"
[0m19:50:39.462010 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:39.913303 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'runnable_timing', 'label': '265074e8-dbd1-4e43-b7e1-1ccdc13deaa6', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd8420039d0>]}
[0m19:50:39.914861 [info ] [MainThread]: Concurrency: 5 threads (target='dbt_x_airflow')
[0m19:50:39.915930 [info ] [MainThread]: 
[0m19:50:39.921572 [debug] [Thread-1  ]: Began running node model.dbt_x_airflow.agg_transactions
[0m19:50:39.921790 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.stg_customers
[0m19:50:39.922024 [debug] [Thread-3  ]: Began running node model.dbt_x_airflow.stg_orders
[0m19:50:39.922312 [debug] [Thread-4  ]: Began running node model.dbt_x_airflow.stg_payments
[0m19:50:39.922599 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:39.923325 [debug] [Thread-1  ]: Acquiring new bigquery connection "model.dbt_x_airflow.agg_transactions"
[0m19:50:39.924487 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_customers"
[0m19:50:39.925700 [debug] [Thread-3  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_orders"
[0m19:50:39.926940 [debug] [Thread-4  ]: Acquiring new bigquery connection "model.dbt_x_airflow.stg_payments"
[0m19:50:39.928100 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:39.929133 [debug] [Thread-1  ]: Began compiling node model.dbt_x_airflow.agg_transactions
[0m19:50:39.930138 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.stg_customers
[0m19:50:39.930973 [debug] [Thread-3  ]: Began compiling node model.dbt_x_airflow.stg_orders
[0m19:50:39.932021 [debug] [Thread-4  ]: Began compiling node model.dbt_x_airflow.stg_payments
[0m19:50:39.932809 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:39.933651 [debug] [Thread-1  ]: Compiling model.dbt_x_airflow.agg_transactions
[0m19:50:39.934531 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.stg_customers
[0m19:50:39.935446 [debug] [Thread-3  ]: Compiling model.dbt_x_airflow.stg_orders
[0m19:50:39.936386 [debug] [Thread-4  ]: Compiling model.dbt_x_airflow.stg_payments
[0m19:50:39.937295 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:39.944742 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_customers"
[0m19:50:39.949475 [debug] [Thread-3  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_orders"
[0m19:50:39.954117 [debug] [Thread-1  ]: Writing injected SQL for node "model.dbt_x_airflow.agg_transactions"
[0m19:50:39.959285 [debug] [Thread-4  ]: Writing injected SQL for node "model.dbt_x_airflow.stg_payments"
[0m19:50:39.975445 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f"
[0m19:50:39.983495 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:39.984998 [debug] [Thread-3  ]: Began executing node model.dbt_x_airflow.stg_orders
[0m19:50:39.985515 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:39.985976 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:39.986640 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:39.986878 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:39.987951 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.stg_customers
[0m19:50:39.988209 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:39.988796 [debug] [Thread-1  ]: Began executing node model.dbt_x_airflow.agg_transactions
[0m19:50:39.990050 [debug] [Thread-3  ]: Finished running node model.dbt_x_airflow.stg_orders
[0m19:50:39.990824 [debug] [Thread-4  ]: Began executing node model.dbt_x_airflow.stg_payments
[0m19:50:39.991766 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:39.992556 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:39.993396 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:39.994306 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:39.995532 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:39.997087 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.stg_customers
[0m19:50:39.998043 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:39.999533 [debug] [Thread-1  ]: Finished running node model.dbt_x_airflow.agg_transactions
[0m19:50:40.000802 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:40.002472 [debug] [Thread-4  ]: Finished running node model.dbt_x_airflow.stg_payments
[0m19:50:40.003390 [debug] [Thread-2  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:40.004847 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_customers_id.50aa22178f
[0m19:50:40.005702 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:40.006489 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:40.007264 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:40.008477 [debug] [Thread-2  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:40.009316 [debug] [Thread-5  ]: Began running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:40.010405 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:40.011409 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:40.013164 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:40.014114 [debug] [Thread-2  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:40.015573 [debug] [Thread-5  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:40.016483 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:40.022207 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13"
[0m19:50:40.023387 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:40.024234 [debug] [Thread-2  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:40.025098 [debug] [Thread-5  ]: Began compiling node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:40.026042 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:40.027726 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:40.038641 [debug] [Thread-2  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e"
[0m19:50:40.039521 [debug] [Thread-5  ]: Compiling test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:40.045748 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:40.047012 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba"
[0m19:50:40.060225 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1"
[0m19:50:40.067129 [debug] [Thread-5  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64"
[0m19:50:40.068480 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:40.071920 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:40.073163 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:40.073576 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:40.073877 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:40.074429 [debug] [Thread-2  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:40.076072 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.source_not_null_jaffle_shop_orders_id.f924998b13
[0m19:50:40.076321 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:40.076947 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:40.077797 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:40.078740 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:40.079640 [debug] [Thread-3  ]: Began running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:40.080507 [debug] [Thread-5  ]: Began executing node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:40.081249 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:40.082184 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:40.083394 [debug] [Thread-2  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_customers_id.2777a7933e
[0m19:50:40.084689 [debug] [Thread-3  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:40.085585 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:40.086834 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.source_unique_jaffle_shop_orders_id.8a425b2fba
[0m19:50:40.088032 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.accepted_values_stg_orders_status__completed__shipped__returned__return_pending__placed.89a0a7d5f1
[0m19:50:40.088761 [debug] [Thread-2  ]: Began running node model.dbt_x_airflow.dim_customers
[0m19:50:40.089485 [debug] [Thread-3  ]: Began compiling node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:40.090582 [debug] [Thread-5  ]: Finished running node test.dbt_x_airflow.not_null_stg_orders_order_id.81cfe2fe64
[0m19:50:40.091340 [debug] [Thread-1  ]: Began running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:40.092108 [debug] [Thread-4  ]: Began running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:40.093117 [debug] [Thread-2  ]: Acquiring new bigquery connection "model.dbt_x_airflow.dim_customers"
[0m19:50:40.093790 [debug] [Thread-3  ]: Compiling test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:40.094856 [debug] [Thread-5  ]: Began running node model.dbt_x_airflow.pivoted_orders
[0m19:50:40.096251 [debug] [Thread-1  ]: Acquiring new bigquery connection "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:40.097404 [debug] [Thread-4  ]: Acquiring new bigquery connection "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:40.098093 [debug] [Thread-2  ]: Began compiling node model.dbt_x_airflow.dim_customers
[0m19:50:40.103689 [debug] [Thread-3  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a"
[0m19:50:40.105206 [debug] [Thread-5  ]: Acquiring new bigquery connection "model.dbt_x_airflow.pivoted_orders"
[0m19:50:40.106035 [debug] [Thread-1  ]: Began compiling node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:40.107199 [debug] [Thread-4  ]: Began compiling node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:40.108034 [debug] [Thread-2  ]: Compiling model.dbt_x_airflow.dim_customers
[0m19:50:40.109517 [debug] [Thread-5  ]: Began compiling node model.dbt_x_airflow.pivoted_orders
[0m19:50:40.110238 [debug] [Thread-1  ]: Compiling test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:40.110988 [debug] [Thread-4  ]: Compiling test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:40.117496 [debug] [Thread-2  ]: Writing injected SQL for node "model.dbt_x_airflow.dim_customers"
[0m19:50:40.118959 [debug] [Thread-5  ]: Compiling model.dbt_x_airflow.pivoted_orders
[0m19:50:40.119724 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:40.124859 [debug] [Thread-1  ]: Writing injected SQL for node "test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa"
[0m19:50:40.131780 [debug] [Thread-4  ]: Writing injected SQL for node "test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada"
[0m19:50:40.137685 [debug] [Thread-5  ]: Writing injected SQL for node "model.dbt_x_airflow.pivoted_orders"
[0m19:50:40.138810 [debug] [Thread-3  ]: Began executing node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:40.142497 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:40.142768 [debug] [Thread-3  ]: finished collecting timing info
[0m19:50:40.143772 [debug] [Thread-2  ]: Began executing node model.dbt_x_airflow.dim_customers
[0m19:50:40.144878 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:40.146388 [debug] [Thread-3  ]: Finished running node test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a
[0m19:50:40.146793 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:40.147707 [debug] [Thread-2  ]: finished collecting timing info
[0m19:50:40.148324 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:40.148997 [debug] [Thread-1  ]: Began executing node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:40.150756 [debug] [Thread-4  ]: Began executing node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:40.152285 [debug] [Thread-2  ]: Finished running node model.dbt_x_airflow.dim_customers
[0m19:50:40.153242 [debug] [Thread-5  ]: Began executing node model.dbt_x_airflow.pivoted_orders
[0m19:50:40.153994 [debug] [Thread-1  ]: finished collecting timing info
[0m19:50:40.154746 [debug] [Thread-4  ]: finished collecting timing info
[0m19:50:40.156109 [debug] [Thread-5  ]: finished collecting timing info
[0m19:50:40.157342 [debug] [Thread-1  ]: Finished running node test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa
[0m19:50:40.158514 [debug] [Thread-4  ]: Finished running node test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada
[0m19:50:40.159642 [debug] [Thread-5  ]: Finished running node model.dbt_x_airflow.pivoted_orders
[0m19:50:40.163625 [debug] [MainThread]: Connection 'master' was properly closed.
[0m19:50:40.164663 [debug] [MainThread]: Connection 'test.dbt_x_airflow.not_null_stg_customers_customer_id.e2cfb1f9aa' was properly closed.
[0m19:50:40.165454 [debug] [MainThread]: Connection 'model.dbt_x_airflow.dim_customers' was properly closed.
[0m19:50:40.166132 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_orders_order_id.e3b841c71a' was properly closed.
[0m19:50:40.166789 [debug] [MainThread]: Connection 'test.dbt_x_airflow.unique_stg_customers_customer_id.c7614daada' was properly closed.
[0m19:50:40.167415 [debug] [MainThread]: Connection 'model.dbt_x_airflow.pivoted_orders' was properly closed.
[0m19:50:40.187215 [info ] [MainThread]: Done.
[0m19:50:40.241131 [debug] [MainThread]: Acquiring new bigquery connection "generate_catalog"
[0m19:50:40.241960 [info ] [MainThread]: Building catalog
[0m19:50:40.243619 [debug] [MainThread]: Opening a new connection, currently in state init
[0m19:50:41.039370 [debug] [ThreadPool]: Acquiring new bigquery connection "airflow-docker-352518.information_schema"
[0m19:50:41.040092 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m19:50:41.040798 [debug] [ThreadPool]: Acquiring new bigquery connection "dbt-tutorial.information_schema"
[0m19:50:41.057860 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:41.061033 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:41.064726 [debug] [ThreadPool]: Opening a new connection, currently in state init
[0m19:50:41.109117 [debug] [ThreadPool]: On airflow-docker-352518.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "airflow-docker-352518.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `airflow-docker-352518`.`dbt_x_airflow`.__TABLES__
        where (upper(dataset_id) = upper('dbt_x_airflow'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `airflow-docker-352518`.`dbt_x_airflow`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:41.110040 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`jaffle_shop`.__TABLES__
        where (upper(dataset_id) = upper('jaffle_shop'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`jaffle_shop`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:41.111419 [debug] [ThreadPool]: On dbt-tutorial.information_schema: /* {"app": "dbt", "dbt_version": "1.2.0", "profile_name": "bigquery", "target_name": "dbt_x_airflow", "connection_name": "dbt-tutorial.information_schema"} */

    with tables as (
        select
            project_id as table_database,
            dataset_id as table_schema,
            table_id as original_table_name,

            concat(project_id, '.', dataset_id, '.', table_id) as relation_id,

            row_count,
            size_bytes as size_bytes,
            case
                when type = 1 then 'table'
                when type = 2 then 'view'
                else 'external'
            end as table_type,

            REGEXP_CONTAINS(table_id, '^.+[0-9]{8}$') and coalesce(type, 0) = 1 as is_date_shard,
            REGEXP_EXTRACT(table_id, '^(.+)[0-9]{8}$') as shard_base_name,
            REGEXP_EXTRACT(table_id, '^.+([0-9]{8})$') as shard_name

        from `dbt-tutorial`.`stripe`.__TABLES__
        where (upper(dataset_id) = upper('stripe'))
    ),

    extracted as (

        select *,
            case
                when is_date_shard then shard_base_name
                else original_table_name
            end as table_name

        from tables

    ),

    unsharded_tables as (

        select
            table_database,
            table_schema,
            table_name,
            coalesce(table_type, 'external') as table_type,
            is_date_shard,

            struct(
                min(shard_name) as shard_min,
                max(shard_name) as shard_max,
                count(*) as shard_count
            ) as table_shards,

            sum(size_bytes) as size_bytes,
            sum(row_count) as row_count,

            max(relation_id) as relation_id

        from extracted
        group by 1,2,3,4,5

    ),

    info_schema_columns as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            table_catalog as table_database,
            table_schema,
            table_name,

            -- use the "real" column name from the paths query below
            column_name as base_column_name,
            ordinal_position as column_index,

            is_partitioning_column,
            clustering_ordinal_position

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMNS
        where ordinal_position is not null

    ),

    info_schema_column_paths as (

        select
            concat(table_catalog, '.', table_schema, '.', table_name) as relation_id,
            field_path as column_name,
            data_type as column_type,
            column_name as base_column_name,
            description as column_comment

        from `dbt-tutorial`.`stripe`.INFORMATION_SCHEMA.COLUMN_FIELD_PATHS

    ),

    columns as (

        select * except (base_column_name)
        from info_schema_columns
        join info_schema_column_paths using (relation_id, base_column_name)

    ),

    column_stats as (

        select
            table_database,
            table_schema,
            table_name,
            max(relation_id) as relation_id,
            max(case when is_partitioning_column = 'YES' then 1 else 0 end) = 1 as is_partitioned,
            max(case when is_partitioning_column = 'YES' then column_name else null end) as partition_column,
            max(case when clustering_ordinal_position is not null then 1 else 0 end) = 1 as is_clustered,
            array_to_string(
                array_agg(
                    case
                        when clustering_ordinal_position is not null then column_name
                        else null
                    end ignore nulls
                    order by clustering_ordinal_position
                ), ', '
            ) as clustering_columns

        from columns
        group by 1,2,3

    )

    select
        unsharded_tables.table_database,
        unsharded_tables.table_schema,
        case
            when is_date_shard then concat(unsharded_tables.table_name, '*')
            else unsharded_tables.table_name
        end as table_name,
        unsharded_tables.table_type,

        -- coalesce name and type for External tables - these columns are not
        -- present in the COLUMN_FIELD_PATHS resultset
        coalesce(columns.column_name, '<unknown>') as column_name,
        -- invent a row number to account for nested fields -- BQ does
        -- not treat these nested properties as independent fields
        row_number() over (
            partition by relation_id
            order by columns.column_index, columns.column_name
        ) as column_index,
        coalesce(columns.column_type, '<unknown>') as column_type,
        columns.column_comment,

        'Shard count' as `stats__date_shards__label`,
        table_shards.shard_count as `stats__date_shards__value`,
        'The number of date shards in this table' as `stats__date_shards__description`,
        is_date_shard as `stats__date_shards__include`,

        'Shard (min)' as `stats__date_shard_min__label`,
        table_shards.shard_min as `stats__date_shard_min__value`,
        'The first date shard in this table' as `stats__date_shard_min__description`,
        is_date_shard as `stats__date_shard_min__include`,

        'Shard (max)' as `stats__date_shard_max__label`,
        table_shards.shard_max as `stats__date_shard_max__value`,
        'The last date shard in this table' as `stats__date_shard_max__description`,
        is_date_shard as `stats__date_shard_max__include`,

        '# Rows' as `stats__num_rows__label`,
        row_count as `stats__num_rows__value`,
        'Approximate count of rows in this table' as `stats__num_rows__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_rows__include`,

        'Approximate Size' as `stats__num_bytes__label`,
        size_bytes as `stats__num_bytes__value`,
        'Approximate size of table as reported by BigQuery' as `stats__num_bytes__description`,
        (unsharded_tables.table_type = 'table') as `stats__num_bytes__include`,

        'Partitioned By' as `stats__partitioning_type__label`,
        partition_column as `stats__partitioning_type__value`,
        'The partitioning column for this table' as `stats__partitioning_type__description`,
        is_partitioned as `stats__partitioning_type__include`,

        'Clustered By' as `stats__clustering_fields__label`,
        clustering_columns as `stats__clustering_fields__value`,
        'The clustering columns for this table' as `stats__clustering_fields__description`,
        is_clustered as `stats__clustering_fields__include`

    -- join using relation_id (an actual relation, not a shard prefix) to make
    -- sure that column metadata is picked up through the join. This will only
    -- return the column information for the "max" table in a date-sharded table set
    from unsharded_tables
    left join columns using (relation_id)
    left join column_stats using (relation_id)
  
[0m19:50:44.085332 [info ] [MainThread]: Catalog written to /dbt-airflow/dbt_jaffleshop/target/catalog.json
[0m19:50:44.086696 [debug] [MainThread]: Sending event: {'category': 'dbt', 'action': 'invocation', 'label': 'end', 'context': [<snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd842177370>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd841e70550>, <snowplow_tracker.self_describing_json.SelfDescribingJson object at 0x7fd841e701f0>]}
[0m19:50:44.547388 [debug] [MainThread]: Connection 'generate_catalog' was properly closed.
[0m19:50:44.548864 [debug] [MainThread]: Connection 'airflow-docker-352518.information_schema' was properly closed.
[0m19:50:44.549914 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.
[0m19:50:44.550882 [debug] [MainThread]: Connection 'dbt-tutorial.information_schema' was properly closed.
